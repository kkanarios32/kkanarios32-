<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3208</fr:anchor><fr:addr
type="user">choMiniBatchOptimizationContrastive2023</fr:addr><fr:route>choMiniBatchOptimizationContrastive2023.xml</fr:route><fr:title
text="Mini-Batch Optimization of Contrastive Loss">Mini-Batch Optimization of Contrastive Loss</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2023</fr:year><fr:month>7</fr:month></fr:date><fr:authors><fr:author>Jaewoong Cho</fr:author><fr:author>Kartik Sreenivasan</fr:author><fr:author>Keon Lee</fr:author><fr:author>Kyunghoo Mun</fr:author><fr:author>Soheun Yi</fr:author><fr:author>Jeong-Gwan Lee</fr:author><fr:author>Anna Lee</fr:author><fr:author>Jy-yong Sohn</fr:author><fr:author>Dimitris Papailiopoulos</fr:author><fr:author>Kangwook Lee</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/2307.05906v1</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{choMiniBatchOptimizationContrastive2023,
 title = {Mini-{{Batch Optimization}} of {{Contrastive Loss}}},
 author = {Cho, Jaewoong and Sreenivasan, Kartik and Lee, Keon and Mun, Kyunghoo and Yi, Soheun and Lee, Jeong-Gwan and Lee, Anna and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
 year = {2023},
 urldate = {2024-09-06},
 howpublished = {https://arxiv.org/abs/2307.05906v1},
 journal = {arXiv.org},
 file = {/home/kellen/Zotero/storage/G87QGI8T/Cho et al. - 2023 - Mini-Batch Optimization of Contrastive Loss.pdf},
 langid = {english},
 abstract = {Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all \${\textbackslash}binom\{N\}\{B\}\$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experimental results validate our theoretical findings and demonstrate that our proposed algorithm outperforms vanilla SGD in practically relevant settings, providing a better understanding of mini-batch optimization in contrastive learning.},
 month = {July}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>