<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3423</fr:anchor><fr:addr
type="user">elsayedDeepReinforcementLearning</fr:addr><fr:route>elsayedDeepReinforcementLearning.xml</fr:route><fr:title
text="Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates">Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author>Mohamed Elsayed</fr:author><fr:author>Gautham Vasan</fr:author><fr:author>A Rupam Mahmood</fr:author></fr:authors><fr:meta
name="bibtex"><![CDATA[@article{elsayedDeepReinforcementLearning,
 title = {Deep {{Reinforcement Learning Without Experience Replay}}, {{Target Networks}}, or {{Batch Updates}}},
 author = {Elsayed, Mohamed and Vasan, Gautham and Mahmood, A Rupam},
 file = {/home/kellen/Zotero/storage/XDJZFXVT/Elsayed et al. - Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates.pdf},
 langid = {english},
 abstract = {Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resource-constrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>