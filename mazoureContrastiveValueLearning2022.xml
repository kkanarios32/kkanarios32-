<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3218</fr:anchor><fr:addr
type="user">mazoureContrastiveValueLearning2022</fr:addr><fr:route>mazoureContrastiveValueLearning2022.xml</fr:route><fr:title
text="Contrastive Value Learning: Implicit Models for Simple Offline RL">Contrastive Value Learning: Implicit Models for Simple Offline RL</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2022</fr:year><fr:month>11</fr:month></fr:date><fr:authors><fr:author>Bogdan Mazoure</fr:author><fr:author>Benjamin Eysenbach</fr:author><fr:author>Ofir Nachum</fr:author><fr:author>Jonathan Tompson</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2211.02100</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2211.02100</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{mazoureContrastiveValueLearning2022,
 title = {Contrastive {{Value Learning}}: {{Implicit Models}} for {{Simple Offline RL}}},
 author = {Mazoure, Bogdan and Eysenbach, Benjamin and Nachum, Ofir and Tompson, Jonathan},
 year = {2022},
 doi = {10.48550/arXiv.2211.02100},
 urldate = {2025-01-09},
 number = {arXiv:2211.02100},
 publisher = {arXiv},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Model-based reinforcement learning (RL) methods are appealing in the offline setting because they allow an agent to reason about the consequences of actions without interacting with the environment. Prior methods learn a 1-step dynamics model, which predicts the next state given the current state and action. These models do not immediately tell the agent which actions to take, but must be integrated into a larger RL framework. Can we model the environment dynamics in a different way, such that the learned model does directly indicate the value of each action? In this paper, we propose Contrastive Value Learning (CVL), which learns an implicit, multi-step model of the environment dynamics. This model can be learned without access to reward functions, but nonetheless can be used to directly estimate the value of each action, without requiring any TD learning. Because this model represents the multi-step transitions implicitly, it avoids having to predict high-dimensional observations and thus scales to high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior offline RL methods on complex continuous control benchmarks.},
 primaryclass = {cs},
 eprint = {2211.02100},
 month = {November},
 shorttitle = {Contrastive {{Value Learning}}}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>