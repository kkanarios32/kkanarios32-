<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3679</fr:anchor><fr:addr
type="user">gutmann2012</fr:addr><fr:route>gutmann2012.xml</fr:route><fr:title
text="Noise-contrastive Estimation">Noise-contrastive Estimation</fr:title><fr:taxon>Reference</fr:taxon><fr:authors /><fr:meta
name="abstract">We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.</fr:meta><fr:meta
name="bibtex"><![CDATA[@InProceedings{pmlr-v9-gutmann10a,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and HyvÃ¤rinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Backlinks">Backlinks</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3680</fr:anchor><fr:addr
type="user">kak-0009</fr:addr><fr:route>kak-0009.xml</fr:route><fr:title
text="Contrastive Learning">Contrastive Learning</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Prior to understanding contrastive reinforcement learning, it is important to have an at least rudimentary understanding of contrastive learning. Historically, contrastive learning has been used to learn representations. The fundamental idea behind contrastive learning is to encourage the representations of similar outputs to be similar in representation space.</fr:p><fr:p><fr:strong>Supervised setting:</fr:strong> For now, assume we are in the supervised setting (we have access to lables). Suppose that we are learning a representation in <fr:tex
display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Our model is a classifier on dogs and cats. If we have two dogs <fr:tex
display="inline"><![CDATA[y_1]]></fr:tex> and <fr:tex
display="inline"><![CDATA[y_2]]></fr:tex> then we want the learned representation map <fr:tex
display="block"><![CDATA[\phi : \{\text {dogs}, \text {cats}\} \to  \mathbb {R}^d]]></fr:tex> to be such that <fr:tex
display="inline"><![CDATA[\phi (y_1)]]></fr:tex> and <fr:tex
display="inline"><![CDATA[\phi (y_2)]]></fr:tex> are "close" in <fr:tex
display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Now the notion of "close" is to be determined by the user. An example could be to minimize the inner product between their representation maps i.e. we could learn a feature map parametrized by <fr:tex
display="inline"><![CDATA[\theta ]]></fr:tex> with the following objective <fr:tex
display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle .]]></fr:tex> Similarly, we want dissimilar outputs to be far apart in representation space. If <fr:tex
display="inline"><![CDATA[y_3]]></fr:tex> is a cat, then we can introduce a regularization to encourage this i.e.
<fr:tex
display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle  - \sum _{i \in  \{1, 2\}} \langle  \phi _{\theta }(y_i), \phi _{\theta }(y_3) \rangle .]]></fr:tex></fr:p><fr:p><fr:strong>Unsupervised setting:</fr:strong> Now suppose that we get rid of labels and are just given <fr:tex
display="inline"><![CDATA[n]]></fr:tex> dog samples <fr:tex
display="inline"><![CDATA[\mathcal {D}]]></fr:tex> from some distribution <fr:tex
display="inline"><![CDATA[p_{\mathcal {D}}]]></fr:tex>. We now want to be able to learn <fr:tex
display="inline"><![CDATA[p_{\theta }]]></fr:tex> to somehow estimate this distribution. An approach is to learn to distinguish the sample dogs given from random noise. To do so, we generate <fr:tex
display="inline"><![CDATA[n]]></fr:tex> random images <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex> according to some distribution <fr:tex
display="inline"><![CDATA[p_{\mathcal {R}}]]></fr:tex>. We can now return to the supervised learning setting, where we treat <fr:tex
display="inline"><![CDATA[\mathcal {D}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex> as two classes. If we recall standard supervised learning practice, given a sample <fr:tex
display="inline"><![CDATA[x]]></fr:tex>, we then want to find <fr:tex
display="block"><![CDATA[p(\mathcal {D} \mid  x) = 1 - p(\mathcal {R} \mid  X).]]></fr:tex> 
As an explicit example, we will use logistic regression. Namely, we will model <fr:tex
display="inline"><![CDATA[p(x) = p(\mathcal {D} \mid  x)]]></fr:tex> as <fr:tex
display="block"><![CDATA[p_{\theta }(x) = \frac {1}{1 + e^{-G_{\theta }(x)}}.]]></fr:tex> However, <fr:tex
display="inline"><![CDATA[p_{\theta }(x)]]></fr:tex> is estimating <fr:tex
display="inline"><![CDATA[p(\mathcal {D} \mid  x)]]></fr:tex>, where we care about <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex>. To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if <fr:tex
display="inline"><![CDATA[p_{\theta }(x) = p(\mathcal {D} \mid  x)]]></fr:tex> then <fr:tex
display="inline"><![CDATA[G_{\theta }(x) = \log  \frac {p(x \mid  \mathcal {D})}{p(x \mid  \mathcal {R})}]]></fr:tex>. Since we generated the samples from <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex>, we have the explicit distribution i.e. <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {R}) = p_{\mathcal {R}}(x)]]></fr:tex>. Therefore, we can restrict <fr:tex
display="inline"><![CDATA[G_{\theta }]]></fr:tex> to explicitly learn <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex> by considering <fr:tex
display="block"><![CDATA[G_{\theta }(x) = \log  p_{\theta }(x \mid  \mathcal {D}) - \log  p_{\mathcal {R}}(x),]]></fr:tex> considering the cross entropy loss we get the <fr:link
type="external"
href="nce">NCE loss</fr:link></fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>439</fr:anchor><fr:addr
type="user">kak-000E</fr:addr><fr:route>kak-000E.xml</fr:route><fr:title
text="NCE loss">NCE loss</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em><fr:link
type="external"
href="nce">NCE</fr:link></fr:em> loss aims to minimize the following objective <fr:tex
display="block"><![CDATA[\mathcal {L}_{N} = - \sum _{t} \log  \left [h(x_t; \theta )\right ] + \log \left [1 - h(y_t; \theta )\right ],]]></fr:tex> where <fr:tex
display="inline"><![CDATA[x_t]]></fr:tex> are samples from the data distribution and <fr:tex
display="inline"><![CDATA[y_t]]></fr:tex> are randomly generated samples and <fr:tex
display="block"><![CDATA[\begin {array}{r c l}{{h({\bf  u};\theta )}}&{{=}}&{{\frac {1}{1+\exp \left [-G({\bf  u};\theta )\right ]},}}\\ {{G({\bf  u};\theta )}}&{{=}}&{{\ln  p_{m}({\bf  u};\theta )-\ln  p_{n}({\bf  u}).}}\end {array}]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>In <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">Noise-contrastive Estimation</fr:link>, they show under mild conditions that the estimator <fr:tex
display="inline"><![CDATA[p_{\theta }(x \mid  D) \to  p_{\mathcal {D}}(x)]]></fr:tex> in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is <fr:link
type="local"
href="kak-000F.xml"
addr="kak-000F"
title="Consistent estimator">consistent</fr:link>.</fr:p><fr:p><fr:strong>Time series:</fr:strong> Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner. For a <fr:tex
display="inline"><![CDATA[k]]></fr:tex> step prediction, this would just be your model predicting what will happen in <fr:tex
display="inline"><![CDATA[k]]></fr:tex> steps then seeing if it matches what occured <fr:tex
display="inline"><![CDATA[k]]></fr:tex> steps in the future in the sample trajectory. However, if your sample space <fr:tex
display="inline"><![CDATA[\mathcal {X}]]></fr:tex> is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.</fr:p><fr:p>Fast forwarding to contrastive RL, current work is primarily considered with a particular contrastive objective.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>440</fr:anchor><fr:addr
type="user">kak-000B</fr:addr><fr:route>kak-000B.xml</fr:route><fr:title
text="InfoNCE">InfoNCE</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em><fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">InfoNCE</fr:link></fr:em> loss aims to minimize the following information-theoretic objective <fr:tex
display="block"><![CDATA[\mathcal {L}_{N} = - \mathbb {E}_{\mathcal {X}} \left [\log  \frac {f_k(x_{t + k}, c_t)}{\sum _{x_j \in  \mathcal {X}} f_k(x_j, c_t)}\right ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>Now we need to unpack this very ominous loss. To start, what are <fr:tex
display="inline"><![CDATA[x_k]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_t]]></fr:tex>?</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>441</fr:anchor><fr:addr
type="user">kak-000C</fr:addr><fr:route>kak-000C.xml</fr:route><fr:title
text="Maximize Mutual info">Maximize Mutual info</fr:title><fr:taxon>Theorem</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p><fr:tex
display="inline"><![CDATA[\mathcal {L}_N]]></fr:tex> from <fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">Contrastive Predictive Decoding</fr:link> maximizes a lower bound on the <fr:link
type="local"
href="kak-000V.xml"
addr="kak-000V"
title="Mutual Information">Mutual Information</fr:link> between <fr:tex
display="inline"><![CDATA[x_{t + k}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_t]]></fr:tex>.</fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>442</fr:anchor><fr:addr
type="machine">#377</fr:addr><fr:route>unstable-377.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>All we must do is plug <fr:tex
display="inline"><![CDATA[\frac {p(x \mid  c)}{p(x)}]]></fr:tex> back into the objective.
  <fr:tex
display="block"><![CDATA[\begin {align*}     \mathcal {L}_{\mathbb {N}}^{\text {opt}}&=-\,\mathbb {E}\log \left [\frac {\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}}{\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}+\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &\approx \mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}(N-1)\,\mathbb {E}\,\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k} \mid  c_t)}N\right ] \\     &\geq  \mathbb {E} \log  \left [\frac {p(x_{t + k})}{p(x_{t + k} \mid  c_t)}N \right ] \\     &= - I(x_{t + k}, c_t) + \log  N  \end {align*}   ]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree><fr:tex
display="block"><![CDATA[ \operatorname *{max}_{f(u,v)}\mathbb {E}_{(u,v^{+})\sim  p(u,v)}\left [\log \sigma (\underbrace {f(u,{\green  v^{+}})}_{\phi (u)^{T}\psi ({\green  v^{+}})})+\log (1-\sigma (\underbrace {f(u,{\red  v^{-}})}_{\phi (u)^{T}\psi ({\red  v^{-}})}))\right ] ]]></fr:tex><fr:tex
display="block"><![CDATA[ \begin {align*} &\operatorname *{max}_{f}\mathbb {E}_{(s,a)\sim  p(s,a),s_{f}^{-}\sim  p(s_{f})}\left [\mathcal {L}(s,a,s_{f}^{+},s_{f}^{-})\right ] \\ \end {align*} ]]></fr:tex><fr:tex
display="block"><![CDATA[ \mathcal {L}_1(\theta ) = \log \sigma (f_{\theta }(s_1,a_1,{\color {green} s_{8}})) + \log (1-\sigma (f_{\theta }(s_1,a_1, {\color {red} s_3}))) ]]></fr:tex><fr:tex
display="block"><![CDATA[ \begin {align*} \widehat {\mathcal {L}}(\theta ) &= \frac {1}{n} \sum _{i = 1}^{n} \mathcal {L}_i \\ &= \frac {1}{n} \sum _{i = 1}^{n} \Big [\log \sigma (f_{\theta }(s_i,a_i,{\color {green} s_{f}^{+}})) + \log (1-\sigma (f_{\theta }(s_i,a_i, {\color {red} s_{f}^{-}})))\Big ] \end {align*} ]]></fr:tex><fr:tex
display="block"><![CDATA[ \mathcal {L}(\theta ) = \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] ]]></fr:tex><fr:tex
display="block"><![CDATA[f^*(s, a, s_g) = \log \left (\frac {p^{\pi (\cdot  \mid  \cdot )}(s_g \mid  s, a)}{p(s_g)}\right )]]></fr:tex>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>443</fr:anchor><fr:addr
type="machine">#378</fr:addr><fr:route>unstable-378.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    We want to maximize
<fr:tex
display="block"><![CDATA[     \begin {align*} \mathcal {L}(\theta ) &= \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] \\ &= \int  \log \sigma (f_{\theta }(x)) P_X(x) + \int  \log (1-\sigma (f_{\theta }(y))) P_Y(y) \\ &= \int  \log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)     \end {align*}     ]]></fr:tex>
    Since we are maximizing <fr:tex
display="inline"><![CDATA[f(s)]]></fr:tex>, we can just maximize the integrand i.e.
<fr:tex
display="block"><![CDATA[       \begin {align*}         \frac {\mathrm {d}}{\mathrm {d}f(z)} \Big [\log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)\Big ] = 0       \end {align*}     ]]></fr:tex>
    Solving,
    <fr:tex
display="block"><![CDATA[         \begin {align*}           P_X(z)\big (1 - \sigma (f(z))\big ) - P_Y(z)\sigma (f(z)) = 0 &\iff  \sigma (f(z)) = \frac {P_X(z)}{P_X(z) + P_Y(z)} \\           &\iff  f(z) = \log \left (\frac {P_X(z)}{P_Y(z)}\right )         \end {align*}       ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 


   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>444</fr:anchor><fr:addr
type="machine">#379</fr:addr><fr:route>unstable-379.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
The first step is to prove that the average Q-values are close to the task-conditioned Q-values. Below, we will use <fr:tex
display="inline"><![CDATA[R_{c}(\tau )\triangleq \sum _{\ell =0}^{\infty }\gamma ^{\ell }r_{\ell }(s_{\ell },a_{\ell })]]></fr:tex>:

<fr:tex
display="block"><![CDATA[ \begin {align*} \left |Q^{\beta (\cdot |\cdot ,a)}(s,a,e)-Q^{\beta (\cdot |\cdot ,\epsilon ^{\prime })}(s,a,e)\right |&=\left |\int \beta (\tau \mid  s,a,e)R_{e}(\tau )d\tau -\int \beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right |\\  &=\left |\int \beta (\tau \mid  s,a,e)-\beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right | \\ &=\left |\int \beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )R_{e}(\tau )d\tau \right | \\ &\leq \int \left |\beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )\right |d\tau \cdot \operatorname *{max}_{\tau }|R_{e}(\tau )d\tau | \\ &\leq \int \beta (\tau \mid  s,a,e)\left |1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right |d\tau \cdot  1 \\ &=\mathbb {E}_{\beta (\tau |s,a,e)}\left [\left |1-{\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}}\right |\right ] \\ &\leq  \epsilon . \end {align*}   ]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3681</fr:anchor><fr:addr
type="user">log-0007</fr:addr><fr:route>log-0007.xml</fr:route><fr:title
text="11/21/2024">11/21/2024</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:meta
name="author">false</fr:meta></fr:frontmatter><fr:mainmatter><fr:p>Not a maximally productive day but good progress was made on (almost?) every project.</fr:p>
  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2597</fr:anchor><fr:addr
type="machine">#244</fr:addr><fr:route>unstable-244.xml</fr:route><fr:title
text="Daily Summary">Daily Summary</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:p><fr:strong>Probability Theory:</fr:strong> I am having a surprisingly good time learning this compared to my experience in undergrad. Today, I finished and thorougly understood the extension of a measure to the <fr:tex
display="inline"><![CDATA[\sigma ]]></fr:tex>-algebra induced by the outer measure. I left the uniqueness for the next session. I guess the only outstanding question I had was 
    <fr:ul><fr:li>Why do we need to impose the extra condition that we need the set to satisfy <fr:tex
display="inline"><![CDATA[P^*(A \cap  E) + P^*(A^c \cap  E) = P^*(E)]]></fr:tex> for every <fr:tex
display="inline"><![CDATA[E \subset  \Omega ]]></fr:tex> vs. just taking <fr:tex
display="inline"><![CDATA[E = \Omega ]]></fr:tex>?</fr:li></fr:ul></fr:p>

  <fr:p><fr:strong>Contrastive RL:</fr:strong> Today was big for my understanding along with an interesting new direction.
    <fr:ul><fr:li>I went over both the original <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">NCE</fr:link> and <fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">InfoNCE</fr:link> papers to better understand the non-RL versions.</fr:li>
      <fr:li>Finally have kind of understood, where the <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function comes from in <fr:link
type="local"
href="eysenbach2023.xml"
addr="eysenbach2023"
title="Constrastive Learning as Goal Conditioned Reinforcement Learning">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:link> even though they do not provide proof of the Lemma that claims it.</fr:li>
      <fr:ul><fr:li>In the <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">Noise-contrastive Estimation</fr:link> paper, they show that their objective will converge to the distribution that generated the data <fr:tex
display="inline"><![CDATA[p(x)]]></fr:tex>. In <fr:link
type="local"
href="eysenbach2023.xml"
addr="eysenbach2023"
title="Constrastive Learning as Goal Conditioned Reinforcement Learning">Constrastive Learning as Goal Conditioned Reinforcement Learning</fr:link>, they want to learn the discounted state occupancy measure. To make this their <fr:tex
display="inline"><![CDATA[p(x)]]></fr:tex>, 
          <fr:ol><fr:li>they take their "positive" examples as drawn from the state occupancy (turns out to do this is not that hard).</fr:li>
            <fr:li>they just sample their negative examples uniformly.</fr:li></fr:ol>
          <fr:li>Need to look a little more at <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function is state discounted probability proof. Why is it not just linearity of expectation / Fubini? Inner series is clearly convergent? Probably related to policy stuff I need to get better at.</fr:li></fr:li></fr:ul>
        <fr:li>An interesting idea that came up is to apply goal-conditioned RL to safe RL. Namely, we can discourage the visitation of unsafe states by considering <fr:tex
display="inline"><![CDATA[Q(s, a, \text {unsafe})]]></fr:tex></fr:li>
        <fr:ul><fr:li>Take <fr:tex
display="inline"><![CDATA[\pi  \in  \arg \max _{\pi } Q(s, \pi (s), g) - \lambda (\delta ) Q(s, \pi (s), \text {obstacle})]]></fr:tex>, or</fr:li>
            <fr:li>take <fr:tex
display="inline"><![CDATA[\pi  \in  \arg \max _{\pi } Q(s, \pi (s), g)]]></fr:tex>, such that <fr:tex
display="inline"><![CDATA[Q(s, \pi (s), \text {obstacle}) < \epsilon ]]></fr:tex>.</fr:li></fr:ul></fr:ul></fr:p>
<fr:p><fr:strong>Ghost in the shell:</fr:strong> New ideas have arisen to turn this course project into a research project.
<fr:ul><fr:li>Professors introduce the concept of "phase detection" from microarchitecture.</fr:li>
    <fr:li>It seems that this area is concerned with detecting how inputs may influence things like hot paths and detecting when this is going to occur.</fr:li>
    <fr:li>These methods all seems to choose metrics and algorithms for these metrics arbitrarily.</fr:li>
    <fr:li>Can we pass a superset of all these metrics to an LLM and have it detect phase changes automatically?</fr:li></fr:ul></fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2599</fr:anchor><fr:addr
type="machine">#245</fr:addr><fr:route>unstable-245.xml</fr:route><fr:title
text="Tomorrow Todo's">Tomorrow Todo's</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>I want to start <fr:link
type="local"
href="jiang2024.xml"
addr="jiang2024"
title="Reinforcement Learning: Theory and Algorithms">Reinforcement Learning: Theory and Algorithms</fr:link> tomorrow, taking a short break from very abstract / rigorous mathematics.</fr:li>
    <fr:li>Focus tomorrow is getting profiling up and running.</fr:li>
    <fr:ul><fr:li>By EOD, need to have edges of CFG filled with visitation frequencies.</fr:li></fr:ul>
    <fr:li>If extra time, finish / work on <fr:link
type="local"
href="kak-0005.xml"
addr="kak-0005"
title="Contrastive Reinforcement Learning">Contrastive Reinforcement Learning</fr:link> blog post with newfound knowledge.</fr:li></fr:ul>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>