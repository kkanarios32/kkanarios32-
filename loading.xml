<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2718</fr:anchor><fr:addr
type="user">loading</fr:addr><fr:route>loading.xml</fr:route><fr:title
text="Blog › Coming Soon!!!"><fr:link
type="local"
href="kak-0002.xml"
addr="kak-0002"
title="Blog">Blog</fr:link> › Coming Soon!!!</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>29</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>438</fr:anchor><fr:addr
type="user">kak-005D</fr:addr><fr:route>kak-005D.xml</fr:route><fr:title
text="Optimization from a Deep Learning Perspective">Optimization from a Deep Learning Perspective</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>440</fr:anchor><fr:addr
type="user">kak-004X</fr:addr><fr:route>kak-004X.xml</fr:route><fr:title
text="A Note on Advantage Estimation">A Note on Advantage Estimation</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>442</fr:anchor><fr:addr
type="machine">#257</fr:addr><fr:route>unstable-257.xml</fr:route><fr:title
text="What is it?">What is it?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>444</fr:anchor><fr:addr
type="machine">#258</fr:addr><fr:route>unstable-258.xml</fr:route><fr:title
text="Why do we do it?">Why do we do it?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>446</fr:anchor><fr:addr
type="machine">#259</fr:addr><fr:route>unstable-259.xml</fr:route><fr:title
text="Generalized Advantage Estimation">Generalized Advantage Estimation</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

  </fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>448</fr:anchor><fr:addr
type="user">kak-003Y</fr:addr><fr:route>kak-003Y.xml</fr:route><fr:title
text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Rough itinerary,
  <fr:ul><fr:li>Vanilla policy gradient
      <fr:ul><fr:li>Policy gradient theorem + proof</fr:li>
          <fr:li>Deterministic policy gradient theorem + (maybe)proof</fr:li></fr:ul></fr:li>
      <fr:li>Actor critic method
      <fr:ul><fr:li>A2C: Variance reduction method</fr:li>
        <fr:li>(Maybe) A3C: Asynchronous update</fr:li></fr:ul></fr:li>
      <fr:li>Trust region policy optimization</fr:li>
      <fr:li>Soft Actor Critic</fr:li>
      <fr:li><fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">Proximal Policy Optimization</fr:link></fr:li>
      <fr:li><fr:link
type="local"
href="kak-003X.xml"
addr="kak-003X"
title="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:link></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>450</fr:anchor><fr:addr
type="user">kak-003D</fr:addr><fr:route>kak-003D.xml</fr:route><fr:title
text="Deepseek v1 through R1: RL is back!">Deepseek v1 through R1: RL is back!</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>In this blog, we will aim to understand the key contributions of <fr:link
type="local"
href="deepseekai2025deepseekr1incentivizingreasoningcapability.xml"
addr="deepseekai2025deepseekr1incentivizingreasoningcapability"
title="DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning">DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning</fr:link>. It will serve as the complement to my group meeting presentation possibly consisting of more in-depth explanations. Time permitting, we might go over the engineering innovations introduced in <fr:link
type="local"
href="deepseekai2024deepseekv3technicalreport.xml"
addr="deepseekai2024deepseekv3technicalreport"
title="DeepSeek-V3 technical report">DeepSeek-V3 technical report</fr:link>.</fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>452</fr:anchor><fr:addr
type="machine">#292</fr:addr><fr:route>unstable-292.xml</fr:route><fr:title
text="Background">Background</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    By request of my advisor, I will cover the basics of LLMs prior to the innovations in the Deepseek lineage. For those familiar with LLMs, please skip this section.
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>454</fr:anchor><fr:addr
type="user">kak-004J</fr:addr><fr:route>kak-004J.xml</fr:route><fr:title
text="Word Embeddings">Word Embeddings</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>3</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tex
display="block"><![CDATA[\mathrm {Tok}(\mathbf {x})=\begin {bmatrix} 132\\ 17 \\ 87\\ 83\\ 184\end {bmatrix}]]></fr:tex></fr:mainmatter><fr:backmatter /></fr:tree>
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>455</fr:anchor><fr:addr
type="user">kak-004G</fr:addr><fr:route>kak-004G.xml</fr:route><fr:title
text="Self-Attention">Self-Attention</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>TLDR: Learned weighting of token embeddings. Essentially, learning which words to "attend" to in the input sequence. Have matrices
<fr:tex
display="inline"><![CDATA[\mathbf {Q} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {q}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {q}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_q}]]></fr:tex>, 
<fr:tex
display="inline"><![CDATA[ \mathbf {K} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {k}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {k}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_k} ]]></fr:tex>
<fr:tex
display="inline"><![CDATA[ \mathbf {V} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_v} ]]></fr:tex></fr:p><fr:p><fr:strong>Intuition 1:</fr:strong> Convex re-weighting of input tokens.
  Note that
<fr:tex
display="block"><![CDATA[ \begin {align*}   \begin {bmatrix}     p_1 & p_2 & p_3   \end {bmatrix} \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(2)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(3)} & \text {---}   \end {bmatrix}   \end {bmatrix} = p_1 \mathbf {v}^{(1)} + p_2 \mathbf {v}^{(2)} + p_3 \mathbf {v}^{(3)} \end {align*}   ]]></fr:tex>
<fr:tex
display="block"><![CDATA[ \begin {align*}   \begin {bmatrix}     p_{11} & 0 & 0 \\     p_{21} & p_{22} & 0 \\     p_{31} & p_{32} & p_{33}   \end {bmatrix} \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(2)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(3)} & \text {---}   \end {bmatrix}   \end {bmatrix} =    \begin {bmatrix}   p_{11} \mathbf {v}^{(1)}  \\   p_{21} \mathbf {v}^{(1)} + p_{22} \mathbf {v}^{(2)} \\   p_{31} \mathbf {v}^{(1)} + p_{32} \mathbf {v}^{(2)} + p_{33} \mathbf {v}^{(3)}   \end {bmatrix} \end {align*}   ]]></fr:tex>
  <fr:strong>Intuition 2:</fr:strong> Context dependent re-weighting.
      If <fr:tex
display="inline"><![CDATA[\mathbf {p} = \mathbb {S}(\mathbf {Q} \mathbf {K}^T)]]></fr:tex> then
      <fr:tex
display="block"><![CDATA[         \begin {align*}           p_{ij} = \frac {\mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}{\sum _{j} \mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}         \end {align*}       ]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>457</fr:anchor><fr:addr
type="machine">#264</fr:addr><fr:route>unstable-264.xml</fr:route><fr:taxon>Example</fr:taxon><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    Suppose that <fr:tex
display="inline"><![CDATA[\mathbf {x} = \text {I play with the ball}]]></fr:tex>. Then 
<fr:tex
display="block"><![CDATA[     \begin {align*}       \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``ball"})     \end {align*}   ]]></fr:tex>
  A feasible query for "ball" would be a verb describing the action of the ball, so maybe
  <fr:tex
display="block"><![CDATA[   \begin {align*}       W_q \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``play"})   \end {align*}   ]]></fr:tex>
  and a key for "play" would be what you are playing with like a ball, so 
  <fr:tex
display="block"><![CDATA[   \begin {align*}       W_k \mathbf {x}^{(2)} = \mathrm {Embed}(\text {``ball"})   \end {align*}   ]]></fr:tex>
  i.e.
<fr:tex
display="block"><![CDATA[   \begin {align*}     \mathrm {Query}(\text {``quantum"}) \cdot  \mathrm {Key}(\text {``mechanics"}) \approx      ||\mathrm {Query}(\text {``quantum"})|| \cdot  ||\mathrm {Key}(\text {``mechanics"})||   \end {align*} ]]></fr:tex>

</fr:mainmatter><fr:backmatter /></fr:tree>
 
<fr:p><fr:tex
display="block"><![CDATA[   \begin {align*} \left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} &= \mathbb {S}\left (\begin {bmatrix} \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(1)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(2)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(3)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(4)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(5)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(6)} \end {bmatrix} \right ) \\ &= \begin {bmatrix} 0 & 0.2 & 0.3 & 0.5 & 0 & 0 \end {bmatrix}   \end {align*} ]]></fr:tex>
<fr:tex
display="block"><![CDATA[ \left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} \mathbf {V} = 0.2 \mathbf {v}^{(2)} + 0.3 \mathbf {v}^{(3)} + 0.5 \mathbf {v}^{(5)} ]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree>
    
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>459</fr:anchor><fr:addr
type="machine">#291</fr:addr><fr:route>unstable-291.xml</fr:route><fr:title
text="RLHF">RLHF</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
<fr:tex
display="block"><![CDATA[\mathrm {loss}\left (\phi \right )=E_{\left (x,y\right )\sim  D_{\pi _{\phi }^{\mathrm {RL}}}}\left [r_\theta (x,y)-\beta \log \left (\pi _{\phi }^{\mathrm {RL}}(y\mid  x)/\pi ^{\mathrm {SFT}}(y\mid  x)\right )\right ] + \gamma  E_{x\sim  D_{\mathrm {pretrain}}}\left [\log (\pi _{\phi }^{\mathrm {RL}}(x))\right ]]]></fr:tex>
      </fr:mainmatter><fr:backmatter /></fr:tree>
  

</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>461</fr:anchor><fr:addr
type="machine">#293</fr:addr><fr:route>unstable-293.xml</fr:route><fr:title
text="Deepseek v2">Deepseek v2</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    Paper 
  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>463</fr:anchor><fr:addr
type="machine">#294</fr:addr><fr:route>unstable-294.xml</fr:route><fr:title
text="Deepseek v3">Deepseek v3</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
TODO. Kinda wanna look into the architectural / training innovations from this paper.
  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>465</fr:anchor><fr:addr
type="machine">#298</fr:addr><fr:route>unstable-298.xml</fr:route><fr:title
text="Deepseek R1">Deepseek R1</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>467</fr:anchor><fr:addr
type="machine">#295</fr:addr><fr:route>unstable-295.xml</fr:route><fr:title
text="How is R1 different then previous iterations of models?">How is R1 different then previous iterations of models?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>In R1-Zero, they do <fr:strong>ZERO</fr:strong> SFT on the base model - directly apply reinforcement learning.</fr:li>
    <fr:li>Use PPO like policy optimization but do <fr:strong>NOT</fr:strong> learn a reward model.</fr:li>
    <fr:ul><fr:li>Use very simple reward: 
        <fr:ul><fr:li><fr:tex
display="inline"><![CDATA[+1]]></fr:tex> for correct answer</fr:li> 
          <fr:li><fr:tex
display="inline"><![CDATA[-0.5]]></fr:tex> for incorrect answer</fr:li> 
          <fr:li><fr:tex
display="inline"><![CDATA[-1]]></fr:tex> for inability to answer.</fr:li></fr:ul></fr:li></fr:ul></fr:ul>
</fr:mainmatter><fr:backmatter /></fr:tree>
  


<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>469</fr:anchor><fr:addr
type="user">kak-003X</fr:addr><fr:route>kak-003X.xml</fr:route><fr:title
text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link
type="local"
href="shao2024deepseekmathpushinglimitsmathematical.xml"
addr="shao2024deepseekmathpushinglimitsmathematical"
title="DeepSeekMath: Pushing the limits of mathematical reasoning in open language models">DeepSeekMath: Pushing the limits of mathematical reasoning in open language models</fr:link></fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>470</fr:anchor><fr:addr
type="machine">#288</fr:addr><fr:route>unstable-288.xml</fr:route><fr:title
text="Math">Math</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:tex
display="block"><![CDATA[     \begin {align*}     {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\     &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )     \end {align*}   ]]></fr:tex>
  where
  <fr:tex
display="block"><![CDATA[         \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1     ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex
display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex
display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree>


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>471</fr:anchor><fr:addr
type="machine">#296</fr:addr><fr:route>unstable-296.xml</fr:route><fr:title
text="Post-training">Post-training</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    <fr:ul><fr:li><fr:em>Reinforcement Learning for all Scenarios:</fr:em> Seems like they do RLHF after the pure RL stage.</fr:li>
        <fr:ul><fr:li>Do traditional helpfulness harmfulness RLHF with trained reward model.</fr:li></fr:ul></fr:ul>
  </fr:mainmatter><fr:backmatter /></fr:tree>
  



  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>473</fr:anchor><fr:addr
type="machine">#297</fr:addr><fr:route>unstable-297.xml</fr:route><fr:title
text="Distilling Models with R1">Distilling Models with R1</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>To distill, they do only SFT with R1 generated COT.</fr:li>
      <fr:li>They show that distillation outperforms doing pure RL approach on smaller model</fr:li>
      <fr:ul><fr:li>Seems contradictory to <fr:link
type="local"
href="zeng2025simplerl.xml"
addr="zeng2025simplerl"
title="7B model and 8K examples: Emerging reasoning with reinforcement learning is both effective and efficient">7B model and 8K examples: Emerging reasoning with reinforcement learning is both effective and efficient</fr:link></fr:li></fr:ul></fr:ul>
  </fr:mainmatter><fr:backmatter /></fr:tree>
  


</fr:mainmatter><fr:backmatter /></fr:tree>
  

  <html:hr
xmlns:html="http://www.w3.org/1999/xhtml" />
<html:script
xmlns:html="http://www.w3.org/1999/xhtml"
src="https://utteranc.es/client.js"
repo="kkanarios32/website-comments"
issue-term="pathname"
theme="boxy-light"
crossorigin="anonymous"
async="" /></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Context">Context</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2720</fr:anchor><fr:addr
type="user">kak-0002</fr:addr><fr:route>kak-0002.xml</fr:route><fr:title
text="Blog">Blog</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>29</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>This is my blog, in which I write about a variety of topics including computer science, mathematics, and more.</fr:p>


<fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>426</fr:anchor><fr:addr
type="user">kak-0005</fr:addr><fr:route>kak-0005.xml</fr:route><fr:title
text="Contrastive Reinforcement Learning">Contrastive Reinforcement Learning</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>29</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>In this blog post, we aim to demistify <fr:link
type="external"
href="gcrl"><fr:em>Contrastive Reinforcement Learning</fr:em></fr:link>. This term often gets thrown around in the dark inner circles of the reinforcement learning community. However, for those that are not familiar with contrastive learning, what does contrastive even mean? For those that are, how can reinforcement learning be contrastive? Throughout this blog post, we will answer these questions and many more.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>428</fr:anchor><fr:addr
type="user">kak-0009</fr:addr><fr:route>kak-0009.xml</fr:route><fr:title
text="Contrastive Learning">Contrastive Learning</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Prior to understanding contrastive reinforcement learning, it is important to have an at least rudimentary understanding of contrastive learning. Historically, contrastive learning has been used to learn representations. The fundamental idea behind contrastive learning is to encourage the representations of similar outputs to be similar in representation space.</fr:p><fr:p><fr:strong>Supervised setting:</fr:strong> For now, assume we are in the supervised setting (we have access to lables). Suppose that we are learning a representation in <fr:tex
display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Our model is a classifier on dogs and cats. If we have two dogs <fr:tex
display="inline"><![CDATA[y_1]]></fr:tex> and <fr:tex
display="inline"><![CDATA[y_2]]></fr:tex> then we want the learned representation map <fr:tex
display="block"><![CDATA[\phi : \{\text {dogs}, \text {cats}\} \to  \mathbb {R}^d]]></fr:tex> to be such that <fr:tex
display="inline"><![CDATA[\phi (y_1)]]></fr:tex> and <fr:tex
display="inline"><![CDATA[\phi (y_2)]]></fr:tex> are "close" in <fr:tex
display="inline"><![CDATA[\mathbb {R}^d]]></fr:tex>. Now the notion of "close" is to be determined by the user. An example could be to minimize the inner product between their representation maps i.e. we could learn a feature map parametrized by <fr:tex
display="inline"><![CDATA[\theta ]]></fr:tex> with the following objective <fr:tex
display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle .]]></fr:tex> Similarly, we want dissimilar outputs to be far apart in representation space. If <fr:tex
display="inline"><![CDATA[y_3]]></fr:tex> is a cat, then we can introduce a regularization to encourage this i.e.
<fr:tex
display="block"><![CDATA[\max _{\theta }\ \langle  \phi _{\theta }(y_1), \phi _{\theta }(y_2) \rangle  - \sum _{i \in  \{1, 2\}} \langle  \phi _{\theta }(y_i), \phi _{\theta }(y_3) \rangle .]]></fr:tex></fr:p><fr:p><fr:strong>Unsupervised setting:</fr:strong> Now suppose that we get rid of labels and are just given <fr:tex
display="inline"><![CDATA[n]]></fr:tex> dog samples <fr:tex
display="inline"><![CDATA[\mathcal {D}]]></fr:tex> from some distribution <fr:tex
display="inline"><![CDATA[p_{\mathcal {D}}]]></fr:tex>. We now want to be able to learn <fr:tex
display="inline"><![CDATA[p_{\theta }]]></fr:tex> to somehow estimate this distribution. An approach is to learn to distinguish the sample dogs given from random noise. To do so, we generate <fr:tex
display="inline"><![CDATA[n]]></fr:tex> random images <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex> according to some distribution <fr:tex
display="inline"><![CDATA[p_{\mathcal {R}}]]></fr:tex>. We can now return to the supervised learning setting, where we treat <fr:tex
display="inline"><![CDATA[\mathcal {D}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex> as two classes. If we recall standard supervised learning practice, given a sample <fr:tex
display="inline"><![CDATA[x]]></fr:tex>, we then want to find <fr:tex
display="block"><![CDATA[p(\mathcal {D} \mid  x) = 1 - p(\mathcal {R} \mid  X).]]></fr:tex> 
As an explicit example, we will use logistic regression. Namely, we will model <fr:tex
display="inline"><![CDATA[p(x) = p(\mathcal {D} \mid  x)]]></fr:tex> as <fr:tex
display="block"><![CDATA[p_{\theta }(x) = \frac {1}{1 + e^{-G_{\theta }(x)}}.]]></fr:tex> However, <fr:tex
display="inline"><![CDATA[p_{\theta }(x)]]></fr:tex> is estimating <fr:tex
display="inline"><![CDATA[p(\mathcal {D} \mid  x)]]></fr:tex>, where we care about <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex>. To estimate the correct quantity, we need to leverage our knowledge of the noise distribution. Recall that if <fr:tex
display="inline"><![CDATA[p_{\theta }(x) = p(\mathcal {D} \mid  x)]]></fr:tex> then <fr:tex
display="inline"><![CDATA[G_{\theta }(x) = \log  \frac {p(x \mid  \mathcal {D})}{p(x \mid  \mathcal {R})}]]></fr:tex>. Since we generated the samples from <fr:tex
display="inline"><![CDATA[\mathcal {R}]]></fr:tex>, we have the explicit distribution i.e. <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {R}) = p_{\mathcal {R}}(x)]]></fr:tex>. Therefore, we can restrict <fr:tex
display="inline"><![CDATA[G_{\theta }]]></fr:tex> to explicitly learn <fr:tex
display="inline"><![CDATA[p(x \mid  \mathcal {D})]]></fr:tex> by considering <fr:tex
display="block"><![CDATA[G_{\theta }(x) = \log  p_{\theta }(x \mid  \mathcal {D}) - \log  p_{\mathcal {R}}(x),]]></fr:tex> considering the cross entropy loss we get the <fr:link
type="external"
href="nce">NCE loss</fr:link></fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>429</fr:anchor><fr:addr
type="user">kak-000E</fr:addr><fr:route>kak-000E.xml</fr:route><fr:title
text="NCE loss">NCE loss</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em><fr:link
type="external"
href="nce">NCE</fr:link></fr:em> loss aims to minimize the following objective <fr:tex
display="block"><![CDATA[\mathcal {L}_{N} = - \sum _{t} \log  \left [h(x_t; \theta )\right ] + \log \left [1 - h(y_t; \theta )\right ],]]></fr:tex> where <fr:tex
display="inline"><![CDATA[x_t]]></fr:tex> are samples from the data distribution and <fr:tex
display="inline"><![CDATA[y_t]]></fr:tex> are randomly generated samples and <fr:tex
display="block"><![CDATA[\begin {array}{r c l}{{h({\bf  u};\theta )}}&{{=}}&{{\frac {1}{1+\exp \left [-G({\bf  u};\theta )\right ]},}}\\ {{G({\bf  u};\theta )}}&{{=}}&{{\ln  p_{m}({\bf  u};\theta )-\ln  p_{n}({\bf  u}).}}\end {array}]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>In <fr:link
type="local"
href="gutmann2012.xml"
addr="gutmann2012"
title="Noise-contrastive Estimation">Noise-contrastive Estimation</fr:link>, they show under mild conditions that the estimator <fr:tex
display="inline"><![CDATA[p_{\theta }(x \mid  D) \to  p_{\mathcal {D}}(x)]]></fr:tex> in probability as the number of samples in the loss goes to infinity. Equivalently, the estimator is <fr:link
type="local"
href="kak-000F.xml"
addr="kak-000F"
title="Consistent estimator">consistent</fr:link>.</fr:p><fr:p><fr:strong>Time series:</fr:strong> Before we get to contrastive RL, it is a natural question to wonder how does this apply to temporal sequences? Concretely, we want to make predictions about the future given the current "context". However, we want to do so in an unsupervised way, meaning we are only given trajectories not a notion of what it means for a trajectory to be good. Naively, one can try to do this in a supervised manner. For a <fr:tex
display="inline"><![CDATA[k]]></fr:tex> step prediction, this would just be your model predicting what will happen in <fr:tex
display="inline"><![CDATA[k]]></fr:tex> steps then seeing if it matches what occured <fr:tex
display="inline"><![CDATA[k]]></fr:tex> steps in the future in the sample trajectory. However, if your sample space <fr:tex
display="inline"><![CDATA[\mathcal {X}]]></fr:tex> is very high-dimensional, modeling this relationship can require an exorbinant amount of trajectories.</fr:p><fr:p>Fast forwarding to contrastive RL, current work is primarily considered with a particular contrastive objective.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>430</fr:anchor><fr:addr
type="user">kak-000B</fr:addr><fr:route>kak-000B.xml</fr:route><fr:title
text="InfoNCE">InfoNCE</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em><fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">InfoNCE</fr:link></fr:em> loss aims to minimize the following information-theoretic objective <fr:tex
display="block"><![CDATA[\mathcal {L}_{N} = - \mathbb {E}_{\mathcal {X}} \left [\log  \frac {f_k(x_{t + k}, c_t)}{\sum _{x_j \in  \mathcal {X}} f_k(x_j, c_t)}\right ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>Now we need to unpack this very ominous loss. To start, what are <fr:tex
display="inline"><![CDATA[x_k]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_t]]></fr:tex>?</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>431</fr:anchor><fr:addr
type="user">kak-000C</fr:addr><fr:route>kak-000C.xml</fr:route><fr:title
text="Maximize Mutual info">Maximize Mutual info</fr:title><fr:taxon>Theorem</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p><fr:tex
display="inline"><![CDATA[\mathcal {L}_N]]></fr:tex> from <fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">Contrastive Predictive Decoding</fr:link> maximizes a lower bound on the <fr:link
type="local"
href="kak-000V.xml"
addr="kak-000V"
title="Mutual Information">Mutual Information</fr:link> between <fr:tex
display="inline"><![CDATA[x_{t + k}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_t]]></fr:tex>.</fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>432</fr:anchor><fr:addr
type="machine">#367</fr:addr><fr:route>unstable-367.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>All we must do is plug <fr:tex
display="inline"><![CDATA[\frac {p(x \mid  c)}{p(x)}]]></fr:tex> back into the objective.
  <fr:tex
display="block"><![CDATA[\begin {align*}     \mathcal {L}_{\mathbb {N}}^{\text {opt}}&=-\,\mathbb {E}\log \left [\frac {\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}}{\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}+\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &\approx \mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}(N-1)\,\mathbb {E}\,\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k} \mid  c_t)}N\right ] \\     &\geq  \mathbb {E} \log  \left [\frac {p(x_{t + k})}{p(x_{t + k} \mid  c_t)}N \right ] \\     &= - I(x_{t + k}, c_t) + \log  N  \end {align*}   ]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree><fr:tex
display="block"><![CDATA[ \operatorname *{max}_{f(u,v)}\mathbb {E}_{(u,v^{+})\sim  p(u,v)}\left [\log \sigma (\underbrace {f(u,{\green  v^{+}})}_{\phi (u)^{T}\psi ({\green  v^{+}})})+\log (1-\sigma (\underbrace {f(u,{\red  v^{-}})}_{\phi (u)^{T}\psi ({\red  v^{-}})}))\right ] ]]></fr:tex><fr:tex
display="block"><![CDATA[ \begin {align*} &\operatorname *{max}_{f}\mathbb {E}_{(s,a)\sim  p(s,a),s_{f}^{-}\sim  p(s_{f})}\left [\mathcal {L}(s,a,s_{f}^{+},s_{f}^{-})\right ] \\ \end {align*} ]]></fr:tex><fr:tex
display="block"><![CDATA[ \mathcal {L}_1(\theta ) = \log \sigma (f_{\theta }(s_1,a_1,{\color {green} s_{8}})) + \log (1-\sigma (f_{\theta }(s_1,a_1, {\color {red} s_3}))) ]]></fr:tex><fr:tex
display="block"><![CDATA[ \begin {align*} \widehat {\mathcal {L}}(\theta ) &= \frac {1}{n} \sum _{i = 1}^{n} \mathcal {L}_i \\ &= \frac {1}{n} \sum _{i = 1}^{n} \Big [\log \sigma (f_{\theta }(s_i,a_i,{\color {green} s_{f}^{+}})) + \log (1-\sigma (f_{\theta }(s_i,a_i, {\color {red} s_{f}^{-}})))\Big ] \end {align*} ]]></fr:tex><fr:tex
display="block"><![CDATA[ \mathcal {L}(\theta ) = \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] ]]></fr:tex><fr:tex
display="block"><![CDATA[f^*(s, a, s_g) = \log \left (\frac {p^{\pi (\cdot  \mid  \cdot )}(s_g \mid  s, a)}{p(s_g)}\right )]]></fr:tex>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>433</fr:anchor><fr:addr
type="machine">#368</fr:addr><fr:route>unstable-368.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    We want to maximize
<fr:tex
display="block"><![CDATA[     \begin {align*} \mathcal {L}(\theta ) &= \mathbb {E}_{x \sim  p_X, y \sim  p_Y}\Big [\log \sigma (f_{\theta }(x)) + \log (1-\sigma (f_{\theta }(y)))\Big ] \\ &= \int  \log \sigma (f_{\theta }(x)) P_X(x) + \int  \log (1-\sigma (f_{\theta }(y))) P_Y(y) \\ &= \int  \log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)     \end {align*}     ]]></fr:tex>
    Since we are maximizing <fr:tex
display="inline"><![CDATA[f(s)]]></fr:tex>, we can just maximize the integrand i.e.
<fr:tex
display="block"><![CDATA[       \begin {align*}         \frac {\mathrm {d}}{\mathrm {d}f(z)} \Big [\log \sigma (f_{\theta }(z)) P_X(z) + \log (1-\sigma (f_{\theta }(z))) P_Y(z)\Big ] = 0       \end {align*}     ]]></fr:tex>
    Solving,
    <fr:tex
display="block"><![CDATA[         \begin {align*}           P_X(z)\big (1 - \sigma (f(z))\big ) - P_Y(z)\sigma (f(z)) = 0 &\iff  \sigma (f(z)) = \frac {P_X(z)}{P_X(z) + P_Y(z)} \\           &\iff  f(z) = \log \left (\frac {P_X(z)}{P_Y(z)}\right )         \end {align*}       ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 


   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>434</fr:anchor><fr:addr
type="machine">#369</fr:addr><fr:route>unstable-369.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>31</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
The first step is to prove that the average Q-values are close to the task-conditioned Q-values. Below, we will use <fr:tex
display="inline"><![CDATA[R_{c}(\tau )\triangleq \sum _{\ell =0}^{\infty }\gamma ^{\ell }r_{\ell }(s_{\ell },a_{\ell })]]></fr:tex>:

<fr:tex
display="block"><![CDATA[ \begin {align*} \left |Q^{\beta (\cdot |\cdot ,a)}(s,a,e)-Q^{\beta (\cdot |\cdot ,\epsilon ^{\prime })}(s,a,e)\right |&=\left |\int \beta (\tau \mid  s,a,e)R_{e}(\tau )d\tau -\int \beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right |\\  &=\left |\int \beta (\tau \mid  s,a,e)-\beta (\tau \mid  s,a,e^{\prime })R_{e}(\tau )d\tau \right | \\ &=\left |\int \beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )R_{e}(\tau )d\tau \right | \\ &\leq \int \left |\beta (\tau \mid  s,a,e)\left (1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right )\right |d\tau \cdot \operatorname *{max}_{\tau }|R_{e}(\tau )d\tau | \\ &\leq \int \beta (\tau \mid  s,a,e)\left |1-\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}\right |d\tau \cdot  1 \\ &=\mathbb {E}_{\beta (\tau |s,a,e)}\left [\left |1-{\frac {\beta (\tau \mid  s,a,e^{\prime })}{\beta (\tau \mid  s,a,e)}}\right |\right ] \\ &\leq  \epsilon . \end {align*}   ]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree>
<fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>435</fr:anchor><fr:addr
type="user">kak-005F</fr:addr><fr:route>kak-005F.xml</fr:route><fr:title
text="Rebuilding My (Neo)Vim Config From Scratch">Rebuilding My (Neo)Vim Config From Scratch</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>I have been using LazyVim for some time now, but I have now run into issues multiple times where understanding how LazyVim is doing something is far more difficult than if I had written my own setup. I allocated one day for this adventure and really just wanted to make sure I had support for <fr:tex
display="inline"><![CDATA[\TeX ]]></fr:tex>, python, forester, and C/C++. Due to my (self-imposed) time constraint, I do not have the associated resources linked for each of the things discussed below. At some point, I hope to come back and more thoroughly cover each of the components.</fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>409</fr:anchor><fr:addr
type="machine">#244</fr:addr><fr:route>unstable-244.xml</fr:route><fr:title
text="Sane Defaults">Sane Defaults</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
To my surprise, a lot of the features that I had come to take for granted were actually options set up internally by Lazyvim. For example, I was shocked with 8 space indents!! and I could not even copy from one terminal instance to another... Due to this, I went and found all of the options I liked from Lazyvim and added them to my new configuration in <fr:code>configs/options.lua</fr:code>.
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>410</fr:anchor><fr:addr
type="machine">#245</fr:addr><fr:route>unstable-245.xml</fr:route><fr:title
text="Installing a Plugin Manager">Installing a Plugin Manager</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
For this, we will be using the defacto standard <fr:code>lazy.nvim</fr:code>. This is actually straightforward and kind of "just works". Just follow the installation guide in their documentation.
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>411</fr:anchor><fr:addr
type="machine">#249</fr:addr><fr:route>unstable-249.xml</fr:route><fr:title
text="Setting up Auto Complete">Setting up Auto Complete</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
This is one of the main motivations for me making the switch. It seems <fr:code>nvim-cmp</fr:code> has finally been replaced with a new <fr:code>blink.cmp</fr:code>, so that is what we will be using.


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>412</fr:anchor><fr:addr
type="machine">#246</fr:addr><fr:route>unstable-246.xml</fr:route><fr:title
text="Language Server Protocol">Language Server Protocol</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
It turns out there is a lot that goes into getting LSP setup correctly.
<fr:ol><fr:li>First we must actually install the language servers. To do this the easiest way, we use the <fr:code>mason.nvim</fr:code> and <fr:code>mason.nvim-lspconfig</fr:code> plugins. At some point, I might actually figure out how to set up lsp myself without lspconfig but that point is not now.</fr:li>
<fr:li>Through <fr:code>nvim-lspconfig</fr:code>, we can set up each of the servers we want to have LSP support. I just set up clangd, pyright, and texlab.</fr:li></fr:ol>
This was a bit ridiculous. The first of many challenges was around import resolution in python. To remedy this, I needed to write a function to find the virtual environment directory and then set the <fr:code>pythonPath</fr:code> to the venv python binary. Previously, I think I was just using pylsp and installing it as a pip package to each python venv. I much prefer the new way, and I think pyright is overall a much better lsp.
</fr:mainmatter><fr:backmatter /></fr:tree>
  


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>413</fr:anchor><fr:addr
type="machine">#247</fr:addr><fr:route>unstable-247.xml</fr:route><fr:title
text="Forester Completion">Forester Completion</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  Another necessary completion source for me is the one provided by <fr:code>forester.nvim</fr:code>. Similar to vimtex, the reference completion support is VERY useful. Obviously, I need completion when I am writing this blog!!! This was a little more involved. The first difficulty was that the completion source provided by the <fr:code>forester.nvim</fr:code> plugin was for <fr:code>nvim-cmp</fr:code>. It turns out this is a prevalent enough problem that the author of <fr:code>blink.cmp</fr:code> wrote an additional plugin <fr:code>blink.compat</fr:code> to allow for <fr:code>nvim-cmp</fr:code> completion sources. While this sounds all fine and good, <fr:code>nvim-compat</fr:code> expects plugins that return the completion source themselves, whereas in <fr:code>forester.nvim</fr:code> the completion source is just one submodule of a more feature-rich plugin. To get around this, I needed to look into the <fr:code>blink.compat</fr:code> code and find how they are registering the sources and just do it myself.
</fr:mainmatter><fr:backmatter /></fr:tree>
  



  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>414</fr:anchor><fr:addr
type="machine">#248</fr:addr><fr:route>unstable-248.xml</fr:route><fr:title
text="Snippets">Snippets</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:p>Going all the way back to the <fr:link
type="external"
href="https://castel.dev/post/lecture-notes-1/">Gilles Castel blog post</fr:link>, I have always been partial to snippets that auto-expand. I had them set up prior to Lazyvim but with Lazyvim I had resigned to using friendly-snippets with native nvim snippets. Since I was already redoing everything, this time around I decided not to compromise. Once upon a time (right when it came out I think?) I tried out Luasnips, but it seems that they now have far more extensive features. They are also natively supported by <fr:code>blink.cmp</fr:code>!! It feels necessary that I plug the <fr:link
type="external"
href="https://github.com/iurimateus/luasnip-latex-snippets.nvim">awesome repo</fr:link> that ports the original Ultisnips snippets to Luasnip. With this, I was able to easily add my own forester snippets!!!</fr:p>
  <fr:p>A fun little thing that I had been hoping to do for awhile and is finally now possible - I can load latex snippets when inside math environments in forester!!!!</fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>415</fr:anchor><fr:addr
type="machine">#250</fr:addr><fr:route>unstable-250.xml</fr:route><fr:title
text="Treesitter">Treesitter</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>416</fr:anchor><fr:addr
type="machine">#251</fr:addr><fr:route>unstable-251.xml</fr:route><fr:title
text="Formatters and Linters">Formatters and Linters</fr:title><fr:date><fr:year>2025</fr:year><fr:month>3</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:code>compat.nvim</fr:code>, <fr:code>mason.nvim</fr:code>, black, isort.
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree>

  <html:hr
xmlns:html="http://www.w3.org/1999/xhtml" />
<fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>436</fr:anchor><fr:addr
type="user">loading</fr:addr><fr:route>loading.xml</fr:route><fr:title
text="Coming Soon!!!">Coming Soon!!!</fr:title><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month><fr:day>29</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>438</fr:anchor><fr:addr
type="user">kak-005D</fr:addr><fr:route>kak-005D.xml</fr:route><fr:title
text="Optimization from a Deep Learning Perspective">Optimization from a Deep Learning Perspective</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>24</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>440</fr:anchor><fr:addr
type="user">kak-004X</fr:addr><fr:route>kak-004X.xml</fr:route><fr:title
text="A Note on Advantage Estimation">A Note on Advantage Estimation</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>442</fr:anchor><fr:addr
type="machine">#257</fr:addr><fr:route>unstable-257.xml</fr:route><fr:title
text="What is it?">What is it?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>444</fr:anchor><fr:addr
type="machine">#258</fr:addr><fr:route>unstable-258.xml</fr:route><fr:title
text="Why do we do it?">Why do we do it?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>446</fr:anchor><fr:addr
type="machine">#259</fr:addr><fr:route>unstable-259.xml</fr:route><fr:title
text="Generalized Advantage Estimation">Generalized Advantage Estimation</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>4</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>

  </fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>448</fr:anchor><fr:addr
type="user">kak-003Y</fr:addr><fr:route>kak-003Y.xml</fr:route><fr:title
text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Rough itinerary,
  <fr:ul><fr:li>Vanilla policy gradient
      <fr:ul><fr:li>Policy gradient theorem + proof</fr:li>
          <fr:li>Deterministic policy gradient theorem + (maybe)proof</fr:li></fr:ul></fr:li>
      <fr:li>Actor critic method
      <fr:ul><fr:li>A2C: Variance reduction method</fr:li>
        <fr:li>(Maybe) A3C: Asynchronous update</fr:li></fr:ul></fr:li>
      <fr:li>Trust region policy optimization</fr:li>
      <fr:li>Soft Actor Critic</fr:li>
      <fr:li><fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">Proximal Policy Optimization</fr:link></fr:li>
      <fr:li><fr:link
type="local"
href="kak-003X.xml"
addr="kak-003X"
title="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:link></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>450</fr:anchor><fr:addr
type="user">kak-003D</fr:addr><fr:route>kak-003D.xml</fr:route><fr:title
text="Deepseek v1 through R1: RL is back!">Deepseek v1 through R1: RL is back!</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>In this blog, we will aim to understand the key contributions of <fr:link
type="local"
href="deepseekai2025deepseekr1incentivizingreasoningcapability.xml"
addr="deepseekai2025deepseekr1incentivizingreasoningcapability"
title="DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning">DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning</fr:link>. It will serve as the complement to my group meeting presentation possibly consisting of more in-depth explanations. Time permitting, we might go over the engineering innovations introduced in <fr:link
type="local"
href="deepseekai2024deepseekv3technicalreport.xml"
addr="deepseekai2024deepseekv3technicalreport"
title="DeepSeek-V3 technical report">DeepSeek-V3 technical report</fr:link>.</fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>452</fr:anchor><fr:addr
type="machine">#292</fr:addr><fr:route>unstable-292.xml</fr:route><fr:title
text="Background">Background</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    By request of my advisor, I will cover the basics of LLMs prior to the innovations in the Deepseek lineage. For those familiar with LLMs, please skip this section.
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>454</fr:anchor><fr:addr
type="user">kak-004J</fr:addr><fr:route>kak-004J.xml</fr:route><fr:title
text="Word Embeddings">Word Embeddings</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>3</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tex
display="block"><![CDATA[\mathrm {Tok}(\mathbf {x})=\begin {bmatrix} 132\\ 17 \\ 87\\ 83\\ 184\end {bmatrix}]]></fr:tex></fr:mainmatter><fr:backmatter /></fr:tree>
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>455</fr:anchor><fr:addr
type="user">kak-004G</fr:addr><fr:route>kak-004G.xml</fr:route><fr:title
text="Self-Attention">Self-Attention</fr:title><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>TLDR: Learned weighting of token embeddings. Essentially, learning which words to "attend" to in the input sequence. Have matrices
<fr:tex
display="inline"><![CDATA[\mathbf {Q} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {q}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {q}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_q}]]></fr:tex>, 
<fr:tex
display="inline"><![CDATA[ \mathbf {K} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {k}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {k}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_k} ]]></fr:tex>
<fr:tex
display="inline"><![CDATA[ \mathbf {V} = \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \vdots  \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(n)} & \text {---}   \end {bmatrix} \end {bmatrix} \in  \mathbb {R}^{n \times  d_v} ]]></fr:tex></fr:p><fr:p><fr:strong>Intuition 1:</fr:strong> Convex re-weighting of input tokens.
  Note that
<fr:tex
display="block"><![CDATA[ \begin {align*}   \begin {bmatrix}     p_1 & p_2 & p_3   \end {bmatrix} \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(2)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(3)} & \text {---}   \end {bmatrix}   \end {bmatrix} = p_1 \mathbf {v}^{(1)} + p_2 \mathbf {v}^{(2)} + p_3 \mathbf {v}^{(3)} \end {align*}   ]]></fr:tex>
<fr:tex
display="block"><![CDATA[ \begin {align*}   \begin {bmatrix}     p_{11} & 0 & 0 \\     p_{21} & p_{22} & 0 \\     p_{31} & p_{32} & p_{33}   \end {bmatrix} \begin {bmatrix}   \begin {bmatrix}     \text {---} & \mathbf {v}^{(1)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(2)} & \text {---}   \end {bmatrix} \\   \begin {bmatrix}     \text {---} & \mathbf {v}^{(3)} & \text {---}   \end {bmatrix}   \end {bmatrix} =    \begin {bmatrix}   p_{11} \mathbf {v}^{(1)}  \\   p_{21} \mathbf {v}^{(1)} + p_{22} \mathbf {v}^{(2)} \\   p_{31} \mathbf {v}^{(1)} + p_{32} \mathbf {v}^{(2)} + p_{33} \mathbf {v}^{(3)}   \end {bmatrix} \end {align*}   ]]></fr:tex>
  <fr:strong>Intuition 2:</fr:strong> Context dependent re-weighting.
      If <fr:tex
display="inline"><![CDATA[\mathbf {p} = \mathbb {S}(\mathbf {Q} \mathbf {K}^T)]]></fr:tex> then
      <fr:tex
display="block"><![CDATA[         \begin {align*}           p_{ij} = \frac {\mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}{\sum _{j} \mathbf {q}^{(i)} \cdot  \mathbf {k}^{(j)}}         \end {align*}       ]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>457</fr:anchor><fr:addr
type="machine">#264</fr:addr><fr:route>unstable-264.xml</fr:route><fr:taxon>Example</fr:taxon><fr:date><fr:year>2025</fr:year><fr:month>2</fr:month><fr:day>1</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    Suppose that <fr:tex
display="inline"><![CDATA[\mathbf {x} = \text {I play with the ball}]]></fr:tex>. Then 
<fr:tex
display="block"><![CDATA[     \begin {align*}       \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``ball"})     \end {align*}   ]]></fr:tex>
  A feasible query for "ball" would be a verb describing the action of the ball, so maybe
  <fr:tex
display="block"><![CDATA[   \begin {align*}       W_q \mathbf {x}^{(5)} = \mathrm {Embed}(\text {``play"})   \end {align*}   ]]></fr:tex>
  and a key for "play" would be what you are playing with like a ball, so 
  <fr:tex
display="block"><![CDATA[   \begin {align*}       W_k \mathbf {x}^{(2)} = \mathrm {Embed}(\text {``ball"})   \end {align*}   ]]></fr:tex>
  i.e.
<fr:tex
display="block"><![CDATA[   \begin {align*}     \mathrm {Query}(\text {``quantum"}) \cdot  \mathrm {Key}(\text {``mechanics"}) \approx      ||\mathrm {Query}(\text {``quantum"})|| \cdot  ||\mathrm {Key}(\text {``mechanics"})||   \end {align*} ]]></fr:tex>

</fr:mainmatter><fr:backmatter /></fr:tree>
 
<fr:p><fr:tex
display="block"><![CDATA[   \begin {align*} \left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} &= \mathbb {S}\left (\begin {bmatrix} \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(1)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(2)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(3)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(4)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(5)} & \mathbf {q}^{(4)} \cdot  \mathbf {k}^{(6)} \end {bmatrix} \right ) \\ &= \begin {bmatrix} 0 & 0.2 & 0.3 & 0.5 & 0 & 0 \end {bmatrix}   \end {align*} ]]></fr:tex>
<fr:tex
display="block"><![CDATA[ \left [\mathbb {S}(\mathbf {Q}\mathbf {K}^T)\right ]_{4} \mathbf {V} = 0.2 \mathbf {v}^{(2)} + 0.3 \mathbf {v}^{(3)} + 0.5 \mathbf {v}^{(5)} ]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree>
    
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>459</fr:anchor><fr:addr
type="machine">#291</fr:addr><fr:route>unstable-291.xml</fr:route><fr:title
text="RLHF">RLHF</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
<fr:tex
display="block"><![CDATA[\mathrm {loss}\left (\phi \right )=E_{\left (x,y\right )\sim  D_{\pi _{\phi }^{\mathrm {RL}}}}\left [r_\theta (x,y)-\beta \log \left (\pi _{\phi }^{\mathrm {RL}}(y\mid  x)/\pi ^{\mathrm {SFT}}(y\mid  x)\right )\right ] + \gamma  E_{x\sim  D_{\mathrm {pretrain}}}\left [\log (\pi _{\phi }^{\mathrm {RL}}(x))\right ]]]></fr:tex>
      </fr:mainmatter><fr:backmatter /></fr:tree>
  

</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>461</fr:anchor><fr:addr
type="machine">#293</fr:addr><fr:route>unstable-293.xml</fr:route><fr:title
text="Deepseek v2">Deepseek v2</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    Paper 
  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>463</fr:anchor><fr:addr
type="machine">#294</fr:addr><fr:route>unstable-294.xml</fr:route><fr:title
text="Deepseek v3">Deepseek v3</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
TODO. Kinda wanna look into the architectural / training innovations from this paper.
  </fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>465</fr:anchor><fr:addr
type="machine">#298</fr:addr><fr:route>unstable-298.xml</fr:route><fr:title
text="Deepseek R1">Deepseek R1</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>467</fr:anchor><fr:addr
type="machine">#295</fr:addr><fr:route>unstable-295.xml</fr:route><fr:title
text="How is R1 different then previous iterations of models?">How is R1 different then previous iterations of models?</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>In R1-Zero, they do <fr:strong>ZERO</fr:strong> SFT on the base model - directly apply reinforcement learning.</fr:li>
    <fr:li>Use PPO like policy optimization but do <fr:strong>NOT</fr:strong> learn a reward model.</fr:li>
    <fr:ul><fr:li>Use very simple reward: 
        <fr:ul><fr:li><fr:tex
display="inline"><![CDATA[+1]]></fr:tex> for correct answer</fr:li> 
          <fr:li><fr:tex
display="inline"><![CDATA[-0.5]]></fr:tex> for incorrect answer</fr:li> 
          <fr:li><fr:tex
display="inline"><![CDATA[-1]]></fr:tex> for inability to answer.</fr:li></fr:ul></fr:li></fr:ul></fr:ul>
</fr:mainmatter><fr:backmatter /></fr:tree>
  


<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>469</fr:anchor><fr:addr
type="user">kak-003X</fr:addr><fr:route>kak-003X.xml</fr:route><fr:title
text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link
type="local"
href="shao2024deepseekmathpushinglimitsmathematical.xml"
addr="shao2024deepseekmathpushinglimitsmathematical"
title="DeepSeekMath: Pushing the limits of mathematical reasoning in open language models">DeepSeekMath: Pushing the limits of mathematical reasoning in open language models</fr:link></fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>470</fr:anchor><fr:addr
type="machine">#288</fr:addr><fr:route>unstable-288.xml</fr:route><fr:title
text="Math">Math</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:tex
display="block"><![CDATA[     \begin {align*}     {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\     &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )     \end {align*}   ]]></fr:tex>
  where
  <fr:tex
display="block"><![CDATA[         \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1     ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex
display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex
display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree>


  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>471</fr:anchor><fr:addr
type="machine">#296</fr:addr><fr:route>unstable-296.xml</fr:route><fr:title
text="Post-training">Post-training</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
    <fr:ul><fr:li><fr:em>Reinforcement Learning for all Scenarios:</fr:em> Seems like they do RLHF after the pure RL stage.</fr:li>
        <fr:ul><fr:li>Do traditional helpfulness harmfulness RLHF with trained reward model.</fr:li></fr:ul></fr:ul>
  </fr:mainmatter><fr:backmatter /></fr:tree>
  



  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>473</fr:anchor><fr:addr
type="machine">#297</fr:addr><fr:route>unstable-297.xml</fr:route><fr:title
text="Distilling Models with R1">Distilling Models with R1</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>27</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>To distill, they do only SFT with R1 generated COT.</fr:li>
      <fr:li>They show that distillation outperforms doing pure RL approach on smaller model</fr:li>
      <fr:ul><fr:li>Seems contradictory to <fr:link
type="local"
href="zeng2025simplerl.xml"
addr="zeng2025simplerl"
title="7B model and 8K examples: Emerging reasoning with reinforcement learning is both effective and efficient">7B model and 8K examples: Emerging reasoning with reinforcement learning is both effective and efficient</fr:link></fr:li></fr:ul></fr:ul>
  </fr:mainmatter><fr:backmatter /></fr:tree>
  


</fr:mainmatter><fr:backmatter /></fr:tree>
  

  <html:hr
xmlns:html="http://www.w3.org/1999/xhtml" />
<html:script
xmlns:html="http://www.w3.org/1999/xhtml"
src="https://utteranc.es/client.js"
repo="kkanarios32/website-comments"
issue-term="pathname"
theme="boxy-light"
crossorigin="anonymous"
async="" /></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>