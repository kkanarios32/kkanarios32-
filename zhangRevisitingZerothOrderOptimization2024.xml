<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2836</fr:anchor><fr:addr
type="user">zhangRevisitingZerothOrderOptimization2024</fr:addr><fr:route>zhangRevisitingZerothOrderOptimization2024.xml</fr:route><fr:title
text="Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>5</fr:month></fr:date><fr:authors><fr:author>Yihua Zhang</fr:author><fr:author>Pingzhi Li</fr:author><fr:author>Junyuan Hong</fr:author><fr:author>Jiaxiang Li</fr:author><fr:author>Yimeng Zhang</fr:author><fr:author>Wenqing Zheng</fr:author><fr:author>Pin-Yu Chen</fr:author><fr:author>Jason D. Lee</fr:author><fr:author>Wotao Yin</fr:author><fr:author>Mingyi Hong</fr:author><fr:author>Zhangyang Wang</fr:author><fr:author>Sijia Liu</fr:author><fr:author>Tianlong Chen</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2402.11592</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2402.11592</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{zhangRevisitingZerothOrderOptimization2024,
 title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}: {{A Benchmark}}},
 author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},
 year = {2024},
 doi = {10.48550/arXiv.2402.11592},
 urldate = {2024-09-08},
 number = {arXiv:2402.11592},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A Benchmark.pdf},
 keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow \{in size\}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .},
 primaryclass = {cs},
 eprint = {2402.11592},
 month = {May},
 shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>