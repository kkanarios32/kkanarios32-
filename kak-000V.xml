<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2460</fr:anchor><fr:addr
type="user">kak-000V</fr:addr><fr:route>kak-000V.xml</fr:route><fr:title
text="Mutual Information">Mutual Information</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>21</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em>mutual information</fr:em> between two random variables <fr:tex
display="inline"><![CDATA[X, Y]]></fr:tex> is defined with respect to their joint distribution. Namely, it is the <fr:link
type="local"
href="kak-000W.xml"
addr="kak-000W"
title="Kullback-Liebler Divergence">Kullback-Liebler Divergence</fr:link> between the joint distribution <fr:tex
display="inline"><![CDATA[p(x,y)]]></fr:tex> and the product of the marginals <fr:tex
display="inline"><![CDATA[p(x)p(y)]]></fr:tex> i.e. 
<fr:tex
display="block"><![CDATA[H(X || Y) = \mathbb {E}_{x,y \sim  p(x,y)}\left [\log  \frac {p(x, y)}{p(x)p(y)}\right ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Backlinks">Backlinks</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2461</fr:anchor><fr:addr
type="user">kak-000C</fr:addr><fr:route>kak-000C.xml</fr:route><fr:title
text="Maximize Mutual info">Maximize Mutual info</fr:title><fr:taxon>Theorem</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p><fr:tex
display="inline"><![CDATA[\mathcal {L}_N]]></fr:tex> from <fr:link
type="local"
href="vandenOord2018.xml"
addr="vandenOord2018"
title="Contrastive Predictive Decoding">Contrastive Predictive Decoding</fr:link> maximizes a lower bound on the <fr:link
type="local"
href="kak-000V.xml"
addr="kak-000V"
title="Mutual Information">Mutual Information</fr:link> between <fr:tex
display="inline"><![CDATA[x_{t + k}]]></fr:tex> and <fr:tex
display="inline"><![CDATA[c_t]]></fr:tex>.</fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>432</fr:anchor><fr:addr
type="machine">#367</fr:addr><fr:route>unstable-367.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>4</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>All we must do is plug <fr:tex
display="inline"><![CDATA[\frac {p(x \mid  c)}{p(x)}]]></fr:tex> back into the objective.
  <fr:tex
display="block"><![CDATA[\begin {align*}     \mathcal {L}_{\mathbb {N}}^{\text {opt}}&=-\,\mathbb {E}\log \left [\frac {\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}}{\frac {p(x_{t+k}|c_{t})}{p(x_{t+k})}+\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}\sum _{x_{j}\in  X_{\text {neg}}}\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &\approx \mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k}|c_{t})}(N-1)\,\mathbb {E}\,\frac {p(x_{j}|c_{t})}{p(x_{j})}\right ] \\     &=\mathbb {E}\log \left [1+\frac {p(x_{t+k})}{p(x_{t+k} \mid  c_t)}N\right ] \\     &\geq  \mathbb {E} \log  \left [\frac {p(x_{t + k})}{p(x_{t + k} \mid  c_t)}N \right ] \\     &= - I(x_{t + k}, c_t) + \log  N  \end {align*}   ]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Related">Related</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2462</fr:anchor><fr:addr
type="user">kak-000W</fr:addr><fr:route>kak-000W.xml</fr:route><fr:title
text="Kullback-Liebler Divergence">Kullback-Liebler Divergence</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>21</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>The <fr:em>KL-divergence</fr:em> between two probability distributions is defined as 
<fr:tex
display="block"><![CDATA[\mathrm {KL}(p || q) = \mathbb {E}_{x \sim  p}\left [\log  \frac {p(x)}{q(x)}\right ] = \sum _{x \in  \mathcal {X}} p(x) \log  \frac {p(x)}{q(x)}]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>