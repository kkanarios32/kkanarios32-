<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2914</fr:anchor><fr:addr
type="user">gomezProperLaplacianRepresentation2024</fr:addr><fr:route>gomezProperLaplacianRepresentation2024.xml</fr:route><fr:title
text="Proper Laplacian Representation Learning">Proper Laplacian Representation Learning</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>4</fr:month></fr:date><fr:authors><fr:author>Diego Gomez</fr:author><fr:author>Michael Bowling</fr:author><fr:author>Marlos C. Machado</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/2310.10833</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{gomezProperLaplacianRepresentation2024,
 title = {Proper {{Laplacian Representation Learning}}},
 author = {Gomez, Diego and Bowling, Michael and Machado, Marlos C.},
 year = {2024},
 urldate = {2024-10-18},
 number = {arXiv:2310.10833},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/AGETEYXM/Gomez et al. - 2024 - Proper Laplacian Representation Learning.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.},
 primaryclass = {cs},
 eprint = {2310.10833},
 month = {April}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>