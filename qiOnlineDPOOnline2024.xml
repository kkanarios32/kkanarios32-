<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2815</fr:anchor><fr:addr
type="user">qiOnlineDPOOnline2024</fr:addr><fr:route>qiOnlineDPOOnline2024.xml</fr:route><fr:title
text="Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing">Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month></fr:date><fr:authors><fr:author>Biqing Qi</fr:author><fr:author>Pengfei Li</fr:author><fr:author>Fangyuan Li</fr:author><fr:author>Junqi Gao</fr:author><fr:author>Kaiyan Zhang</fr:author><fr:author>Bowen Zhou</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2406.05534</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2406.05534</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{qiOnlineDPOOnline2024,
 title = {Online {{DPO}}: {{Online Direct Preference Optimization}} with {{Fast-Slow Chasing}}},
 author = {Qi, Biqing and Li, Pengfei and Li, Fangyuan and Gao, Junqi and Zhang, Kaiyan and Zhou, Bowen},
 year = {2024},
 doi = {10.48550/arXiv.2406.05534},
 urldate = {2024-09-08},
 number = {arXiv:2406.05534},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/74CS8AIC/Qi et al. - 2024 - Online DPO Online Direct Preference Optimization with Fast-Slow Chasing.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.},
 primaryclass = {cs},
 eprint = {2406.05534},
 month = {June},
 shorttitle = {Online {{DPO}}}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>