<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3159</fr:anchor><fr:addr
type="user">klissarovDeepLaplacianbasedOptions2023</fr:addr><fr:route>klissarovDeepLaplacianbasedOptions2023.xml</fr:route><fr:title
text="Deep Laplacian-based Options for Temporally-Extended Exploration">Deep Laplacian-based Options for Temporally-Extended Exploration</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2023</fr:year><fr:month>6</fr:month></fr:date><fr:authors><fr:author>Martin Klissarov</fr:author><fr:author>Marlos C. Machado</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/2301.11181</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{klissarovDeepLaplacianbasedOptions2023,
 title = {Deep {{Laplacian-based Options}} for {{Temporally-Extended Exploration}}},
 author = {Klissarov, Martin and Machado, Marlos C.},
 year = {2023},
 urldate = {2024-10-18},
 number = {arXiv:2301.11181},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/RJ3AYCQA/Klissarov and Machado - 2023 - Deep Laplacian-based Options for Temporally-Extended Exploration.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up options-based exploration. To do so, we introduce a fully online deep RL algorithm for discovering Laplacianbased options and evaluate our approach on a variety of pixel-based tasks. We compare to several state-of-the-art exploration methods and show that our approach is effective, general, and especially promising in non-stationary settings.},
 primaryclass = {cs},
 eprint = {2301.11181},
 month = {June}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>