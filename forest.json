{"kak-005F":{"title":"Rebuilding My (Neo)Vim Config From Scratch","taxon":null,"tags":[],"route":"kak-005F.xml","metas":{}},"kak-005D":{"title":"Optimization from a Deep Learning Perspective","taxon":null,"tags":["blog","upcoming"],"route":"kak-005D.xml","metas":{}},"kak-005A":{"title":"Achievability","taxon":"Definition","tags":[],"route":"kak-005A.xml","metas":{}},"kak-0059":{"title":"Coding system","taxon":"Definition","tags":[],"route":"kak-0059.xml","metas":{}},"kak-005C":{"title":"Entropy as MRSC","taxon":"Theorem","tags":[],"route":"kak-005C.xml","metas":{}},"kak-005B":{"title":"Minimum rate of source coding","taxon":"Definition","tags":[],"route":"kak-005B.xml","metas":{}},"kak-0052":{"title":null,"taxon":"Corollary","tags":[],"route":"kak-0052.xml","metas":{}},"kak-0055":{"title":"-Typical set","taxon":null,"tags":[],"route":"kak-0055.xml","metas":{}},"kak-0058":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-0058.xml","metas":{}},"kak-0054":{"title":"AEP Theorem","taxon":"Theorem","tags":[],"route":"kak-0054.xml","metas":{}},"kak-0053":{"title":"Continuous Mapping Theorem","taxon":"Theorem","tags":[],"route":"kak-0053.xml","metas":{}},"kak-0057":{"title":"Decomposition Theorem","taxon":"Theorem","tags":[],"route":"kak-0057.xml","metas":{}},"kak-0056":{"title":"High probability sets","taxon":"Corollary","tags":[],"route":"kak-0056.xml","metas":{}},"kak-0051":{"title":"Weak Law of Large Numbers","taxon":"Theorem","tags":[],"route":"kak-0051.xml","metas":{}},"kak-004Z":{"title":"Data Processing Inequality","taxon":"Theorem","tags":[],"route":"kak-004Z.xml","metas":{}},"kak-0050":{"title":"Fano's Inequality","taxon":null,"tags":[],"route":"kak-0050.xml","metas":{}},"kak-004Y":{"title":"Maximum Aposteriori Estimator","taxon":"Theorem","tags":[],"route":"kak-004Y.xml","metas":{}},"kak-004X":{"title":"A Note on Advantage Estimation","taxon":null,"tags":["blog","upcoming"],"route":"kak-004X.xml","metas":{}},"kak-004R":{"title":"Armijo Condition","taxon":"Definition","tags":[],"route":"kak-004R.xml","metas":{}},"kak-004P":{"title":"Convergence of accelerated GD","taxon":"Theorem","tags":[],"route":"kak-004P.xml","metas":{}},"kak-004T":{"title":"Convergence of GD with backtracking","taxon":"Theorem","tags":[],"route":"kak-004T.xml","metas":{}},"kak-004Q":{"title":"Exact Linesearch","taxon":"Definition","tags":[],"route":"kak-004Q.xml","metas":{}},"kak-004U":{"title":"Linear convergence for strong convexity","taxon":"Theorem","tags":[],"route":"kak-004U.xml","metas":{}},"kak-004O":{"title":"Nesterov's Method","taxon":null,"tags":[],"route":"kak-004O.xml","metas":{}},"kak-004V":{"title":"Newton's Method","taxon":null,"tags":[],"route":"kak-004V.xml","metas":{}},"kak-004W":{"title":"Quasi-Newton's Method","taxon":null,"tags":[],"route":"kak-004W.xml","metas":{}},"kak-004M":{"title":"Sublinear convergence of GD","taxon":"Theorem","tags":[],"route":"kak-004M.xml","metas":{}},"kak-004N":{"title":"Suboptimality of Gradient Descent","taxon":"Theorem","tags":[],"route":"kak-004N.xml","metas":{}},"kak-004L":{"title":"Sufficient Value","taxon":"Lemma","tags":[],"route":"kak-004L.xml","metas":{}},"kak-004S":{"title":"Wolfe condition","taxon":"Definition","tags":[],"route":"kak-004S.xml","metas":{}},"kak-004J":{"title":"Word Embeddings","taxon":null,"tags":[],"route":"kak-004J.xml","metas":{}},"kak-004K":{"title":"Forester2html","taxon":null,"tags":[],"route":"kak-004K.xml","metas":{}},"kak-004H":{"title":"Notes on Systems Programming","taxon":null,"tags":["note","unfinished","systems","systems"],"route":"kak-004H.xml","metas":{}},"kak-004I":{"title":"Virtual Memory","taxon":null,"tags":["memory"],"route":"kak-004I.xml","metas":{}},"kak-004C":{"title":"Convex set","taxon":"Definition","tags":[],"route":"kak-004C.xml","metas":{}},"kak-004D":{"title":"Convex Function","taxon":"Definition","tags":[],"route":"kak-004D.xml","metas":{}},"kak-004E":{"title":"Gibb's inequality","taxon":"Theorem","tags":[],"route":"kak-004E.xml","metas":{}},"kak-004B":{"title":"Information Theory","taxon":null,"tags":["note","unfinished","lost"],"route":"kak-004B.xml","metas":{}},"kak-004F":{"title":"Log-Sum Inequality","taxon":null,"tags":[],"route":"kak-004F.xml","metas":{}},"kak-004G":{"title":"Self-Attention","taxon":null,"tags":["llms"],"route":"kak-004G.xml","metas":{}},"kak-0041":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-0041.xml","metas":{}},"kak-0048":{"title":null,"taxon":"Definition","tags":[],"route":"kak-0048.xml","metas":{}},"kak-0049":{"title":null,"taxon":"Lemma","tags":[],"route":"kak-0049.xml","metas":{}},"kak-0042":{"title":"Chapman-Kolmogorov Equation","taxon":"Definition","tags":[],"route":"kak-0042.xml","metas":{}},"kak-0046":{"title":"Communicates","taxon":"Definition","tags":[],"route":"kak-0046.xml","metas":{}},"crlr1":{"title":"Contrastive Learning as Goal Conditioned RL + Deepseek R1","taxon":"Reference","tags":["talk","group"],"route":"crlr1.xml","metas":{"external":"Slides/clrl-r1.pdf"}},"kak-003X":{"title":"Group Relative Policy Optimization","taxon":null,"tags":["rl","llms"],"route":"kak-003X.xml","metas":{}},"kak-0040":{"title":"Markov Chain","taxon":"Definition","tags":[],"route":"kak-0040.xml","metas":{}},"kak-004A":{"title":"Old Talks","taxon":null,"tags":[],"route":"kak-004A.xml","metas":{}},"rlhf":{"title":"Reinforcement Learning from Human Feedback","taxon":"Reference","tags":["talk","group"],"route":"rlhf.xml","metas":{"external":"Slides/RLHF-Group-Meeting.pdf"}},"kak-003Z":{"title":"Stochastic Processes","taxon":null,"tags":["note","prob","unfinished"],"route":"kak-003Z.xml","metas":{}},"kak-0043":{"title":"Stopping time","taxon":"Definition","tags":[],"route":"kak-0043.xml","metas":{}},"kak-0044":{"title":"Strong Markov Property","taxon":"Theorem","tags":[],"route":"kak-0044.xml","metas":{}},"kak-003Y":{"title":"The History and Evolution of Policy Gradient Algorithms","taxon":null,"tags":["blog","upcoming","rl"],"route":"kak-003Y.xml","metas":{}},"kak-0045":{"title":"Transient and recurrent states","taxon":"Definition","tags":[],"route":"kak-0045.xml","metas":{}},"kak-0047":{"title":"Transitivity of communcation","taxon":"Lemma","tags":[],"route":"kak-0047.xml","metas":{}},"kak-003V":{"title":"Chain Rule of Mutual Info","taxon":"Theorem","tags":["infot"],"route":"kak-003V.xml","metas":{}},"kak-003W":{"title":"LLMs stuff","taxon":null,"tags":["note","lost"],"route":"kak-003W.xml","metas":{}},"kak-003U":{"title":"Threads on the hardware level","taxon":null,"tags":[],"route":"kak-003U.xml","metas":{}},"kak-003Q":{"title":"Q-convergence","taxon":"Definition","tags":[],"route":"kak-003Q.xml","metas":{}},"kak-003S":{"title":"R-convergence","taxon":"Definition","tags":[],"route":"kak-003S.xml","metas":{}},"kak-003D":{"title":"Deepseek v1 through R1: RL is back!","taxon":null,"tags":["blog","llms","upcoming"],"route":"kak-003D.xml","metas":{}},"kak-003N":{"title":"First-order approximation","taxon":"Theorem","tags":[],"route":"kak-003N.xml","metas":{}},"kak-003R":{"title":"First-order convergence","taxon":"Definition","tags":[],"route":"kak-003R.xml","metas":{}},"kak-003J":{"title":"Lipschitz Continous","taxon":"Definition","tags":[],"route":"kak-003J.xml","metas":{}},"kak-003G":{"title":"Matrix Operator Norm","taxon":"Definition","tags":[],"route":"kak-003G.xml","metas":{}},"kak-003L":{"title":"Mean Value Theorem I","taxon":"Theorem","tags":[],"route":"kak-003L.xml","metas":{}},"kak-003M":{"title":"Mean Value Theorem II","taxon":"Theorem","tags":[],"route":"kak-003M.xml","metas":{}},"kak-003F":{"title":"Misc.","taxon":null,"tags":["subject","unfinished"],"route":"kak-003F.xml","metas":{}},"kak-003I":{"title":"Schatten p-norm","taxon":"Definition","tags":[],"route":"kak-003I.xml","metas":{}},"kak-003O":{"title":"Second-order approximation","taxon":"Theorem","tags":[],"route":"kak-003O.xml","metas":{}},"kak-003K":{"title":"Smooth Function","taxon":"Definition","tags":[],"route":"kak-003K.xml","metas":{}},"kak-003P":{"title":"Stationary point","taxon":"Definition","tags":[],"route":"kak-003P.xml","metas":{}},"kak-003H":{"title":"Unitary Invariant Matrix Norm","taxon":"Definition","tags":[],"route":"kak-003H.xml","metas":{}},"kak-003C":{"title":"Best rank-r approximation","taxon":"Theorem","tags":[],"route":"kak-003C.xml","metas":{}},"kak-0034":{"title":"First-order condition","taxon":"Theorem","tags":["optimization"],"route":"kak-0034.xml","metas":{}},"kak-0033":{"title":"Optimization Theory","taxon":null,"tags":["lost","note","unfinished"],"route":"kak-0033.xml","metas":{}},"kak-003A":{"title":"Positive (semi)definiteness","taxon":"Definition","tags":[],"route":"kak-003A.xml","metas":{}},"kak-0035":{"title":"Second-order condition","taxon":"Theorem","tags":["optimization"],"route":"kak-0035.xml","metas":{}},"kak-003B":{"title":"Singular Value Decomposition","taxon":"Fact","tags":["optimization"],"route":"kak-003B.xml","metas":{}},"kak-0037":{"title":"Smooth Problem","taxon":"Definition","tags":["optimization"],"route":"kak-0037.xml","metas":{}},"kak-0036":{"title":"Strong convexity","taxon":"Definition","tags":["optimization"],"route":"kak-0036.xml","metas":{}},"kak-0039":{"title":"Subdifferential","taxon":"Definition","tags":["optimization"],"route":"kak-0039.xml","metas":{}},"kak-0038":{"title":"Subgradient","taxon":"Definition","tags":["optimization"],"route":"kak-0038.xml","metas":{}},"kak-0031":{"title":"Probability","taxon":null,"tags":["unfinished","subject"],"route":"kak-0031.xml","metas":{}},"kak-0030":{"title":"Reinforcement Learning","taxon":null,"tags":["unfinished","subject"],"route":"kak-0030.xml","metas":{}},"kak-0032":{"title":"Systems Programming","taxon":null,"tags":["subject","unfinished"],"route":"kak-0032.xml","metas":{}},"kak-002Z":{"title":null,"taxon":null,"tags":[],"route":"kak-002Z.xml","metas":{}},"kak-002T":{"title":"Soft Actor Critic","taxon":null,"tags":["rl"],"route":"kak-002T.xml","metas":{}},"kak-002U":{"title":"Unreasonable Effectiveness of Eligibility Traces","taxon":null,"tags":["blog","draft"],"route":"kak-002U.xml","metas":{}},"kak-002M":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-002M.xml","metas":{}},"kak-002Q":{"title":"Banach Space","taxon":null,"tags":[],"route":"kak-002Q.xml","metas":{}},"kak-002N":{"title":"Hoeffding's Inequality","taxon":"Theorem","tags":[],"route":"kak-002N.xml","metas":{}},"kak-002R":{"title":"Holder's Inequality","taxon":null,"tags":[],"route":"kak-002R.xml","metas":{}},"kak-002S":{"title":"Linear MDP","taxon":"Definition","tags":[],"route":"kak-002S.xml","metas":{}},"kak-002L":{"title":"Sub-Gaussian Random Variables","taxon":"Definition","tags":[],"route":"kak-002L.xml","metas":{}},"kak-002P":{"title":"Sub-Gaussian Norm","taxon":"Definition","tags":[],"route":"kak-002P.xml","metas":{}},"kak-002O":{"title":"Khinchin's Inequality","taxon":"Corollary","tags":[],"route":"kak-002O.xml","metas":{}},"zeng2025simplerl":{"title":"7B model and 8K examples: Emerging reasoning with reinforcement learning is both effective and efficient","taxon":"Reference","tags":[],"route":"zeng2025simplerl.xml","metas":{"external":"Https://hkust-nlp.notion.site/simplerl-reason","bibtex":"@Misc{zeng2025simplerl,\n title = {7B Model and 8K Examples: Emerging Reasoning with Reinforcement\nLearning is Both Effective and Efficient},\n author = {Weihao Zeng and Yuzhen Huang and Wei Liu and Keqing He and Qian Liu\nand Zejun Ma and Junxian He},\n year = {2025},\n howpublished = {\\url{https://hkust-nlp.notion.site/simplerl-reason}},\n note = {Notion Blog}\n}"}},"deepseekai2025deepseekr1incentivizingreasoningcapability":{"title":"DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning","taxon":"Reference","tags":[],"route":"deepseekai2025deepseekr1incentivizingreasoningcapability.xml","metas":{"external":"Https://arxiv.org/abs/2501.12948","bibtex":"@Misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n title = {DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\nReinforcement Learning},\n author = {DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and\nJunxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong\nMa and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and\nYu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and\nZiyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu\nand Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and\nChenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie\nJi and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and\nGuangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao\nand Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and\nHuazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and\nJiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and\nJunlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and\nKai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and\nKuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang\nand Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and\nMinghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and\nMingming Li and Ning Tian and Panpan Huang and Peng Zhang and\nQiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong\nZhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and\nRuyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and\nShengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and\nShuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and\nShengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and\nWangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and\nWenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An\nand Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie\nand Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang\nand Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and\nXiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and\nXiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and\nXinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang\nand Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui\nWang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and\nYing He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma\nand Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue\nGong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and\nYuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong\nXu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and\nYunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren\nand Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie\nand Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and\nZhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and\nZiwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng\nXu and Zhongyu Zhang and Zhen Zhang},\n year = {2025},\n url = {https://arxiv.org/abs/2501.12948},\n primaryclass = {cs.CL},\n archiveprefix = {arXiv},\n eprint = {2501.12948}\n}"}},"kak-002F":{"title":"Chebyshev's Inequality","taxon":"Theorem","tags":[],"route":"kak-002F.xml","metas":{}},"kak-002H":{"title":"Holder's Inequality","taxon":"Theorem","tags":[],"route":"kak-002H.xml","metas":{}},"kak-002G":{"title":"Jensen's Inequality","taxon":"Theorem","tags":[],"route":"kak-002G.xml","metas":{}},"kak-002E":{"title":"Markov's Inequality","taxon":"Theorem","tags":[],"route":"kak-002E.xml","metas":{}},"kak-001X":{"title":"-Field of RV","taxon":"Definition","tags":[],"route":"kak-001X.xml","metas":{}},"kak-0024":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-0024.xml","metas":{}},"kak-002A":{"title":"Kth Moment","taxon":"Definition","tags":[],"route":"kak-002A.xml","metas":{}},"kak-002C":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-002C.xml","metas":{}},"kak-002D":{"title":null,"taxon":null,"tags":[],"route":"kak-002D.xml","metas":{}},"kak-001Z":{"title":"Almost Sure Convergence","taxon":null,"tags":[],"route":"kak-001Z.xml","metas":{}},"kak-0020":{"title":"Convergence in Probability","taxon":null,"tags":[],"route":"kak-0020.xml","metas":{}},"kak-001Y":{"title":"Dense Subset","taxon":"Definition","tags":["math"],"route":"kak-001Y.xml","metas":{}},"kak-0023":{"title":"Distribution","taxon":"Definition","tags":[],"route":"kak-0023.xml","metas":{}},"kak-0029":{"title":"Expected Value","taxon":"Definition","tags":[],"route":"kak-0029.xml","metas":{}},"kak-0022":{"title":"Finite Sequence Measurability","taxon":"Theorem","tags":[],"route":"kak-0022.xml","metas":{}},"kak-0021":{"title":"Independence of Random Variable","taxon":"Definition","tags":[],"route":"kak-0021.xml","metas":{}},"kak-001W":{"title":"Simple Random Variable","taxon":"Definition","tags":[],"route":"kak-001W.xml","metas":{}},"kak-0025":{"title":"Stack vs. Heap","taxon":null,"tags":["memory"],"route":"kak-0025.xml","metas":{}},"kak-002B":{"title":"Uniformly Bounded","taxon":"Definition","tags":[],"route":"kak-002B.xml","metas":{}},"kak-001P":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-001P.xml","metas":{}},"kak-001S":{"title":null,"taxon":"Definition","tags":[],"route":"kak-001S.xml","metas":{}},"kak-001R":{"title":"Array Method","taxon":"Theorem","tags":[],"route":"kak-001R.xml","metas":{}},"kak-001Q":{"title":"Borel-Cantelli One","taxon":"Theorem","tags":[],"route":"kak-001Q.xml","metas":{}},"kak-001T":{"title":"Borel-Cantelli Two","taxon":"Theorem","tags":[],"route":"kak-001T.xml","metas":{}},"kak-001V":{"title":"Kolmogorov's Zero-one Law","taxon":"Theorem","tags":[],"route":"kak-001V.xml","metas":{}},"kak-001U":{"title":"Tail -field","taxon":"Definition","tags":[],"route":"kak-001U.xml","metas":{}},"kak-001O":{"title":"Independence","taxon":"Definition","tags":[],"route":"kak-001O.xml","metas":{}},"kak-001N":{"title":"Liminf of sets","taxon":"Definition","tags":[],"route":"kak-001N.xml","metas":{}},"kak-001M":{"title":"Limsup of sets","taxon":"Definition","tags":[],"route":"kak-001M.xml","metas":{}},"kak-001K":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-001K.xml","metas":{}},"kak-001I":{"title":"Chain Rule of Probability","taxon":"Proposition","tags":[],"route":"kak-001I.xml","metas":{}},"kak-001J":{"title":"Conditional Probability","taxon":"Definition","tags":[],"route":"kak-001J.xml","metas":{}},"klissarovMaestroMotifSkillDesign2024":{"title":"MaestroMotif: Skill Design from Artificial Intelligence Feedback","taxon":"Reference","tags":[],"route":"klissarovMaestroMotifSkillDesign2024.xml","metas":{"doi":"10.48550/ArXiv.2412.08542","external":"Https://arxiv.org/abs/2412.08542","bibtex":"@Misc{klissarovMaestroMotifSkillDesign2024,\n title = {{{MaestroMotif}}: {{Skill Design}} from {{Artificial Intelligence Feedback}}},\n author = {Klissarov, Martin and Henaff, Mikael and Raileanu, Roberta and Sodhani, Shagun and Vincent, Pascal and Zhang, Amy and Bacon, Pierre-Luc and Precup, Doina and Machado, Marlos C. and D'Oro, Pierluca},\n year = {2024},\n doi = {10.48550/arXiv.2412.08542},\n urldate = {2025-01-02},\n number = {arXiv:2412.08542},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Hierarchical RL/Klissarov et al_2024_MaestroMotif.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Describing skills in natural language has the potential to provide an accessible way to inject human knowledge about decision-making into an AI system. We present MaestroMotif, a method for AI-assisted skill design, which yields highperforming and adaptable agents. MaestroMotif leverages the capabilities of Large Language Models (LLMs) to effectively create and reuse skills. It first uses an LLM's feedback to automatically design rewards corresponding to each skill, starting from their natural language description. Then, it employs an LLM's code generation abilities, together with reinforcement learning, for training the skills and combining them to implement complex behaviors specified in language. We evaluate MaestroMotif using a suite of complex tasks in the NetHack Learning Environment (NLE), demonstrating that it surpasses existing approaches in both performance and usability.},\n primaryclass = {cs},\n eprint = {2412.08542},\n month = {December},\n shorttitle = {{{MaestroMotif}}}\n}"}},"zhengOnlineIntrinsicRewards2024":{"title":"Online Intrinsic Rewards for Decision Making Agents from Large Language Model Feedback","taxon":"Reference","tags":[],"route":"zhengOnlineIntrinsicRewards2024.xml","metas":{"doi":"10.48550/ArXiv.2410.23022","external":"Https://arxiv.org/abs/2410.23022","bibtex":"@Misc{zhengOnlineIntrinsicRewards2024,\n title = {Online {{Intrinsic Rewards}} for {{Decision Making Agents}} from {{Large Language Model Feedback}}},\n author = {Zheng, Qinqing and Henaff, Mikael and Zhang, Amy and Grover, Aditya and Amos, Brandon},\n year = {2024},\n doi = {10.48550/arXiv.2410.23022},\n urldate = {2025-01-02},\n number = {arXiv:2410.23022},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Hierarchical RL/Zheng et al_2024_Online Intrinsic Rewards for Decision Making Agents from Large Language Model.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Automatically synthesizing dense rewards from natural language descriptions is a promising paradigm in reinforcement learning (RL), with applications to sparse reward problems, open-ended exploration, and hierarchical skill design. Recent works have made promising steps by exploiting the prior knowledge of large language models (LLMs). However, these approaches suffer from important limitations: they are either not scalable to problems requiring billions of environment samples, due to requiring LLM annotations for each observation, or they require a diverse offline dataset, which may not exist or be impossible to collect. In this work, we address these limitations through a combination of algorithmic and systems-level contributions. We propose ONI, a distributed architecture that simultaneously learns an RL policy and an intrinsic reward function using LLM feedback. Our approach annotates the agent's collected experience via an asynchronous LLM server, which is then distilled into an intrinsic reward model. We explore a range of algorithmic choices for reward modeling with varying complexity, including hashing, classification, and ranking models. By studying their relative tradeoffs, we shed light on questions regarding intrinsic reward design for sparse reward problems. Our approach achieves state-of-the-art performance across a range of challenging, sparse reward tasks from the NetHack Learning Environment in a simple unified process, solely using the agent's gathered experience, without requiring external datasets. We make our code available at https://github.com/facebookresearch/oni.},\n primaryclass = {cs},\n eprint = {2410.23022},\n month = {December}\n}"}},"chungParsevalRegularizationContinual2024":{"title":"Parseval Regularization for Continual Reinforcement Learning","taxon":"Reference","tags":[],"route":"chungParsevalRegularizationContinual2024.xml","metas":{"doi":"10.48550/ArXiv.2412.07224","external":"Https://arxiv.org/abs/2412.07224","bibtex":"@Misc{chungParsevalRegularizationContinual2024,\n title = {Parseval {{Regularization}} for {{Continual Reinforcement Learning}}},\n author = {Chung, Wesley and Cherif, Lynn and Meger, David and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2412.07224},\n urldate = {2024-12-18},\n number = {arXiv:2412.07224},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Chung et al_2024_Parseval Regularization for Continual Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Loss of plasticity, trainability loss, and primacy bias have been identified as issues arising when training deep neural networks on sequences of tasks---all referring to the increased difficulty in training on new tasks. We propose to use Parseval regularization, which maintains orthogonality of weight matrices, to preserve useful optimization properties and improve training in a continual reinforcement learning setting. We show that it provides significant benefits to RL agents on a suite of gridworld, CARL and MetaWorld tasks. We conduct comprehensive ablations to identify the source of its benefits and investigate the effect of certain metrics associated to network trainability including weight matrix rank, weight norms and policy entropy.},\n primaryclass = {cs},\n eprint = {2412.07224},\n month = {December}\n}"}},"kak-0018":{"title":"-System","taxon":"Definition","tags":[],"route":"kak-0018.xml","metas":{}},"kak-0019":{"title":"-System","taxon":"Definition","tags":[],"route":"kak-0019.xml","metas":{}},"kak-001A":{"title":null,"taxon":"Lemma","tags":[],"route":"kak-001A.xml","metas":{}},"kak-001H":{"title":"Axiom of Choice","taxon":"Axiom","tags":[],"route":"kak-001H.xml","metas":{}},"kak-001F":{"title":"Completeness of a measure","taxon":"Definition","tags":["prog"],"route":"kak-001F.xml","metas":{}},"kak-001B":{"title":"Dynkin's {-}","taxon":"Theorem","tags":[],"route":"kak-001B.xml","metas":{}},"kak-001E":{"title":"Halmo's Monotone Class Theorem","taxon":"Theorem","tags":[],"route":"kak-001E.xml","metas":{}},"kak-001D":{"title":"Monotone Class","taxon":"Definition","tags":[],"route":"kak-001D.xml","metas":{}},"kak-001C":{"title":"Uniqueness of Extension","taxon":"Theorem","tags":[],"route":"kak-001C.xml","metas":{}},"kak-001G":{"title":"Vitali Sets","taxon":"Example","tags":[],"route":"kak-001G.xml","metas":{}},"kak-0016":{"title":null,"taxon":null,"tags":[],"route":"kak-0016.xml","metas":{}},"kak-0017":{"title":null,"taxon":null,"tags":[],"route":"kak-0017.xml","metas":{}},"kak-0014":{"title":"GPU Training Stuff","taxon":null,"tags":[],"route":"kak-0014.xml","metas":{}},"kak-0015":{"title":"Learning","taxon":null,"tags":[],"route":"kak-0015.xml","metas":{}},"kak-0013":{"title":"Mean Square Value Error","taxon":"Definition","tags":[],"route":"kak-0013.xml","metas":{}},"kak-0012":{"title":"Notes on Reinforcement Learning: An Introduction","taxon":null,"tags":["rl","note","unfinished"],"route":"kak-0012.xml","metas":{}},"kak-0011":{"title":"Action Value Function","taxon":"Definition","tags":[],"route":"kak-0011.xml","metas":{}},"kak-000Y":{"title":"Markov Decision Processes","taxon":null,"tags":[],"route":"kak-000Y.xml","metas":{}},"kak-000Z":{"title":"Markov Decision Process","taxon":"Definition","tags":[],"route":"kak-000Z.xml","metas":{}},"kak-0010":{"title":"Value Function","taxon":"Definition","tags":[],"route":"kak-0010.xml","metas":{}},"kak-000U":{"title":"P^*-measurable","taxon":"Definition","tags":[],"route":"kak-000U.xml","metas":{}},"kak-000W":{"title":"Kullback-Liebler Divergence","taxon":"Definition","tags":[],"route":"kak-000W.xml","metas":{}},"kak-000V":{"title":"Mutual Information","taxon":"Definition","tags":[],"route":"kak-000V.xml","metas":{}},"kak-000T":{"title":"Outer Measure","taxon":"Definition","tags":[],"route":"kak-000T.xml","metas":{}},"kak-000O":{"title":"-Field","taxon":"Definition","tags":[],"route":"kak-000O.xml","metas":{}},"kak-000R":{"title":null,"taxon":"Theorem","tags":[],"route":"kak-000R.xml","metas":{}},"kak-000S":{"title":"Additivity of Intervals","taxon":"Theorem","tags":[],"route":"kak-000S.xml","metas":{}},"kak-000P":{"title":"Probability Measure","taxon":"Definition","tags":[],"route":"kak-000P.xml","metas":{}},"kak-000Q":{"title":"Probability Space","taxon":"Definition","tags":[],"route":"kak-000Q.xml","metas":{}},"kak-000N":{"title":"Problem 1.1 Billingsley","taxon":"Solution","tags":[],"route":"kak-000N.xml","metas":{}},"kak-000M":{"title":"Weak Law of Large Numbers (Dyadic)","taxon":"Theorem","tags":[],"route":"kak-000M.xml","metas":{}},"kak-000L":{"title":"Dyadic Intervals","taxon":"Example","tags":[],"route":"kak-000L.xml","metas":{}},"kak-000I":{"title":"Near Optimal Representation Learning for Hierarchical Reinforcement Learning","taxon":null,"tags":[],"route":"kak-000I.xml","metas":{}},"kak-000J":{"title":"Constrastive Learning as Goal Conditioned Reinforcement Learning","taxon":null,"tags":[],"route":"kak-000J.xml","metas":{}},"kak-000K":{"title":"Contrastive Difference Predictive Coding","taxon":null,"tags":[],"route":"kak-000K.xml","metas":{}},"kak-000H":{"title":"Papers of a Week","taxon":null,"tags":[],"route":"kak-000H.xml","metas":{}},"kak-000G":{"title":"On-policy Prediction with Approximation","taxon":null,"tags":[],"route":"kak-000G.xml","metas":{}},"kak-000D":{"title":null,"taxon":null,"tags":[],"route":"kak-000D.xml","metas":{}},"kak-000F":{"title":"Consistent estimator","taxon":"Definition","tags":[],"route":"kak-000F.xml","metas":{}},"kak-000B":{"title":"InfoNCE","taxon":"Definition","tags":["loss"],"route":"kak-000B.xml","metas":{}},"kak-000C":{"title":"Maximize Mutual info","taxon":"Theorem","tags":[],"route":"kak-000C.xml","metas":{}},"kak-000E":{"title":"NCE loss","taxon":"Definition","tags":["loss"],"route":"kak-000E.xml","metas":{}},"agarwalProtoSuccessorMeasure2024":{"title":"Proto Successor Measure: Representing the Space of All Possible Solutions of Reinforcement Learning","taxon":"Reference","tags":[],"route":"agarwalProtoSuccessorMeasure2024.xml","metas":{"doi":"10.48550/ArXiv.2411.19418","external":"Https://arxiv.org/abs/2411.19418","bibtex":"@Misc{agarwalProtoSuccessorMeasure2024,\n title = {Proto {{Successor Measure}}: {{Representing}} the {{Space}} of {{All Possible Solutions}} of {{Reinforcement Learning}}},\n author = {Agarwal, Siddhant and Sikchi, Harshit and Stone, Peter and Zhang, Amy},\n year = {2024},\n doi = {10.48550/arXiv.2411.19418},\n urldate = {2024-12-04},\n number = {arXiv:2411.19418},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/IZN7I7NS/Agarwal et al. - 2024 - Proto Successor Measure Representing the Space of All Possible Solutions of Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Having explored an environment, intelligent agents should be able to transfer their knowledge to most downstream tasks within that environment. Referred to as \"zero-shot learning,\" this ability remains elusive for general-purpose reinforcement learning algorithms. While recent works have attempted to produce zero-shot RL agents, they make assumptions about the nature of the tasks or the structure of the MDP. We present {\\textbackslash}emph\\{Proto Successor Measure\\}: the basis set for all possible solutions of Reinforcement Learning in a dynamical system. We provably show that any possible policy can be represented using an affine combination of these policy independent basis functions. Given a reward function at test time, we simply need to find the right set of linear weights to combine these basis corresponding to the optimal policy. We derive a practical algorithm to learn these basis functions using only interaction data from the environment and show that our approach can produce the optimal policy at test time for any given reward function without additional environmental interactions. Project page: https://agarwalsiddhant10.github.io/projects/psm.html.},\n primaryclass = {cs},\n eprint = {2411.19418},\n month = {November},\n shorttitle = {Proto {{Successor Measure}}}\n}"}},"kak-000A":{"title":"TVM: What makes a compiler an ML compiler?","taxon":null,"tags":["blog","draft"],"route":"kak-000A.xml","metas":{}},"kak-0009":{"title":"Contrastive Learning","taxon":null,"tags":[],"route":"kak-0009.xml","metas":{}},"kak-0007":{"title":"Notes on Probability and Measure","taxon":null,"tags":["prob","note","unfinished"],"route":"kak-0007.xml","metas":{}},"kak-0002":{"title":"Blog","taxon":null,"tags":["top"],"route":"kak-0002.xml","metas":{}},"kak-0005":{"title":"Contrastive Reinforcement Learning","taxon":null,"tags":[],"route":"kak-0005.xml","metas":{}},"kak-0004":{"title":"Disk","taxon":null,"tags":[],"route":"kak-0004.xml","metas":{}},"kak-0003":{"title":"Notes","taxon":null,"tags":[],"route":"kak-0003.xml","metas":{}},"kak-0006":{"title":"What's up with all these fancy optimizers?","taxon":null,"tags":["blog","draft"],"route":"kak-0006.xml","metas":{}},"jainAdaptiveExplorationDataEfficient2024":{"title":"Adaptive Exploration for Data-Efficient General Value Function Evaluations","taxon":"Reference","tags":[],"route":"jainAdaptiveExplorationDataEfficient2024.xml","metas":{"doi":"10.48550/ArXiv.2405.07838","external":"Https://arxiv.org/abs/2405.07838","bibtex":"@Misc{jainAdaptiveExplorationDataEfficient2024,\n title = {Adaptive {{Exploration}} for {{Data-Efficient General Value Function Evaluations}}},\n author = {Jain, Arushi and Hanna, Josiah P. and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2405.07838},\n urldate = {2024-12-18},\n number = {arXiv:2405.07838},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Jain et al_2024_Adaptive Exploration for Data-Efficient General Value Function Evaluations.pdf;/home/kellen/Zotero/storage/SAPYRAL8/2405.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {General Value Functions (GVFs) (Sutton et al., 2011) represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique reward. Existing methods relying on fixed behavior policies or pre-collected data often face data efficiency issues when learning multiple GVFs in parallel using off-policy methods. To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel. Our method optimizes the behavior policy by minimizing the total variance in return across GVFs, thereby reducing the required environmental interactions. We use an existing temporal-difference-style variance estimator to approximate the return variance. We prove that each behavior policy update decreases the overall mean squared error in GVF predictions. We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.},\n primaryclass = {cs},\n eprint = {2405.07838},\n month = {October}\n}"}},"chuaLearningSuccessorFeatures2024":{"title":"Learning Successor Features the Simple Way","taxon":"Reference","tags":[],"route":"chuaLearningSuccessorFeatures2024.xml","metas":{"doi":"10.48550/ArXiv.2410.22133","external":"Https://arxiv.org/abs/2410.22133","bibtex":"@Misc{chuaLearningSuccessorFeatures2024,\n title = {Learning {{Successor Features}} the {{Simple Way}}},\n author = {Chua, Raymond and Ghosh, Arna and Kaplanis, Christos and Richards, Blake A. and Precup, Doina},\n year = {2024},\n doi = {10.48550/arXiv.2410.22133},\n urldate = {2024-12-04},\n number = {arXiv:2410.22133},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/D4MGLIU6/Chua et al. - 2024 - Learning Successor Features the Simple Way.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporaldifference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required1.},\n primaryclass = {cs},\n eprint = {2410.22133},\n month = {October}\n}"}},"charikarQuantifyingGainWeakStrong2024":{"title":"Quantifying the Gain in Weak-to-Strong Generalization","taxon":"Reference","tags":[],"route":"charikarQuantifyingGainWeakStrong2024.xml","metas":{"external":"Https://arxiv.org/abs/2405.15116","bibtex":"@Misc{charikarQuantifyingGainWeakStrong2024,\n title = {Quantifying the {{Gain}} in {{Weak-to-Strong Generalization}}},\n author = {Charikar, Moses and Pabbaraju, Chirag and Shiragur, Kirankumar},\n year = {2024},\n urldate = {2024-10-24},\n number = {arXiv:2405.15116},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/Y3LYJ8Q8/Charikar et al. - 2024 - Quantifying the Gain in Weak-to-Strong Generalization.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. [BIK+23] empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts---a phenomenon they term weak-to-strong generalization.},\n primaryclass = {cs},\n eprint = {2405.15116},\n month = {October}\n}"}},"elsayedStreamingDeepReinforcement2024":{"title":"Streaming Deep Reinforcement Learning Finally Works","taxon":"Reference","tags":[],"route":"elsayedStreamingDeepReinforcement2024.xml","metas":{"doi":"10.48550/ArXiv.2410.14606","external":"Https://arxiv.org/abs/2410.14606","bibtex":"@Misc{elsayedStreamingDeepReinforcement2024,\n title = {Streaming {{Deep Reinforcement Learning Finally Works}}},\n author = {Elsayed, Mohamed and Vasan, Gautham and Mahmood, A. Rupam},\n year = {2024},\n doi = {10.48550/arXiv.2410.14606},\n urldate = {2024-12-03},\n number = {arXiv:2410.14606},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/2HFK85PR/Elsayed et al. - 2024 - Streaming Deep Reinforcement Learning Finally Works.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resourceconstrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.},\n primaryclass = {cs},\n eprint = {2410.14606},\n month = {October}\n}"}},"leeRLAIFVsRLHF2024":{"title":"RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback","taxon":"Reference","tags":[],"route":"leeRLAIFVsRLHF2024.xml","metas":{"external":"Https://arxiv.org/abs/2309.00267","bibtex":"@Misc{leeRLAIFVsRLHF2024,\n title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},\n author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2309.00267},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/IWPVFZCM/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al. (2022b), offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards ``self-improvement'' by demonstrating that RLAIF can outperform a supervised finetuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},\n primaryclass = {cs},\n eprint = {2309.00267},\n month = {September},\n shorttitle = {{{RLAIF}} vs. {{RLHF}}}\n}"}},"meta2024":{"title":"Meta Learning for Continual Reinforcement Learning: An Investigation","taxon":"Reference","tags":["refereed"],"route":"meta2024.xml","metas":{"external":"Slides/continual_rl_slides.pdf","external":"Papers/honors_thesis.pdf"}},"kanarios2024":{"title":"Cost Aware Best Arm Identification","taxon":"Reference","tags":["refereed","accepted"],"route":"kanarios2024.xml","metas":{"external":"Slides/cabai_slides.pdf","external":"Posters/cabai_rlc_poster.pdf","doi":"10.48550/ArXiv.2402.16710","venue":"Reinforcement Learning Conference"}},"liuSingleGoalAll2024":{"title":"A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals","taxon":"Reference","tags":[],"route":"liuSingleGoalAll2024.xml","metas":{"doi":"10.48550/ArXiv.2408.05804","external":"Https://arxiv.org/abs/2408.05804","bibtex":"@Misc{liuSingleGoalAll2024,\n title = {A {{Single Goal}} Is {{All You Need}}: {{Skills}} and {{Exploration Emerge}} from {{Contrastive RL}} without {{Rewards}}, {{Demonstrations}}, or {{Subgoals}}},\n author = {Liu, Grace and Tang, Michael and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2408.05804},\n urldate = {2024-09-06},\n number = {arXiv:2408.05804},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/U8KGMTC6/Liu et al. - 2024 - A Single Goal is All You Need Skills and Exploration Emerge from Contrastive RL without Rewards, De.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints.},\n primaryclass = {cs},\n eprint = {2408.05804},\n month = {August},\n shorttitle = {A {{Single Goal}} Is {{All You Need}}}\n}"}},"kimUnsupervisedOnlineReinforcementLearning2024":{"title":"Unsupervised-to-Online Reinforcement Learning","taxon":"Reference","tags":[],"route":"kimUnsupervisedOnlineReinforcementLearning2024.xml","metas":{"external":"Https://arxiv.org/abs/2408.14785v1","bibtex":"@Misc{kimUnsupervisedOnlineReinforcementLearning2024,\n title = {Unsupervised-to-{{Online Reinforcement Learning}}},\n author = {Kim, Junsu and Park, Seohong and Levine, Sergey},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2408.14785v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/FRV2CM6E/Kim et al. - 2024 - Unsupervised-to-Online Reinforcement Learning.pdf},\n langid = {english},\n abstract = {Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL, has been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL), which replaces domain-specific supervised offline RL with unsupervised offline RL, as a better alternative to offline-to-online RL. U2O RL not only enables reusing a single pre-trained model for multiple downstream tasks, but also learns better representations, which often result in even better performance and stability than supervised offline-to-online RL. To instantiate U2O RL in practice, we propose a general recipe for U2O RL to bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning. Throughout our experiments in nine state-based and pixel-based environments, we empirically demonstrate that U2O RL achieves strong performance that matches or even outperforms previous offline-to-online RL approaches, while being able to reuse a single pre-trained model for a number of different downstream tasks.},\n month = {August}\n}"}},"carvalhoPredictiveRepresentationsBuilding2024":{"title":"Predictive representations: Building blocks of intelligence","taxon":"Reference","tags":[],"route":"carvalhoPredictiveRepresentationsBuilding2024.xml","metas":{"external":"Https://arxiv.org/abs/2402.06590","bibtex":"@Misc{carvalhoPredictiveRepresentationsBuilding2024,\n title = {Predictive Representations: Building Blocks of Intelligence},\n author = {Carvalho, Wilka and Tomov, Momchil S. and {de Cothi}, William and Barry, Caswell and Gershman, Samuel J.},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2402.06590},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/AWBJ53L5/Carvalho et al. - 2024 - Predictive representations building blocks of intelligence.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.},\n primaryclass = {cs},\n eprint = {2402.06590},\n month = {July},\n shorttitle = {Predictive Representations}\n}"}},"mrouehInformationTheoreticGuarantees2024":{"title":"Information Theoretic Guarantees For Policy Alignment In Large Language Models","taxon":"Reference","tags":[],"route":"mrouehInformationTheoreticGuarantees2024.xml","metas":{"external":"Https://arxiv.org/abs/2406.05883","bibtex":"@Misc{mrouehInformationTheoreticGuarantees2024,\n title = {Information {{Theoretic Guarantees For Policy Alignment In Large Language Models}}},\n author = {Mroueh, Youssef},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2406.05883},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/5NKQB3ZU/Mroueh - 2024 - Information Theoretic Guarantees For Policy Alignment In Large Language Models.pdf},\n keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Policy alignment of large language models refers to constrained policy optimization, where the policy is optimized to maximize a reward while staying close to a reference policy with respect to an \\$f\\$-divergence such as the \\${\\textbackslash}mathsf\\{KL\\}\\$ divergence. The best of \\$n\\$ alignment policy selects a sample from the reference policy that has the maximum reward among \\$n\\$ independent samples. For both cases (policy alignment and best of \\$n\\$), recent works showed empirically that the reward improvement of the aligned policy on the reference one scales like \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$, with an explicit bound in \\$n\\$ on the \\${\\textbackslash}mathsf\\{KL\\}\\$ for the best of \\$n\\$ policy. We show in this paper that the \\${\\textbackslash}sqrt\\{{\\textbackslash}mathsf\\{KL\\}\\}\\$ information theoretic upper bound holds if the reward under the reference policy has sub-gaussian tails. Moreover, we prove for the best of \\$n\\$ policy, that the \\${\\textbackslash}mathsf\\{KL\\}\\$ upper bound can be obtained for any \\$f\\$-divergence via a reduction to exponential order statistics owing to the R{\\textbackslash}'enyi representation of order statistics, and a data processing inequality. If additional information is known on the tails of the aligned policy we show that tighter control on the reward improvement can be obtained via the R{\\textbackslash}'enyi divergence. Finally we demonstrate how these upper bounds transfer from proxy rewards to golden rewards which results in a decrease in the golden reward improvement due to overestimation and approximation errors of the proxy reward.},\n primaryclass = {cs, math, stat},\n eprint = {2406.05883},\n month = {June}\n}"}},"lewandowskiLearningContinuallySpectral2024":{"title":"Learning Continually by Spectral Regularization","taxon":"Reference","tags":[],"route":"lewandowskiLearningContinuallySpectral2024.xml","metas":{"external":"Https://arxiv.org/abs/2406.06811","bibtex":"@Misc{lewandowskiLearningContinuallySpectral2024,\n title = {Learning {{Continually}} by {{Spectral Regularization}}},\n author = {Lewandowski, Alex and Kumar, Saurabh and Schuurmans, Dale and Gy{\\\"o}rgy, Andr{\\'a}s and Machado, Marlos C.},\n year = {2024},\n urldate = {2024-10-18},\n number = {arXiv:2406.06811},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/6HX2DA95/Lewandowski et al. - 2024 - Learning Continually by Spectral Regularization.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Loss of plasticity is a phenomenon where neural networks become more difficult to train during the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good predictive performance while maintaining network trainability. We develop new techniques for improving continual learning by first reconsidering how initialization can ensure trainability during early phases of learning. From this perspective, we derive new regularization strategies for continual learning that ensure beneficial initialization properties are better maintained throughout training. In particular, we investigate two new regularization techniques for continual learning: (i) Wasserstein regularization toward the initial weight distribution, which is less restrictive than regularizing toward initial weights; and (ii) regularizing weight matrix singular values, which directly ensures gradient diversity is maintained throughout training. We present an experimental analysis that shows these alternative regularizers can improve continual learning performance across a range of supervised learning tasks and model architectures. The alternative regularizers prove to be less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance.},\n primaryclass = {cs},\n eprint = {2406.06811},\n month = {June}\n}"}},"myersLearningTemporalDistances2024":{"title":"Learning Temporal Distances: Contrastive Successor Features Can Provide a Metric Structure for Decision-Making","taxon":"Reference","tags":[],"route":"myersLearningTemporalDistances2024.xml","metas":{"doi":"10.48550/ArXiv.2406.17098","external":"Https://arxiv.org/abs/2406.17098","bibtex":"@Misc{myersLearningTemporalDistances2024,\n title = {Learning {{Temporal Distances}}: {{Contrastive Successor Features Can Provide}} a {{Metric Structure}} for {{Decision-Making}}},\n author = {Myers, Vivek and Zheng, Chongyi and Dragan, Anca and Levine, Sergey and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2406.17098},\n urldate = {2024-09-06},\n number = {arXiv:2406.17098},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/X3LR92NR/Myers et al. - 2024 - Learning Temporal Distances Contrastive Successor Features Can Provide a Metric Structure for Decis.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Temporal distances lie at the heart of many algorithms for planning, control, and reinforcement learning that involve reaching goals, allowing one to estimate the transit time between two states. However, prior attempts to define such temporal distances in stochastic settings have been stymied by an important limitation: these prior approaches do not satisfy the triangle inequality. This is not merely a definitional concern, but translates to an inability to generalize and find shortest paths. In this paper, we build on prior work in contrastive learning and quasimetrics to show how successor features learned by contrastive learning (after a change of variables) form a temporal distance that does satisfy the triangle inequality, even in stochastic settings. Importantly, this temporal distance is computationally efficient to estimate, even in high-dimensional and stochastic settings. Experiments in controlled settings and benchmark suites demonstrate that an RL algorithm based on these new temporal distances exhibits combinatorial generalization (i.e., \"stitching\") and can sometimes learn more quickly than prior methods, including those based on quasimetrics.},\n primaryclass = {cs},\n eprint = {2406.17098},\n month = {June},\n shorttitle = {Learning {{Temporal Distances}}}\n}"}},"qiOnlineDPOOnline2024":{"title":"Online DPO: Online Direct Preference Optimization with Fast-Slow Chasing","taxon":"Reference","tags":[],"route":"qiOnlineDPOOnline2024.xml","metas":{"doi":"10.48550/ArXiv.2406.05534","external":"Https://arxiv.org/abs/2406.05534","bibtex":"@Misc{qiOnlineDPOOnline2024,\n title = {Online {{DPO}}: {{Online Direct Preference Optimization}} with {{Fast-Slow Chasing}}},\n author = {Qi, Biqing and Li, Pengfei and Li, Fangyuan and Gao, Junqi and Zhang, Kaiyan and Zhou, Bowen},\n year = {2024},\n doi = {10.48550/arXiv.2406.05534},\n urldate = {2024-09-08},\n number = {arXiv:2406.05534},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/74CS8AIC/Qi et al. - 2024 - Online DPO Online Direct Preference Optimization with Fast-Slow Chasing.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Direct Preference Optimization (DPO) improves the alignment of large language models (LLMs) with human values by training directly on human preference datasets, eliminating the need for reward models. However, due to the presence of cross-domain human preferences, direct continual training can lead to catastrophic forgetting, limiting DPO's performance and efficiency. Inspired by intraspecific competition driving species evolution, we propose a Online Fast-Slow chasing DPO (OFS-DPO) for preference alignment, simulating competition through fast and slow chasing among models to facilitate rapid adaptation. Specifically, we first derive the regret upper bound for online learning, validating our motivation with a min-max optimization pattern. Based on this, we introduce two identical modules using Low-rank Adaptive (LoRA) with different optimization speeds to simulate intraspecific competition, and propose a new regularization term to guide their learning. To further mitigate catastrophic forgetting in cross-domain scenarios, we extend the OFS-DPO with LoRA modules combination strategy, resulting in the Cross domain Online Fast-Slow chasing DPO (COFS-DPO). This method leverages linear combinations of fast modules parameters from different task domains, fully utilizing historical information to achive continual value alignment. Experimental results show that OFS-DPO outperforms DPO in in-domain alignment, while COFS-DPO excels in cross-domain continual learning scenarios.},\n primaryclass = {cs},\n eprint = {2406.05534},\n month = {June},\n shorttitle = {Online {{DPO}}}\n}"}},"sanz-alonsoFirstCourseMonte2024":{"title":"A First Course in Monte Carlo Methods","taxon":"Reference","tags":[],"route":"sanz-alonsoFirstCourseMonte2024.xml","metas":{"external":"Https://arxiv.org/abs/2405.16359","bibtex":"@Misc{sanz-alonsoFirstCourseMonte2024,\n title = {A {{First Course}} in {{Monte Carlo Methods}}},\n author = {{Sanz-Alonso}, Daniel and {Al-Ghattas}, Omar},\n year = {2024},\n urldate = {2024-10-28},\n number = {arXiv:2405.16359},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/YQNKW8C5/Sanz-Alonso and Al-Ghattas - 2024 - A First Course in Monte Carlo Methods.pdf},\n keywords = {Computer Science - Numerical Analysis,Mathematics - History and Overview,Mathematics - Numerical Analysis,Statistics - Computation},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {This is a concise mathematical introduction to Monte Carlo methods, a rich family of algorithms with far-reaching applications in science and engineering. Monte Carlo methods are an exciting subject for mathematical statisticians and computational and applied mathematicians: the design and analysis of modern algorithms are rooted in a broad mathematical toolbox that includes ergodic theory of Markov chains, Hamiltonian dynamical systems, transport maps, stochastic differential equations, information theory, optimization, Riemannian geometry, and gradient flows, among many others. These lecture notes celebrate the breadth of mathematical ideas that have led to tangible advancements in Monte Carlo methods and their applications. To accommodate a diverse audience, the level of mathematical rigor varies from chapter to chapter, giving only an intuitive treatment to the most technically demanding subjects. The aim is not to be comprehensive or encyclopedic, but rather to illustrate some key principles in the design and analysis of Monte Carlo methods through a carefully-crafted choice of topics that emphasizes timeless over timely ideas. Algorithms are presented in a way that is conducive to conceptual understanding and mathematical analysis -- clarity and intuition are favored over state-of-the-art implementations that are harder to comprehend or rely on ad-hoc heuristics. To help readers navigate the expansive landscape of Monte Carlo methods, each algorithm is accompanied by a summary of its pros and cons, and by a discussion of the type of problems for which they are most useful. The presentation is self-contained, and therefore adequate for self-guided learning or as a teaching resource. Each chapter contains a section with bibliographic remarks that will be useful for those interested in conducting research on Monte Carlo methods and their applications.},\n primaryclass = {stat},\n eprint = {2405.16359},\n month = {May}\n}"}},"somerstepStatisticalFrameworkWeakstrong2024":{"title":"A statistical framework for weak-to-strong generalization","taxon":"Reference","tags":[],"route":"somerstepStatisticalFrameworkWeakstrong2024.xml","metas":{"external":"Https://arxiv.org/abs/2405.16236","bibtex":"@Misc{somerstepStatisticalFrameworkWeakstrong2024,\n title = {A Statistical Framework for Weak-to-Strong Generalization},\n author = {Somerstep, Seamus and Polo, Felipe Maia and Banerjee, Moulinath and Ritov, Ya'acov and Yurochkin, Mikhail and Sun, Yuekai},\n year = {2024},\n urldate = {2024-09-25},\n number = {arXiv:2405.16236},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/UIG2LRFF/Somerstep et al. - 2024 - A statistical framework for weak-to-strong general.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in three LLM alignment tasks.},\n primaryclass = {cs, stat},\n eprint = {2405.16236},\n month = {May}\n}"}},"parkFoundationPoliciesHilbert2024":{"title":"Foundation Policies with Hilbert Representations","taxon":"Reference","tags":[],"route":"parkFoundationPoliciesHilbert2024.xml","metas":{"doi":"10.48550/ArXiv.2402.15567","external":"Https://arxiv.org/abs/2402.15567","bibtex":"@Misc{parkFoundationPoliciesHilbert2024,\n title = {Foundation {{Policies}} with {{Hilbert Representations}}},\n author = {Park, Seohong and Kreiman, Tobias and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.15567},\n urldate = {2024-09-18},\n number = {arXiv:2402.15567},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/WEHZ63LJ/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Zotero/storage/YNKYPBF3/Park et al. - 2024 - Foundation Policies with Hilbert Representations.pdf;/home/kellen/Zotero/storage/5H2B6U6Y/2402.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Unsupervised and self-supervised objectives, such as next token prediction, have enabled pre-training generalist models from large amounts of unlabeled data. In reinforcement learning (RL), however, finding a truly general and scalable unsupervised pre-training objective for generalist policies from offline data remains a major open question. While a number of methods have been proposed to enable generic self-supervised RL, based on principles such as goal-conditioned RL, behavioral cloning, and unsupervised skill learning, such methods remain limited in terms of either the diversity of the discovered behaviors, the need for high-quality demonstration data, or the lack of a clear adaptation mechanism for downstream tasks. In this work, we propose a novel unsupervised framework to pre-train generalist policies that capture diverse, optimal, long-horizon behaviors from unlabeled offline data such that they can be quickly adapted to any arbitrary new tasks in a zero-shot manner. Our key insight is to learn a structured representation that preserves the temporal structure of the underlying environment, and then to span this learned latent space with directional movements, which enables various zero-shot policy \"prompting\" schemes for downstream tasks. Through our experiments on simulated robotic locomotion and manipulation benchmarks, we show that our unsupervised policies can solve goal-conditioned and general RL tasks in a zero-shot fashion, even often outperforming prior methods designed specifically for each setting. Our code and videos are available at https://seohong.me/projects/hilp/.},\n primaryclass = {cs},\n eprint = {2402.15567},\n month = {May}\n}"}},"zhangRevisitingZerothOrderOptimization2024":{"title":"Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark","taxon":"Reference","tags":[],"route":"zhangRevisitingZerothOrderOptimization2024.xml","metas":{"doi":"10.48550/ArXiv.2402.11592","external":"Https://arxiv.org/abs/2402.11592","bibtex":"@Misc{zhangRevisitingZerothOrderOptimization2024,\n title = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}: {{A Benchmark}}},\n author = {Zhang, Yihua and Li, Pingzhi and Hong, Junyuan and Li, Jiaxiang and Zhang, Yimeng and Zheng, Wenqing and Chen, Pin-Yu and Lee, Jason D. and Yin, Wotao and Hong, Mingyi and Wang, Zhangyang and Liu, Sijia and Chen, Tianlong},\n year = {2024},\n doi = {10.48550/arXiv.2402.11592},\n urldate = {2024-09-08},\n number = {arXiv:2402.11592},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/3FC8QZYA/Zhang et al. - 2024 - Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning A Benchmark.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow \\{in size\\}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .},\n primaryclass = {cs},\n eprint = {2402.11592},\n month = {May},\n shorttitle = {Revisiting {{Zeroth-Order Optimization}} for {{Memory-Efficient LLM Fine-Tuning}}}\n}"}},"langTheoreticalAnalysisWeakStrong2024":{"title":"Theoretical Analysis of Weak-to-Strong Generalization","taxon":"Reference","tags":[],"route":"langTheoreticalAnalysisWeakStrong2024.xml","metas":{"external":"Https://arxiv.org/abs/2405.16043","bibtex":"@Misc{langTheoreticalAnalysisWeakStrong2024,\n title = {Theoretical {{Analysis}} of {{Weak-to-Strong Generalization}}},\n author = {Lang, Hunter and Sontag, David and Vijayaraghavan, Aravindan},\n year = {2024},\n urldate = {2024-09-18},\n number = {arXiv:2405.16043},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/LJQ96YTP/Lang et al. - 2024 - Theoretical Analysis of Weak-to-Strong Generalization.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.},\n primaryclass = {cs, stat},\n eprint = {2405.16043},\n month = {May}\n}"}},"gomezProperLaplacianRepresentation2024":{"title":"Proper Laplacian Representation Learning","taxon":"Reference","tags":[],"route":"gomezProperLaplacianRepresentation2024.xml","metas":{"external":"Https://arxiv.org/abs/2310.10833","bibtex":"@Misc{gomezProperLaplacianRepresentation2024,\n title = {Proper {{Laplacian Representation Learning}}},\n author = {Gomez, Diego and Bowling, Michael and Machado, Marlos C.},\n year = {2024},\n urldate = {2024-10-18},\n number = {arXiv:2310.10833},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/AGETEYXM/Gomez et al. - 2024 - Proper Laplacian Representation Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.},\n primaryclass = {cs},\n eprint = {2310.10833},\n month = {April}\n}"}},"chenDeepZeroScalingZerothOrder2024":{"title":"DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training","taxon":"Reference","tags":[],"route":"chenDeepZeroScalingZerothOrder2024.xml","metas":{"doi":"10.48550/ArXiv.2310.02025","external":"Https://arxiv.org/abs/2310.02025","bibtex":"@Misc{chenDeepZeroScalingZerothOrder2024,\n title = {{{DeepZero}}: {{Scaling}} up {{Zeroth-Order Optimization}} for {{Deep Model Training}}},\n author = {Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},\n year = {2024},\n doi = {10.48550/arXiv.2310.02025},\n urldate = {2024-09-08},\n number = {arXiv:2310.02025},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/7ALRQ8GJ/Chen et al. - 2024 - DeepZero Scaling up Zeroth-Order Optimization for Deep Model Training.pdf},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinatewise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsityinduced ZO training protocol that extends the model pruning methodology using only finite differences to explore and exploit the sparse DL prior in CGE. Third, we develop the methods of feature reuse and forward parallelization to advance the practical implementations of ZO training. Our extensive experiments show that DeepZero achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10, approaching FO training performance for the first time. Furthermore, we show the practical utility of DeepZero in applications of certified adversarial defense and DL-based partial differential equation error correction, achieving 10-20\\% improvement over SOTA. We believe our results will inspire future research on scalable ZO optimization and contribute to advancing DL with black box. Codes are available at https://github.com/OPTML-Group/DeepZero.},\n primaryclass = {cs},\n eprint = {2310.02025},\n month = {March},\n shorttitle = {{{DeepZero}}}\n}"}},"dongMm2gbGPUAccelerated2024":{"title":"Mm2-gb: GPU Accelerated Minimap2 for Long Read DNA Mapping","taxon":"Reference","tags":[],"route":"dongMm2gbGPUAccelerated2024.xml","metas":{"doi":"10.1101/2024.03.23.586366","bibtex":"@Misc{dongMm2gbGPUAccelerated2024,\n title = {Mm2-Gb: {{GPU Accelerated Minimap2}} for {{Long Read DNA Mapping}}},\n author = {Dong, Juechu and Liu, Xueshen and Sadasivan, Harisankar and Sitaraman, Sriranjani and Narayanasamy, Satish},\n year = {2024},\n doi = {10.1101/2024.03.23.586366},\n urldate = {2024-12-11},\n file = {/home/kellen/Zotero/storage/3PUF5B6Y/Dong et al. - 2024 - mm2-gb GPU Accelerated Minimap2 for Long Read DNA.pdf},\n langid = {english},\n copyright = {http://creativecommons.org/licenses/by-nc/4.0/},\n abstract = {Long-read DNA sequencing is becoming increasingly popular for genetic diagnostics. Minimap2 is the state-of-the-art long-read aligner. However, Minimap2's chaining step is slow on the CPU and takes 40-68\\% of the time especially for long DNA reads. Prior works in accelerating Minimap2 either lose mapping accuracy, are closed source (and not updated) or deliver inconsistent speedups for longer reads. We introduce mm2-gb which accelerates the chaining step of Minimap2 on GPU without compromising mapping accuracy. In addition to intra- and inter-read parallelism exploited by prior works, mm2-gb exploits finer levels of parallelism by breaking down high latency large workloads into smaller independent segments that can be run in parallel and leverages several strategies for better workload balancing including split-kernels and prioritized scheduling of segments based on sorted size. We show that mm2-gb on an AMD Instinct MI210 GPU achieves 2.57-5.33x performance improvement on long nanopore reads (10kb-100kb), and 1.87x performance gain on super long reads (100kb-300kb) compared to SIMD accelerated mm2-fast. mm2-gb is open-sourced and available at https://github.com/Minimap2onGPU/minimap2.},\n month = {March},\n shorttitle = {Mm2-Gb}\n}"}},"caoOfflineGoalConditionedReinforcement2024a":{"title":"Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy","taxon":"Reference","tags":[],"route":"caoOfflineGoalConditionedReinforcement2024a.xml","metas":{"doi":"10.48550/ArXiv.2403.01734","external":"Https://arxiv.org/abs/2403.01734","bibtex":"@Misc{caoOfflineGoalConditionedReinforcement2024a,\n title = {Offline {{Goal-Conditioned Reinforcement Learning}} for {{Safety-Critical Tasks}} with {{Recovery Policy}}},\n author = {Cao, Chenyang and Yan, Zichen and Lu, Renhao and Tan, Junbo and Wang, Xueqian},\n year = {2024},\n doi = {10.48550/arXiv.2403.01734},\n urldate = {2025-01-03},\n number = {arXiv:2403.01734},\n publisher = {arXiv},\n file = {/home/kellenkanarios/Downloads/Papers/Safe-RL/Cao et al_2024_Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with.pdf;/home/kellen/Zotero/storage/MIF9JL6P/2403.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},\n archiveprefix = {arXiv},\n abstract = {Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.},\n primaryclass = {cs},\n eprint = {2403.01734},\n month = {March}\n}"}},"zhengContrastiveDifferencePredictive2024":{"title":"Contrastive Difference Predictive Coding","taxon":"Reference","tags":[],"route":"zhengContrastiveDifferencePredictive2024.xml","metas":{"doi":"10.48550/ArXiv.2310.20141","external":"Https://arxiv.org/abs/2310.20141","bibtex":"@Misc{zhengContrastiveDifferencePredictive2024,\n title = {Contrastive {{Difference Predictive Coding}}},\n author = {Zheng, Chongyi and Salakhutdinov, Ruslan and Eysenbach, Benjamin},\n year = {2024},\n doi = {10.48550/arXiv.2310.20141},\n urldate = {2024-09-06},\n number = {arXiv:2310.20141},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/LXYBRXB5/Zheng et al. - 2024 - Contrastive Difference Predictive Coding.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves \\$2 {\\textbackslash}times\\$ median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about \\$20 {\\textbackslash}times\\$ more sample efficient than the successor representation and \\$1500 {\\textbackslash}times\\$ more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.},\n primaryclass = {cs},\n eprint = {2310.20141},\n month = {February}\n}"}},"guoDirectLanguageModel2024":{"title":"Direct Language Model Alignment from Online AI Feedback","taxon":"Reference","tags":[],"route":"guoDirectLanguageModel2024.xml","metas":{"external":"Https://arxiv.org/abs/2402.04792","bibtex":"@Misc{guoDirectLanguageModel2024,\n title = {Direct {{Language Model Alignment}} from {{Online AI Feedback}}},\n author = {Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and Ferret, Johan and Blondel, Mathieu},\n year = {2024},\n urldate = {2024-09-11},\n number = {arXiv:2402.04792},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GTI664DM/Guo et al. - 2024 - Direct Language Model Alignment from Online AI Feedback.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Direct alignment from preferences (DAP) methods, such as DPO, have recently emerged as efficient alternatives to reinforcement learning from human feedback (RLHF), that do not require a separate reward model. However, the preference datasets used in DAP methods are usually collected ahead of training and never updated, thus the feedback is purely offline. Moreover, responses in these datasets are often sampled from a language model distinct from the one being aligned, and since the model evolves over training, the alignment phase is inevitably off-policy. In this study, we posit that online feedback is key and improves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as annotator: on each training iteration, we sample two responses from the current model and prompt the LLM annotator to choose which one is preferred, thus providing online feedback. Despite its simplicity, we demonstrate via human evaluation in several tasks that OAIF outperforms both offline DAP and RLHF methods. We further show that the feedback leveraged in OAIF is easily controllable, via instruction prompts to the LLM annotator.},\n primaryclass = {cs},\n eprint = {2402.04792},\n month = {February}\n}"}},"fransUnsupervisedZeroShotReinforcement2024":{"title":"Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings","taxon":"Reference","tags":[],"route":"fransUnsupervisedZeroShotReinforcement2024.xml","metas":{"doi":"10.48550/ArXiv.2402.17135","external":"Https://arxiv.org/abs/2402.17135","bibtex":"@Misc{fransUnsupervisedZeroShotReinforcement2024,\n title = {Unsupervised {{Zero-Shot Reinforcement Learning}} via {{Functional Reward Encodings}}},\n author = {Frans, Kevin and Park, Seohong and Abbeel, Pieter and Levine, Sergey},\n year = {2024},\n doi = {10.48550/arXiv.2402.17135},\n urldate = {2024-12-04},\n number = {arXiv:2402.17135},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FUDVV3U4/Frans et al. - 2024 - Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformerbased variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zeroshot manner, given a small number of rewardannotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: github.com/kvfrans/fre.},\n primaryclass = {cs},\n eprint = {2402.17135},\n month = {February}\n}"}},"deepseekai2024deepseekv3technicalreport":{"title":"DeepSeek-V3 technical report","taxon":"Reference","tags":[],"route":"deepseekai2024deepseekv3technicalreport.xml","metas":{"external":"Https://arxiv.org/abs/2412.19437","bibtex":"@Misc{deepseekai2024deepseekv3technicalreport,\n title = {DeepSeek-V3 Technical Report},\n author = {DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan\nWang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng\nand Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian\nYang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and\nFucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei\nLi and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and\nHaowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui\nLi and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and\nJiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang\nChen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song\nand Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang\nand Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia\nand Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and\nMiaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang\nand Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and\nPeng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi\nDu and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and\nRuizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen\nand S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and\nShaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and\nShiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and\nShuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and\nW. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu\nand Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and\nX. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong\nLiu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang\nZhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang\nWang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and\nXingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu\nYang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and\nY. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu\nand Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng\nSun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao\nZhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and\nYishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan\nLiu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan\nWang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and\nYunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and\nYuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren\nand Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang\nand Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and\nZhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and\nZhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu\nand Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang\nSong and Ziyi Gao and Zizheng Pan},\n year = {2024},\n url = {https://arxiv.org/abs/2412.19437},\n primaryclass = {cs.CL},\n archiveprefix = {arXiv},\n eprint = {2412.19437}\n}"}},"shao2024deepseekmathpushinglimitsmathematical":{"title":"DeepSeekMath: Pushing the limits of mathematical reasoning in open language models","taxon":"Reference","tags":[],"route":"shao2024deepseekmathpushinglimitsmathematical.xml","metas":{"external":"Https://arxiv.org/abs/2402.03300","bibtex":"@Misc{shao2024deepseekmathpushinglimitsmathematical,\n title = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\nLanguage Models},\n author = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao\nSong and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li\nand Y. Wu and Daya Guo},\n year = {2024},\n url = {https://arxiv.org/abs/2402.03300},\n primaryclass = {cs.CL},\n archiveprefix = {arXiv},\n eprint = {2402.03300}\n}"}},"malladiFineTuningLanguageModels2024":{"title":"Fine-Tuning Language Models with Just Forward Passes","taxon":"Reference","tags":[],"route":"malladiFineTuningLanguageModels2024.xml","metas":{"doi":"10.48550/ArXiv.2305.17333","external":"Https://arxiv.org/abs/2305.17333","bibtex":"@Misc{malladiFineTuningLanguageModels2024,\n title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},\n author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},\n year = {2024},\n doi = {10.48550/arXiv.2305.17333},\n urldate = {2024-09-08},\n number = {arXiv:2305.17333},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/2J3FEQX7/Malladi et al. - 2024 - Fine-Tuning Language Models with Just Forward Passes.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},\n primaryclass = {cs},\n eprint = {2305.17333},\n month = {January}\n}"}},"degrisStepsizeOptimizationContinual2024":{"title":"Step-size Optimization for Continual Learning","taxon":"Reference","tags":[],"route":"degrisStepsizeOptimizationContinual2024.xml","metas":{"external":"Https://arxiv.org/abs/2401.17401v1","bibtex":"@Misc{degrisStepsizeOptimizationContinual2024,\n title = {Step-Size {{Optimization}} for {{Continual Learning}}},\n author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu, Yuxin and Sutton, Richard},\n year = {2024},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2401.17401v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/YSGENB8H/Degris et al. - 2024 - Step-size Optimization for Continual Learning.pdf},\n langid = {english},\n abstract = {In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.},\n month = {January}\n}"}},"moonDiscoveringHierarchicalAchievements2023":{"title":"Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning","taxon":"Reference","tags":[],"route":"moonDiscoveringHierarchicalAchievements2023.xml","metas":{"bibtex":"@Article{moonDiscoveringHierarchicalAchievements2023,\n title = {Discovering {{Hierarchical Achievements}} in {{Reinforcement Learning}} via {{Contrastive Learning}}},\n author = {Moon, Seungyong and Yeom, Junyoung and Park, Bumsoo and Song, Hyun Oh},\n year = {2023},\n urldate = {2024-12-21},\n journal = {Advances in Neural Information Processing Systems},\n volume = {36},\n pages = {63674--63686},\n file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Moon et al_2023_Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive.pdf},\n langid = {english},\n month = {December}\n}"}},"fosterFoundationsReinforcementLearning2023":{"title":"Foundations of Reinforcement Learning and Interactive Decision Making","taxon":"Reference","tags":[],"route":"fosterFoundationsReinforcementLearning2023.xml","metas":{"doi":"10.48550/ArXiv.2312.16730","external":"Https://arxiv.org/abs/2312.16730","bibtex":"@Misc{fosterFoundationsReinforcementLearning2023,\n title = {Foundations of {{Reinforcement Learning}} and {{Interactive Decision Making}}},\n author = {Foster, Dylan J. and Rakhlin, Alexander},\n year = {2023},\n doi = {10.48550/arXiv.2312.16730},\n urldate = {2025-01-23},\n number = {arXiv:2312.16730},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/YCJRGLTL/Foster and Rakhlin - 2023 - Foundations of Reinforcement Learning and Interactive Decision Making.pdf},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {These lecture notes give a statistical perspective on the foundations of reinforcement learning and interactive decision making. We present a unifying framework for addressing the exploration-exploitation dilemma using frequentist and Bayesian approaches, with connections and parallels between supervised learning/estimation and decision making as an overarching theme. Special attention is paid to function approximation and flexible model classes such as neural networks. Topics covered include multi-armed and contextual bandits, structured bandits, and reinforcement learning with high-dimensional feedback.},\n primaryclass = {cs},\n eprint = {2312.16730},\n month = {December}\n}"}},"fosterFoundationsReinforcementLearning2023a":{"title":"Foundations of Reinforcement Learning and Interactive Decision Making","taxon":"Reference","tags":[],"route":"fosterFoundationsReinforcementLearning2023a.xml","metas":{"doi":"10.48550/ArXiv.2312.16730","external":"Https://arxiv.org/abs/2312.16730","bibtex":"@Misc{fosterFoundationsReinforcementLearning2023a,\n title = {Foundations of {{Reinforcement Learning}} and {{Interactive Decision Making}}},\n author = {Foster, Dylan J. and Rakhlin, Alexander},\n year = {2023},\n doi = {10.48550/arXiv.2312.16730},\n urldate = {2025-01-23},\n number = {arXiv:2312.16730},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/EFVPRXIJ/Foster and Rakhlin - 2023 - Foundations of Reinforcement Learning and Interactive Decision Making.pdf;/home/kellen/Zotero/storage/GJCWUBT3/2312.html},\n keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory},\n archiveprefix = {arXiv},\n abstract = {These lecture notes give a statistical perspective on the foundations of reinforcement learning and interactive decision making. We present a unifying framework for addressing the exploration-exploitation dilemma using frequentist and Bayesian approaches, with connections and parallels between supervised learning/estimation and decision making as an overarching theme. Special attention is paid to function approximation and flexible model classes such as neural networks. Topics covered include multi-armed and contextual bandits, structured bandits, and reinforcement learning with high-dimensional feedback.},\n primaryclass = {cs},\n eprint = {2312.16730},\n month = {December}\n}"}},"kanariosdas2023":{"title":"Parallel Algebraic Multigrid for Higher Order PDEs","taxon":"Reference","tags":["refereed"],"route":"kanariosdas2023.xml","metas":{"external":"Slides/amg_slides.pdf","external":"Posters/amg_poster.pptx","doi":"Https://www.osti.gov/servlets/purl/2205732/"}},"choMiniBatchOptimizationContrastive2023":{"title":"Mini-Batch Optimization of Contrastive Loss","taxon":"Reference","tags":[],"route":"choMiniBatchOptimizationContrastive2023.xml","metas":{"external":"Https://arxiv.org/abs/2307.05906v1","bibtex":"@Misc{choMiniBatchOptimizationContrastive2023,\n title = {Mini-{{Batch Optimization}} of {{Contrastive Loss}}},\n author = {Cho, Jaewoong and Sreenivasan, Kartik and Lee, Keon and Mun, Kyunghoo and Yi, Soheun and Lee, Jeong-Gwan and Lee, Anna and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},\n year = {2023},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2307.05906v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/G87QGI8T/Cho et al. - 2023 - Mini-Batch Optimization of Contrastive Loss.pdf},\n langid = {english},\n abstract = {Contrastive learning has gained significant attention as a method for self-supervised learning. The contrastive loss function ensures that embeddings of positive sample pairs (e.g., different samples from the same class or different views of the same object) are similar, while embeddings of negative pairs are dissimilar. Practical constraints such as large memory requirements make it challenging to consider all possible positive and negative pairs, leading to the use of mini-batch optimization. In this paper, we investigate the theoretical aspects of mini-batch optimization in contrastive learning. We show that mini-batch optimization is equivalent to full-batch optimization if and only if all \\${\\textbackslash}binom\\{N\\}\\{B\\}\\$ mini-batches are selected, while sub-optimality may arise when examining only a subset. We then demonstrate that utilizing high-loss mini-batches can speed up SGD convergence and propose a spectral clustering-based approach for identifying these high-loss mini-batches. Our experimental results validate our theoretical findings and demonstrate that our proposed algorithm outperforms vanilla SGD in practically relevant settings, providing a better understanding of mini-batch optimization in contrastive learning.},\n month = {July}\n}"}},"klissarovDeepLaplacianbasedOptions2023":{"title":"Deep Laplacian-based Options for Temporally-Extended Exploration","taxon":"Reference","tags":[],"route":"klissarovDeepLaplacianbasedOptions2023.xml","metas":{"external":"Https://arxiv.org/abs/2301.11181","bibtex":"@Misc{klissarovDeepLaplacianbasedOptions2023,\n title = {Deep {{Laplacian-based Options}} for {{Temporally-Extended Exploration}}},\n author = {Klissarov, Martin and Machado, Marlos C.},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2301.11181},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/RJ3AYCQA/Klissarov and Machado - 2023 - Deep Laplacian-based Options for Temporally-Extended Exploration.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Selecting exploratory actions that generate a rich stream of experience for better learning is a fundamental challenge in reinforcement learning (RL). An approach to tackle this problem consists in selecting actions according to specific policies for an extended period of time, also known as options. A recent line of work to derive such exploratory options builds upon the eigenfunctions of the graph Laplacian. Importantly, until now these methods have been mostly limited to tabular domains where (1) the graph Laplacian matrix was either given or could be fully estimated, (2) performing eigendecomposition on this matrix was computationally tractable, and (3) value functions could be learned exactly. Additionally, these methods required a separate option discovery phase. These assumptions are fundamentally not scalable. In this paper we address these limitations and show how recent results for directly approximating the eigenfunctions of the Laplacian can be leveraged to truly scale up options-based exploration. To do so, we introduce a fully online deep RL algorithm for discovering Laplacianbased options and evaluate our approach on a variety of pixel-based tasks. We compare to several state-of-the-art exploration methods and show that our approach is effective, general, and especially promising in non-stationary settings.},\n primaryclass = {cs},\n eprint = {2301.11181},\n month = {June}\n}"}},"wangInvestigatingPropertiesNeural2023":{"title":"Investigating the Properties of Neural Network Representations in Reinforcement Learning","taxon":"Reference","tags":[],"route":"wangInvestigatingPropertiesNeural2023.xml","metas":{"external":"Https://arxiv.org/abs/2203.15955","bibtex":"@Misc{wangInvestigatingPropertiesNeural2023,\n title = {Investigating the {{Properties}} of {{Neural Network Representations}} in {{Reinforcement Learning}}},\n author = {Wang, Han and Miahi, Erfan and White, Martha and Machado, Marlos C. and Abbas, Zaheer and Kumaraswamy, Raksha and Liu, Vincent and White, Adam},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2203.15955},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/3QBBT8HR/Wang et al. - 2023 - Investigating the Properties of Neural Network Representations in Reinforcement Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this paper we investigate the properties of representations learned by deep reinforcement learning systems. Much of the early work on representations for reinforcement learning focused on designing fixed-basis architectures to achieve properties thought to be desirable, such as orthogonality and sparsity. In contrast, the idea behind deep reinforcement learning methods is that the agent designer should not encode representational properties, but rather that the data stream should determine the properties of the representation---good representations emerge under appropriate training schemes. In this paper we bring these two perspectives together, empirically investigating the properties of representations that support transfer in reinforcement learning. We introduce and measure six representational properties over more than 25 thousand agent-task settings. We consider Deep Q-learning agents with different auxiliary losses in a pixel-based navigation environment, with source and transfer tasks corresponding to different goal locations. We develop a method to better understand why some representations work better for transfer, through a systematic approach varying task similarity and measuring and correlating representation properties with transfer performance. We demonstrate the generality of the methodology by investigating representations learned by a Rainbow agent that successfully transfer across games modes in Atari 2600.},\n primaryclass = {cs},\n eprint = {2203.15955},\n month = {May}\n}"}},"zhaoPyTorchFSDPExperiences2023":{"title":"PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel","taxon":"Reference","tags":[],"route":"zhaoPyTorchFSDPExperiences2023.xml","metas":{"external":"Https://arxiv.org/abs/2304.11277v2","bibtex":"@Misc{zhaoPyTorchFSDPExperiences2023,\n title = {{{PyTorch FSDP}}: {{Experiences}} on {{Scaling Fully Sharded Data Parallel}}},\n author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and Desmaison, Alban and Balioglu, Can and Damania, Pritam and Nguyen, Bernard and Chauhan, Geeta and Hao, Yuchen and Mathews, Ajit and Li, Shen},\n year = {2023},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2304.11277v2},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/C7SNWUS4/Zhao et al. - 2023 - PyTorch FSDP Experiences on Scaling Fully Sharded Data Parallel.pdf},\n langid = {english},\n abstract = {It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.},\n month = {April},\n shorttitle = {{{PyTorch FSDP}}}\n}"}},"machadoTemporalAbstractionReinforcement2023":{"title":"Temporal Abstraction in Reinforcement Learning with the Successor Representation","taxon":"Reference","tags":[],"route":"machadoTemporalAbstractionReinforcement2023.xml","metas":{"external":"Https://arxiv.org/abs/2110.05740","bibtex":"@Misc{machadoTemporalAbstractionReinforcement2023,\n title = {Temporal {{Abstraction}} in {{Reinforcement Learning}} with the {{Successor Representation}}},\n author = {Machado, Marlos C. and Barreto, Andre and Precup, Doina and Bowling, Michael},\n year = {2023},\n urldate = {2024-10-18},\n number = {arXiv:2110.05740},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/PEJ6S5ZX/Machado et al. - 2023 - Temporal Abstraction in Reinforcement Learning with the Successor Representation.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the successor representation can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent's representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we also discuss how the successor representation allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them. Our results shed light on important design decisions involved in the definition of options and demonstrate the synergy of different methods based on the successor representation, such as eigenoptions and the option keyboard.},\n primaryclass = {cs},\n eprint = {2110.05740},\n month = {April}\n}"}},"touatiDoesZeroShotReinforcement2023":{"title":"Does Zero-Shot Reinforcement Learning Exist?","taxon":"Reference","tags":[],"route":"touatiDoesZeroShotReinforcement2023.xml","metas":{"doi":"10.48550/ArXiv.2209.14935","external":"Https://arxiv.org/abs/2209.14935","bibtex":"@Misc{touatiDoesZeroShotReinforcement2023,\n title = {Does {{Zero-Shot Reinforcement Learning Exist}}?},\n author = {Touati, Ahmed and Rapin, J{\\'e}r{\\'e}my and Ollivier, Yann},\n year = {2023},\n doi = {10.48550/arXiv.2209.14935},\n urldate = {2024-09-18},\n number = {arXiv:2209.14935},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GFV8I99T/Touati et al. - 2023 - Does Zero-Shot Reinforcement Learning Exist.pdf;/home/kellen/Zotero/storage/9RA8LX73/2209.html},\n keywords = {Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards \"controllable\" agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL ave been suggested using successor features (SFs) [BBQ+ 18] or forward-backward (FB) representations [TO21], but testing has been limited. After clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark [LYL+21]. To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers. SFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS), perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching 85\\% of supervised RL performance with a good replay buffer, in a zero-shot manner.},\n primaryclass = {cs},\n eprint = {2209.14935},\n month = {March}\n}"}},"eysenbachContrastiveLearningGoalConditioned2023":{"title":"Contrastive Learning as Goal-Conditioned Reinforcement Learning","taxon":"Reference","tags":[],"route":"eysenbachContrastiveLearningGoalConditioned2023.xml","metas":{"doi":"10.48550/ArXiv.2206.07568","external":"Https://arxiv.org/abs/2206.07568","bibtex":"@Misc{eysenbachContrastiveLearningGoalConditioned2023,\n title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning}}},\n author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},\n year = {2023},\n doi = {10.48550/arXiv.2206.07568},\n urldate = {2024-09-06},\n number = {arXiv:2206.07568},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/WVQKIMN4/Eysenbach et al. - 2023 - Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n archiveprefix = {arXiv},\n abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},\n primaryclass = {cs},\n eprint = {2206.07568},\n month = {February}\n}"}},"huangReinforcementLearningLowRank2023":{"title":"Reinforcement Learning in Low-Rank MDPs with Density Features","taxon":"Reference","tags":[],"route":"huangReinforcementLearningLowRank2023.xml","metas":{"external":"Https://arxiv.org/abs/2302.02252","bibtex":"@Misc{huangReinforcementLearningLowRank2023,\n title = {Reinforcement {{Learning}} in {{Low-Rank MDPs}} with {{Density Features}}},\n author = {Huang, Audrey and Chen, Jinglin and Jiang, Nan},\n year = {2023},\n urldate = {2024-09-16},\n number = {arXiv:2302.02252},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/JV6TZ7S9/Huang et al. - 2023 - Reinforcement Learning in Low-Rank MDPs with Density Features.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {MDPs with low-rank transitions---that is, the transition matrix can be factored into the product of two matrices, left and right---is a highly representative structure that enables tractable learning. The left matrix enables expressive function approximation for value-based learning and has been studied extensively. In this work, we instead investigate sample-efficient learning with density features, i.e., the right matrix, which induce powerful models for state-occupancy distributions. This setting not only sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex RL. In the offline setting, we propose an algorithm for off-policy estimation of occupancies that can handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that constructs exploratory data distributions in a level-by-level manner. As a central technical challenge, the additive error of occupancy estimation is incompatible with the multiplicative definition of data coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend to the representation learning setting, when the density features are unknown and must be learned from an exponentially large candidate set.},\n primaryclass = {cs, stat},\n eprint = {2302.02252},\n month = {February}\n}"}},"khanferFundamentalsFunctionalAnalysis2023":{"title":"Fundamentals of Functional Analysis","taxon":"Reference","tags":[],"route":"khanferFundamentalsFunctionalAnalysis2023.xml","metas":{"doi":"10.1007/978-981-99-3029-6","bibtex":"@Book{khanferFundamentalsFunctionalAnalysis2023,\n title = {Fundamentals of {{Functional Analysis}}},\n author = {Khanfer, Ammar},\n year = {2023},\n isbn = {978-981-99-3028-9 978-981-99-3029-6},\n doi = {10.1007/978-981-99-3029-6},\n urldate = {2024-10-18},\n publisher = {Springer Nature Singapore},\n address = {Singapore},\n file = {/home/kellen/Zotero/storage/Z2NQSGWL/Khanfer - 2023 - Fundamentals of Functional Analysis.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"khanferMeasureTheoryIntegration2023":{"title":"Measure Theory and Integration","taxon":"Reference","tags":[],"route":"khanferMeasureTheoryIntegration2023.xml","metas":{"doi":"10.1007/978-981-99-2882-8","bibtex":"@Book{khanferMeasureTheoryIntegration2023,\n title = {Measure {{Theory}} and {{Integration}}},\n author = {Khanfer, Ammar},\n year = {2023},\n isbn = {978-981-99-2881-1 978-981-99-2882-8},\n doi = {10.1007/978-981-99-2882-8},\n urldate = {2024-10-18},\n publisher = {Springer Nature Singapore},\n address = {Singapore},\n file = {/home/kellen/Zotero/storage/9CDPYJ3A/Khanfer - 2023 - Measure Theory and Integration.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"sniderOperatorFusionXLA2023":{"title":"Operator Fusion in XLA: Analysis and Evaluation","taxon":"Reference","tags":[],"route":"sniderOperatorFusionXLA2023.xml","metas":{"external":"Https://arxiv.org/abs/2301.13062","bibtex":"@Misc{sniderOperatorFusionXLA2023,\n title = {Operator {{Fusion}} in {{XLA}}: {{Analysis}} and {{Evaluation}}},\n author = {Snider, Daniel and Liang, Ruofan},\n year = {2023},\n urldate = {2024-10-20},\n number = {arXiv:2301.13062},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/9M5P2JQF/Snider and Liang - 2023 - Operator Fusion in XLA Analysis and Evaluation.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Machine learning (ML) compilers are an active area of research because they offer the potential to automatically speedup tensor programs. Kernel fusion is often cited as an important optimization performed by ML compilers. However, there exists a knowledge gap about how XLA, the most common ML compiler, applies this nuanced optimization, what kind of speedup it can afford, and what low-level effects it has on hardware. Our paper aims to bridge this knowledge gap by studying key compiler passes of XLA's source code. Our evaluation on a reinforcement learning environment Cartpole shows how different fusion decisions in XLA are made in practice. Furthermore, we implement several XLA kernel fusion strategies that can achieve up to 10.56x speedup compared to our baseline implementation.},\n primaryclass = {cs},\n eprint = {2301.13062},\n month = {January},\n shorttitle = {Operator {{Fusion}} in {{XLA}}}\n}"}},"kingmaAutoEncodingVariationalBayes2022":{"title":"Auto-Encoding Variational Bayes","taxon":"Reference","tags":[],"route":"kingmaAutoEncodingVariationalBayes2022.xml","metas":{"doi":"10.48550/ArXiv.1312.6114","external":"Https://arxiv.org/abs/1312.6114","bibtex":"@Misc{kingmaAutoEncodingVariationalBayes2022,\n title = {Auto-{{Encoding Variational Bayes}}},\n author = {Kingma, Diederik P. and Welling, Max},\n year = {2022},\n doi = {10.48550/arXiv.1312.6114},\n urldate = {2025-01-07},\n number = {arXiv:1312.6114},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/4CIQB3XP/1312.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},\n primaryclass = {stat},\n eprint = {1312.6114},\n month = {December}\n}"}},"mazoureContrastiveValueLearning2022":{"title":"Contrastive Value Learning: Implicit Models for Simple Offline RL","taxon":"Reference","tags":[],"route":"mazoureContrastiveValueLearning2022.xml","metas":{"doi":"10.48550/ArXiv.2211.02100","external":"Https://arxiv.org/abs/2211.02100","bibtex":"@Misc{mazoureContrastiveValueLearning2022,\n title = {Contrastive {{Value Learning}}: {{Implicit Models}} for {{Simple Offline RL}}},\n author = {Mazoure, Bogdan and Eysenbach, Benjamin and Nachum, Ofir and Tompson, Jonathan},\n year = {2022},\n doi = {10.48550/arXiv.2211.02100},\n urldate = {2025-01-09},\n number = {arXiv:2211.02100},\n publisher = {arXiv},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Model-based reinforcement learning (RL) methods are appealing in the offline setting because they allow an agent to reason about the consequences of actions without interacting with the environment. Prior methods learn a 1-step dynamics model, which predicts the next state given the current state and action. These models do not immediately tell the agent which actions to take, but must be integrated into a larger RL framework. Can we model the environment dynamics in a different way, such that the learned model does directly indicate the value of each action? In this paper, we propose Contrastive Value Learning (CVL), which learns an implicit, multi-step model of the environment dynamics. This model can be learned without access to reward functions, but nonetheless can be used to directly estimate the value of each action, without requiring any TD learning. Because this model represents the multi-step transitions implicitly, it avoids having to predict high-dimensional observations and thus scales to high-dimensional tasks. Our experiments demonstrate that CVL outperforms prior offline RL methods on complex continuous control benchmarks.},\n primaryclass = {cs},\n eprint = {2211.02100},\n month = {November},\n shorttitle = {Contrastive {{Value Learning}}}\n}"}},"xieExplanationContextLearning2022":{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference","taxon":"Reference","tags":[],"route":"xieExplanationContextLearning2022.xml","metas":{"external":"Https://arxiv.org/abs/2111.02080","bibtex":"@Misc{xieExplanationContextLearning2022,\n title = {An {{Explanation}} of {{In-context Learning}} as {{Implicit Bayesian Inference}}},\n author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},\n year = {2022},\n urldate = {2024-10-15},\n number = {arXiv:2111.02080},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/RKQR98C3/Xie et al. - 2022 - An Explanation of In-context Learning as Implicit Bayesian Inference.pdf},\n keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},\n primaryclass = {cs},\n eprint = {2111.02080},\n month = {July}\n}"}},"suttonHistoryMetagradientGradient2022":{"title":"A History of Meta-gradient: Gradient Methods for Meta-learning","taxon":"Reference","tags":[],"route":"suttonHistoryMetagradientGradient2022.xml","metas":{"external":"Https://arxiv.org/abs/2202.09701v1","bibtex":"@Misc{suttonHistoryMetagradientGradient2022,\n title = {A {{History}} of {{Meta-gradient}}: {{Gradient Methods}} for {{Meta-learning}}},\n author = {Sutton, Richard S.},\n year = {2022},\n urldate = {2024-09-06},\n howpublished = {https://arxiv.org/abs/2202.09701v1},\n journal = {arXiv.org},\n file = {/home/kellen/Zotero/storage/93SUJHK4/Sutton - 2022 - A History of Meta-gradient Gradient Methods for Meta-learning.pdf},\n langid = {english},\n abstract = {The history of meta-learning methods based on gradient descent is reviewed, focusing primarily on methods that adapt step-size (learning rate) meta-parameters.},\n month = {February},\n shorttitle = {A {{History}} of {{Meta-gradient}}}\n}"}},"mcleodContinualAuxiliaryTask2022":{"title":"Continual Auxiliary Task Learning","taxon":"Reference","tags":[],"route":"mcleodContinualAuxiliaryTask2022.xml","metas":{"external":"Https://arxiv.org/abs/2202.11133","bibtex":"@Misc{mcleodContinualAuxiliaryTask2022,\n title = {Continual {{Auxiliary Task Learning}}},\n author = {McLeod, Matthew and Lo, Chunlok and Schlegel, Matthew and Jacobsen, Andrew and Kumaraswamy, Raksha and White, Martha and White, Adam},\n year = {2022},\n urldate = {2024-10-16},\n number = {arXiv:2202.11133},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/GUALHQEZ/McLeod et al. - 2022 - Continual Auxiliary Task Learning.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.},\n primaryclass = {cs},\n eprint = {2202.11133},\n month = {February}\n}"}},"wrightHighDimensionalDataAnalysis2022":{"title":"High-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications","taxon":"Reference","tags":[],"route":"wrightHighDimensionalDataAnalysis2022.xml","metas":{"doi":"10.1017/9781108779302","bibtex":"@Book{wrightHighDimensionalDataAnalysis2022,\n title = {High-{{Dimensional Data Analysis}} with {{Low-Dimensional Models}}: {{Principles}}, {{Computation}}, and {{Applications}}},\n author = {Wright, John and Ma, Yi},\n year = {2022},\n isbn = {978-1-108-77930-2 978-1-108-48973-7},\n doi = {10.1017/9781108779302},\n urldate = {2024-11-11},\n edition = {1},\n publisher = {Cambridge University Press},\n file = {/home/kellen/Zotero/storage/QH288KYJ/Wright and Ma - 2022 - High-Dimensional Data Analysis with Low-Dimensional Models Principles, Computation, and Application.pdf},\n langid = {english},\n copyright = {https://www.cambridge.org/core/terms},\n abstract = {Connecting theory with practice, this systematic and rigorous introduction covers the fundamental principles, algorithms and applications of key mathematical models for high-dimensional data analysis. Comprehensive in its approach, it provides unified coverage of many different low-dimensional models and analytical techniques, including sparse and low-rank models, and both convex and non-convex formulations. Readers will learn how to develop efficient and scalable algorithms for solving real-world problems, supported by numerous examples and exercises throughout, and how to use the computational tools learnt in several application contexts. Applications presented include scientific imaging, communication, face recognition, 3D vision, and deep networks for classification. With code available online, this is an ideal textbook for senior and graduate students in computer science, data science, and electrical engineering, as well as for those taking courses on sparsity, low-dimensional structures, and high-dimensional data. Foreword by Emmanuel Cand{\\`e}s.},\n month = {January},\n shorttitle = {High-{{Dimensional Data Analysis}} with {{Low-Dimensional Models}}}\n}"}},"paternainSafePoliciesReinforcement2022":{"title":"Safe Policies for Reinforcement Learning via Primal-Dual Methods","taxon":"Reference","tags":[],"route":"paternainSafePoliciesReinforcement2022.xml","metas":{"doi":"10.48550/ArXiv.1911.09101","external":"Https://arxiv.org/abs/1911.09101","bibtex":"@Misc{paternainSafePoliciesReinforcement2022,\n title = {Safe {{Policies}} for {{Reinforcement Learning}} via {{Primal-Dual Methods}}},\n author = {Paternain, Santiago and {Calvo-Fullana}, Miguel and Chamon, Luiz F. O. and Ribeiro, Alejandro},\n year = {2022},\n doi = {10.48550/arXiv.1911.09101},\n urldate = {2025-01-27},\n number = {arXiv:1911.09101},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/F3LLEA4F/Paternain et al. - 2022 - Safe Policies for Reinforcement Learning via Primal-Dual Methods.pdf},\n keywords = {Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this paper, we study the design of controllers in the context of stochastic optimal control under the assumption that the model of the system is not available. This is, we aim to control a Markov Decision Process (MDP) of which we do not know the transition probabilities, but we have access to sample trajectories through experience. We define safety as the agent remaining in a desired safe set with high probability during the operation time. The drawbacks of this formulation are twofold. The problem is non-convex and computing the gradients of the constraints with respect to the policies is prohibitive. Hence, we propose an ergodic relaxation of the constraints with the following advantages. (i) The safety guarantees are maintained in the case of episodic tasks and they hold until a given time horizon for continuing tasks. (ii) The constrained optimization problem despite its non-convexity has arbitrarily small duality gap if the parametrization of the controller is rich enough. (iii) The gradients of the Lagrangian associated to the safe learning problem can be computed using standard Reinforcement Learning (RL) results and stochastic approximation tools. Leveraging these advantages, we exploit primal-dual algorithms to find policies that are safe and optimal. We test the proposed approach in a navigation task in a continuous domain. The numerical results show that our algorithm is capable of dynamically adapting the policy to the environment and the required safety levels.},\n primaryclass = {eess},\n eprint = {1911.09101},\n month = {January}\n}"}},"altmanConstrainedMarkovDecision2021":{"title":"Constrained Markov Decision Processes: Stochastic Modeling","taxon":"Reference","tags":[],"route":"altmanConstrainedMarkovDecision2021.xml","metas":{"doi":"10.1201/9781315140223","bibtex":"@Book{altmanConstrainedMarkovDecision2021,\n title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},\n author = {Altman, Eitan},\n year = {2021},\n isbn = {978-1-315-14022-3},\n doi = {10.1201/9781315140223},\n urldate = {2024-11-27},\n edition = {1},\n publisher = {Routledge},\n address = {Boca Raton},\n file = {/home/kellen/Zotero/storage/GBXKPARY/Altman - 2021 - Constrained Markov Decision Processes Stochastic Modeling.pdf},\n langid = {english},\n month = {December},\n shorttitle = {Constrained {{Markov Decision Processes}}}\n}"}},"kumarDR3ValueBasedDeep2021":{"title":"DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization","taxon":"Reference","tags":[],"route":"kumarDR3ValueBasedDeep2021.xml","metas":{"external":"Https://arxiv.org/abs/2112.04716","bibtex":"@Misc{kumarDR3ValueBasedDeep2021,\n title = {{{DR3}}: {{Value-Based Deep Reinforcement Learning Requires Explicit Regularization}}},\n author = {Kumar, Aviral and Agarwal, Rishabh and Ma, Tengyu and Courville, Aaron and Tucker, George and Levine, Sergey},\n year = {2021},\n urldate = {2024-09-11},\n number = {arXiv:2112.04716},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/S9MJBMRM/Kumar et al. - 2021 - DR3 Value-Based Deep Reinforcement Learning Requires Explicit Regularization.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Despite overparameterization, deep networks trained via supervised learning are easy to optimize and exhibit excellent generalization. One hypothesis to explain this is that overparameterized deep networks enjoy the benefits of implicit regularization induced by stochastic gradient descent, which favors parsimonious solutions that generalize well on test inputs. It is reasonable to surmise that deep reinforcement learning (RL) methods could also benefit from this effect. In this paper, we discuss how the implicit regularization effect of SGD seen in supervised learning could in fact be harmful in the offline deep RL setting, leading to poor generalization and degenerate feature representations. Our theoretical analysis shows that when existing models of implicit regularization are applied to temporal difference learning, the resulting derived regularizer favors degenerate solutions with excessive ``aliasing'', in stark contrast to the supervised learning case. We back up these findings empirically, showing that feature representations learned by a deep network value function trained via bootstrapping can indeed become degenerate, aliasing the representations for state-action pairs that appear on either side of the Bellman backup. To address this issue, we derive the form of this implicit regularizer and, inspired by this derivation, propose a simple and effective explicit regularizer, called DR3, that counteracts the undesirable effects of this implicit regularizer. When combined with existing offline RL methods, DR3 substantially improves performance and stability, alleviating unlearning in Atari 2600 games, D4RL domains and robotic manipulation from images.},\n primaryclass = {cs},\n eprint = {2112.04716},\n month = {December},\n shorttitle = {{{DR3}}}\n}"}},"touatiLearningOneRepresentation2021":{"title":"Learning One Representation to Optimize All Rewards","taxon":"Reference","tags":["paper","todo"],"route":"touatiLearningOneRepresentation2021.xml","metas":{"doi":"10.48550/ArXiv.2103.07945","external":"Https://arxiv.org/abs/2103.07945","bibtex":"@Misc{touatiLearningOneRepresentation2021,\n title = {Learning {{One Representation}} to {{Optimize All Rewards}}},\n author = {Touati, Ahmed and Ollivier, Yann},\n year = {2021},\n doi = {10.48550/arXiv.2103.07945},\n urldate = {2024-09-18},\n number = {arXiv:2103.07945},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FN8MPGES/Touati and Ollivier - 2021 - Learning One Representation to Optimize All Rewards.pdf;/home/kellen/Zotero/storage/SXVNRKWC/2103.html},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},\n archiveprefix = {arXiv},\n abstract = {We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.},\n primaryclass = {cs, math},\n eprint = {2103.07945},\n month = {October}\n}"}},"regehrElementaryProofThat2021":{"title":"An Elementary Proof that Q-learning Converges Almost Surely","taxon":"Reference","tags":[],"route":"regehrElementaryProofThat2021.xml","metas":{"doi":"10.48550/ArXiv.2108.02827","external":"Https://arxiv.org/abs/2108.02827","bibtex":"@Misc{regehrElementaryProofThat2021,\n title = {An {{Elementary Proof}} That {{Q-learning Converges Almost Surely}}},\n author = {Regehr, Matthew T. and Ayoub, Alex},\n year = {2021},\n doi = {10.48550/arXiv.2108.02827},\n urldate = {2025-01-23},\n number = {arXiv:2108.02827},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FLMMCTDT/Regehr and Ayoub - 2021 - An Elementary Proof that Q-learning Converges Almost Surely.pdf},\n keywords = {Computer Science - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Watkins' and Dayan's Q-learning is a model-free reinforcement learning algorithm that iteratively refines an estimate for the optimal action-value function of an MDP by stochastically \"visiting\" many state-ation pairs [Watkins and Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent state-of-the-art achievements in reinforcement learning, including the superhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this paper is to reproduce a precise and (nearly) self-contained proof that Q-learning converges. Much of the available literature leverages powerful theory to obtain highly generalizable results in this vein. However, this approach requires the reader to be familiar with and make many deep connections to different research areas. A student seeking to deepen their understand of Q-learning risks becoming caught in a vicious cycle of \"RL-learning Hell\". For this reason, we give a complete proof from start to finish using only one external result from the field of stochastic approximation, despite the fact that this minimal dependence on other results comes at the expense of some \"shininess\".},\n primaryclass = {cs},\n eprint = {2108.02827},\n month = {August}\n}"}},"kalikarAcceleratingLongreadAnalysis2021":{"title":"Accelerating long-read analysis on modern CPUs","taxon":"Reference","tags":[],"route":"kalikarAcceleratingLongreadAnalysis2021.xml","metas":{"doi":"10.1101/2021.07.21.453294","bibtex":"@Misc{kalikarAcceleratingLongreadAnalysis2021,\n title = {Accelerating Long-Read Analysis on Modern {{CPUs}}},\n author = {Kalikar, Saurabh and Jain, Chirag and Md, Vasimuddin and Misra, Sanchit},\n year = {2021},\n doi = {10.1101/2021.07.21.453294},\n urldate = {2024-12-11},\n file = {/home/kellen/Zotero/storage/FIX8U6F5/Kalikar et al. - 2021 - Accelerating long-read analysis on modern CPUs.pdf},\n langid = {english},\n abstract = {Long read sequencing is now routinely used at scale for genomics and transcriptomics applications. Mapping of long reads or a draft genome assembly to a reference sequence is often one of the most time consuming steps in these applications. Here, we present techniques to accelerate minimap2, a widely used software for mapping. We present multiple optimizations using SIMD parallelization, e cient cache utilization and a learned index data structure to accelerate its three main computational modules, i.e., seeding, chaining and pairwise sequence alignment. These result in reduction of end-to-end mapping time of minimap2 by up to 1.8{$\\RightArrowBar$} while maintaining identical output.},\n month = {July}\n}"}},"yangRepresentationMattersOffline2021":{"title":"Representation Matters: Offline Pretraining for Sequential Decision Making","taxon":"Reference","tags":[],"route":"yangRepresentationMattersOffline2021.xml","metas":{"bibtex":"@Inproceedings{yangRepresentationMattersOffline2021,\n title = {Representation {{Matters}}: {{Offline Pretraining}} for {{Sequential Decision Making}}},\n author = {Yang, Mengjiao and Nachum, Ofir},\n year = {2021},\n urldate = {2024-09-08},\n booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},\n pages = {11784--11794},\n publisher = {PMLR},\n file = {/home/kellen/Zotero/storage/B54IUCU7/Yang and Nachum - 2021 - Representation Matters Offline Pretraining for Sequential Decision Making.pdf;/home/kellen/Zotero/storage/BANYUMGT/Yang and Nachum - 2021 - Representation Matters Offline Pretraining for Sequential Decision Making.pdf},\n langid = {english},\n abstract = {The recent success of supervised learning methods on ever larger offline datasets has spurred interest in the reinforcement learning (RL) field to investigate whether the same paradigms can be translated to RL algorithms. This research area, known as offline RL, has largely focused on offline policy optimization, aiming to find a return-maximizing policy exclusively from offline data. In this paper, we consider a slightly different approach to incorporating offline data into sequential decision-making. We aim to answer the question, what unsupervised objectives applied to offline datasets are able to learn state representations which elevate performance on downstream tasks, whether those downstream tasks be online RL, imitation learning from expert demonstrations, or even offline policy optimization based on the same offline dataset? Through a variety of experiments utilizing standard offline RL datasets, we find that the use of pretraining with unsupervised learning objectives can dramatically improve the performance of policy learning algorithms that otherwise yield mediocre performance on their own. Extensive ablations further provide insights into what components of these unsupervised objectives \\{--\\} e.g., reward prediction, continuous or discrete representations, pretraining or finetuning \\{--\\} are most important and in which settings.},\n issn = {2640-3498},\n month = {July},\n shorttitle = {Representation {{Matters}}}\n}"}},"liDeepLearningCompiler2021":{"title":"The Deep Learning Compiler: A Comprehensive Survey","taxon":"Reference","tags":[],"route":"liDeepLearningCompiler2021.xml","metas":{"doi":"10.1109/TPDS.2020.3030548","external":"Https://arxiv.org/abs/2002.03794","bibtex":"@Article{liDeepLearningCompiler2021,\n title = {The {{Deep Learning Compiler}}: {{A Comprehensive Survey}}},\n author = {Li, Mingzhen and Liu, Yi and Liu, Xiaoyan and Sun, Qingxiao and You, Xin and Yang, Hailong and Luan, Zhongzhi and Gan, Lin and Yang, Guangwen and Qian, Depei},\n year = {2021},\n doi = {10.1109/TPDS.2020.3030548},\n urldate = {2024-10-20},\n journal = {IEEE Transactions on Parallel and Distributed Systems},\n volume = {32},\n number = {3},\n pages = {708--727},\n file = {/home/kellen/Zotero/storage/TLNQRMTF/Li et al. - 2021 - The Deep Learning Compiler A Comprehensive Survey.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Computer Science - Performance},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this paper, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey paper focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler.},\n issn = {1045-9219, 1558-2183, 2161-9883},\n primaryclass = {cs},\n eprint = {2002.03794},\n month = {March},\n shorttitle = {The {{Deep Learning Compiler}}}\n}"}},"levineOfflineReinforcementLearning2020":{"title":"Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems","taxon":"Reference","tags":[],"route":"levineOfflineReinforcementLearning2020.xml","metas":{"external":"Https://arxiv.org/abs/2005.01643","bibtex":"@Misc{levineOfflineReinforcementLearning2020,\n title = {Offline {{Reinforcement Learning}}: {{Tutorial}}, {{Review}}, and {{Perspectives}} on {{Open Problems}}},\n author = {Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},\n year = {2020},\n urldate = {2024-11-17},\n number = {arXiv:2005.01643},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/QV4MPWZ3/Levine et al. - 2020 - Offline Reinforcement Learning Tutorial, Review, and Perspectives on Open Problems.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {In this tutorial article, we aim to provide the reader with the conceptual tools needed to get started on research on offline reinforcement learning algorithms: reinforcement learning algorithms that utilize previously collected data, without additional online data collection. Offline reinforcement learning algorithms hold tremendous promise for making it possible to turn large datasets into powerful decision making engines. Effective offline reinforcement learning methods would be able to extract policies with the maximum possible utility out of the available data, thereby allowing automation of a wide range of decision-making domains, from healthcare and education to robotics. However, the limitations of current algorithms make this difficult. We will aim to provide the reader with an understanding of these challenges, particularly in the context of modern deep reinforcement learning methods, and describe some potential solutions that have been explored in recent work to mitigate these challenges, along with recent applications, and a discussion of perspectives on open problems in the field.},\n primaryclass = {cs},\n eprint = {2005.01643},\n month = {November},\n shorttitle = {Offline {{Reinforcement Learning}}}\n}"}},"lattimoreBanditAlgorithms2020":{"title":"Bandit Algorithms","taxon":"Reference","tags":[],"route":"lattimoreBanditAlgorithms2020.xml","metas":{"doi":"10.1017/9781108571401","bibtex":"@Book{lattimoreBanditAlgorithms2020,\n title = {Bandit {{Algorithms}}},\n author = {Lattimore, Tor and Szepesv{\\'a}ri, Csaba},\n year = {2020},\n isbn = {978-1-108-57140-1 978-1-108-48682-8},\n doi = {10.1017/9781108571401},\n urldate = {2024-10-24},\n edition = {1},\n publisher = {Cambridge University Press},\n file = {/home/kellen/Zotero/storage/U3N2WASX/Lattimore and Szepesvri - 2020 - Bandit Algorithms.pdf},\n langid = {english},\n copyright = {https://www.cambridge.org/core/terms},\n month = {July}\n}"}},"rajbhandariZeROMemoryOptimizations2020":{"title":"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models","taxon":"Reference","tags":[],"route":"rajbhandariZeROMemoryOptimizations2020.xml","metas":{"external":"Https://arxiv.org/abs/1910.02054","bibtex":"@Misc{rajbhandariZeROMemoryOptimizations2020,\n title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},\n author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},\n year = {2020},\n urldate = {2024-09-18},\n number = {arXiv:1910.02054},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/FDXIZE3Y/Rajbhandari et al. - 2020 - ZeRO Memory Optimizations Toward Training Trillion Parameter Models.pdf},\n keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.},\n primaryclass = {cs, stat},\n eprint = {1910.02054},\n month = {May},\n shorttitle = {{{ZeRO}}}\n}"}},"guntherExaminingUseTemporalDifference2020":{"title":"Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for Real-World Predictive Knowledge Architectures","taxon":"Reference","tags":[],"route":"guntherExaminingUseTemporalDifference2020.xml","metas":{"doi":"10.3389/Frobt.2020.00034","bibtex":"@Article{guntherExaminingUseTemporalDifference2020,\n title = {Examining the {{Use}} of {{Temporal-Difference Incremental Delta-Bar-Delta}} for {{Real-World Predictive Knowledge Architectures}}},\n author = {G{\\\"u}nther, Johannes and Ady, Nadia M. and Kearney, Alex and Dawson, Michael R. and Pilarski, Patrick M.},\n year = {2020},\n doi = {10.3389/frobt.2020.00034},\n urldate = {2024-10-29},\n journal = {Frontiers in Robotics and AI},\n volume = {7},\n pages = {34},\n file = {/home/kellen/Zotero/storage/3N55F4WQ/Gnther et al. - 2020 - Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for Real-World Predictive Knowl.pdf},\n langid = {english},\n issn = {2296-9144},\n month = {March}\n}"}},"shoeybiMegatronLMTrainingMultiBillion2020":{"title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism","taxon":"Reference","tags":[],"route":"shoeybiMegatronLMTrainingMultiBillion2020.xml","metas":{"external":"Https://arxiv.org/abs/1909.08053","bibtex":"@Misc{shoeybiMegatronLMTrainingMultiBillion2020,\n title = {Megatron-{{LM}}: {{Training Multi-Billion Parameter Language Models Using Model Parallelism}}},\n author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},\n year = {2020},\n urldate = {2024-09-18},\n number = {arXiv:1909.08053},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/JMJ9WPZ9/Shoeybi et al. - 2020 - Megatron-LM Training Multi-Billion Parameter Language Models Using Model Parallelism.pdf},\n keywords = {Computer Science - Computation and Language},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76\\% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30\\% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5\\% compared to SOTA accuracy of 63.2\\%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9\\% compared to SOTA accuracy of 89.4\\%).},\n primaryclass = {cs},\n eprint = {1909.08053},\n month = {March},\n shorttitle = {Megatron-{{LM}}}\n}"}},"NEURIPS2020_1457c0d6":{"title":"Language models are few-shot learners","taxon":"Reference","tags":[],"route":"NEURIPS2020_1457c0d6.xml","metas":{"bibtex":"@Inproceedings{NEURIPS2020_1457c0d6,\n title = {Language Models Are Few-Shot Learners},\n author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},\n year = {2020},\n booktitle = {Advances in Neural Information Processing Systems},\n editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},\n volume = {33},\n pages = {1877--1901},\n publisher = {Curran Associates, Inc.}\n}"}},"nachumOptimalRepresentationLearning2019":{"title":"Near-Optimal Representation Learning for Hierarchical Reinforcement Learning","taxon":"Reference","tags":["paper","todo"],"route":"nachumOptimalRepresentationLearning2019.xml","metas":{"external":"Https://arxiv.org/abs/1810.01257","bibtex":"@Misc{nachumOptimalRepresentationLearning2019,\n title = {Near-{{Optimal Representation Learning}} for {{Hierarchical Reinforcement Learning}}},\n author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},\n year = {2019},\n urldate = {2024-11-07},\n number = {arXiv:1810.01257},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/SIK848RC/Nachum et al. - 2019 - Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {We study the problem of representation learning in goal-conditioned hierarchical reinforcement learning. In such hierarchical structures, a higher-level controller solves tasks by iteratively communicating goals which a lower-level policy is trained to reach. Accordingly, the choice of representation -- the mapping of observation space to goal space -- is crucial. To study this problem, we develop a notion of sub-optimality of a representation, defined in terms of expected reward of the optimal hierarchical policy using this representation. We derive expressions which bound the sub-optimality and show how these expressions can be translated to representation learning objectives which may be optimized in practice. Results on a number of difficult continuous-control tasks show that our approach to representation learning yields qualitatively better representations as well as quantitatively better hierarchical policies, compared to existing methods (see videos at https://sites.google.com/view/representation-hrl).},\n primaryclass = {cs},\n eprint = {1810.01257},\n month = {January}\n}"}},"oordRepresentationLearningContrastive2019":{"title":"Representation Learning with Contrastive Predictive Coding","taxon":"Reference","tags":[],"route":"oordRepresentationLearningContrastive2019.xml","metas":{"external":"Https://arxiv.org/abs/1807.03748","bibtex":"@Misc{oordRepresentationLearningContrastive2019,\n title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},\n author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},\n year = {2019},\n urldate = {2024-11-21},\n number = {arXiv:1807.03748},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/8ZBPFQEK/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf;/home/kellen/Zotero/storage/H9PLPKN6/Oord et al. - 2019 - Representation Learning with Contrastive Predictive Coding.pdf},\n keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},\n primaryclass = {cs},\n eprint = {1807.03748},\n month = {January}\n}"}},"nachumDataEfficientHierarchicalReinforcement2018":{"title":"Data-Efficient Hierarchical Reinforcement Learning","taxon":"Reference","tags":[],"route":"nachumDataEfficientHierarchicalReinforcement2018.xml","metas":{"external":"Https://arxiv.org/abs/1805.08296","bibtex":"@Misc{nachumDataEfficientHierarchicalReinforcement2018,\n title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},\n author = {Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},\n year = {2018},\n urldate = {2024-10-24},\n number = {arXiv:1805.08296},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/E5S647TN/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},\n langid = {english},\n archiveprefix = {arXiv},\n abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher- and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We term the resulting HRL agent HIRO and find that it is generally applicable and highly sample-efficient. Our experiments show that HIRO can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},\n primaryclass = {cs},\n eprint = {1805.08296},\n month = {October}\n}"}},"chenTVMAutomatedEndtoEnd2018":{"title":"TVM: An Automated End-to-End Optimizing Compiler for Deep Learning","taxon":"Reference","tags":[],"route":"chenTVMAutomatedEndtoEnd2018.xml","metas":{"external":"Https://arxiv.org/abs/1802.04799","bibtex":"@Misc{chenTVMAutomatedEndtoEnd2018,\n title = {{{TVM}}: {{An Automated End-to-End Optimizing Compiler}} for {{Deep Learning}}},\n author = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Cowan, Meghan and Shen, Haichen and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},\n year = {2018},\n urldate = {2024-11-07},\n number = {arXiv:1802.04799},\n publisher = {arXiv},\n file = {/home/kellen/Zotero/storage/IX3XN5F4/Chen et al. - 2018 - TVM An Automated End-to-End Optimizing Compiler for Deep Learning.pdf},\n keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},\n archiveprefix = {arXiv},\n abstract = {There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.},\n primaryclass = {cs},\n eprint = {1802.04799},\n month = {October},\n shorttitle = {{{TVM}}}\n}"}},"liMinimap2PairwiseAlignment2018":{"title":"Minimap2: Pairwise alignment for nucleotide sequences","taxon":"Reference","tags":[],"route":"liMinimap2PairwiseAlignment2018.xml","metas":{"doi":"10.1093/Bioinformatics/bty191","bibtex":"@Article{liMinimap2PairwiseAlignment2018,\n title = {Minimap2: Pairwise Alignment for Nucleotide Sequences},\n author = {Li, Heng},\n year = {2018},\n doi = {10.1093/bioinformatics/bty191},\n urldate = {2024-12-11},\n journal = {Bioinformatics},\n editor = {Birol, Inanc},\n volume = {34},\n number = {18},\n pages = {3094--3100},\n file = {/home/kellen/Zotero/storage/U4V4YFWK/Li - 2018 - Minimap2 pairwise alignment for nucleotide sequen.pdf},\n langid = {english},\n copyright = {https://academic.oup.com/journals/pages/open\\_access/funder\\_policies/chorus/standard\\_publication\\_model},\n abstract = {Motivation: Recent advances in sequencing technologies promise ultra-long reads of \\$100 kb in average, full-length mRNA or cDNA reads in high throughput and genomic contigs over 100 Mb in length. Existing alignment programs are unable or inefficient to process such data at scale, which presses for the development of new alignment algorithms.},\n issn = {1367-4803, 1367-4811},\n month = {September},\n shorttitle = {Minimap2}\n}"}},"suttonReinforcementLearningIntroduction2018":{"title":"Reinforcement learning: An introduction","taxon":"Reference","tags":[],"route":"suttonReinforcementLearningIntroduction2018.xml","metas":{"bibtex":"@Book{suttonReinforcementLearningIntroduction2018,\n title = {Reinforcement Learning: An Introduction},\n author = {Sutton, Richard S. and Barto, Andrew G.},\n year = {2018},\n isbn = {978-0-262-03924-6},\n edition = {Second edition},\n series = {Adaptive Computation and Machine Learning Series},\n publisher = {The MIT Press},\n address = {Cambridge, Massachusetts},\n file = {/home/kellen/Zotero/storage/DY4UI6G7/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},\n keywords = {Reinforcement learning},\n lccn = {Q325.6 .R45 2018},\n langid = {english},\n abstract = {\"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms.\"--},\n shorttitle = {Reinforcement Learning}\n}"}},"diestelGraphTheory2017":{"title":"Graph Theory","taxon":"Reference","tags":[],"route":"diestelGraphTheory2017.xml","metas":{"doi":"10.1007/978-3-662-53622-3","bibtex":"@Book{diestelGraphTheory2017,\n title = {Graph {{Theory}}},\n author = {Diestel, Reinhard},\n year = {2017},\n isbn = {978-3-662-53621-6 978-3-662-53622-3},\n doi = {10.1007/978-3-662-53622-3},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {173},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Zotero/storage/I3MHRRZZ/Diestel - 2017 - Graph Theory.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"schulman2017proximalpolicyoptimizationalgorithms":{"title":"Proximal policy optimization algorithms","taxon":"Reference","tags":[],"route":"schulman2017proximalpolicyoptimizationalgorithms.xml","metas":{"external":"Https://arxiv.org/abs/1707.06347","bibtex":"@Misc{schulman2017proximalpolicyoptimizationalgorithms,\n title = {Proximal Policy Optimization Algorithms},\n author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec\nRadford and Oleg Klimov},\n year = {2017},\n url = {https://arxiv.org/abs/1707.06347},\n primaryclass = {cs.LG},\n archiveprefix = {arXiv},\n eprint = {1707.06347}\n}"}},"durrettEssentialsStochasticProcesses2016":{"title":"Essentials of Stochastic Processes","taxon":"Reference","tags":[],"route":"durrettEssentialsStochasticProcesses2016.xml","metas":{"doi":"10.1007/978-3-319-45614-0","bibtex":"@Book{durrettEssentialsStochasticProcesses2016,\n title = {Essentials of {{Stochastic Processes}}},\n author = {Durrett, Richard},\n year = {2016},\n isbn = {978-3-319-45613-3 978-3-319-45614-0},\n doi = {10.1007/978-3-319-45614-0},\n urldate = {2024-10-18},\n series = {Springer {{Texts}} in {{Statistics}}},\n publisher = {Springer International Publishing},\n address = {Cham},\n file = {/home/kellen/Zotero/storage/MVRAWH8Q/Durrett - 2016 - Essentials of Stochastic Processes.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"leeIntroductionSmoothManifolds2012":{"title":"Introduction to Smooth Manifolds","taxon":"Reference","tags":[],"route":"leeIntroductionSmoothManifolds2012.xml","metas":{"doi":"10.1007/978-1-4419-9982-5","bibtex":"@Book{leeIntroductionSmoothManifolds2012,\n title = {Introduction to {{Smooth Manifolds}}},\n author = {Lee, John M.},\n year = {2012},\n isbn = {978-1-4419-9981-8 978-1-4419-9982-5},\n doi = {10.1007/978-1-4419-9982-5},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {218},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Zotero/storage/GFNZMLM6/Lee - 2012 - Introduction to Smooth Manifolds.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"cinlarProbabilityStochastics2011":{"title":"Probability and Stochastics","taxon":"Reference","tags":[],"route":"cinlarProbabilityStochastics2011.xml","metas":{"doi":"10.1007/978-0-387-87859-1","bibtex":"@Book{cinlarProbabilityStochastics2011,\n title = {Probability and {{Stochastics}}},\n author = {{\\c C}inlar, Erhan},\n year = {2011},\n isbn = {978-0-387-87858-4 978-0-387-87859-1},\n doi = {10.1007/978-0-387-87859-1},\n urldate = {2024-10-18},\n series = {Graduate {{Texts}} in {{Mathematics}}},\n volume = {261},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Zotero/storage/GHJPXQJR/inlar - 2011 - Probability and Stochastics.pdf},\n langid = {english},\n copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining}\n}"}},"gutmannNoisecontrastiveEstimationNew2010":{"title":"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models","taxon":"Reference","tags":[],"route":"gutmannNoisecontrastiveEstimationNew2010.xml","metas":{"bibtex":"@Inproceedings{gutmannNoisecontrastiveEstimationNew2010,\n title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},\n author = {Gutmann, Michael and Hyv{\\\"a}rinen, Aapo},\n year = {2010},\n urldate = {2025-01-24},\n booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},\n pages = {297--304},\n publisher = {{JMLR Workshop and Conference Proceedings}},\n file = {/home/kellenkanarios/Downloads/Papers/Papers/Contrastive RL/Gutmann_Hyvrinen_2010_Noise-contrastive estimation.pdf},\n langid = {english},\n abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.},\n issn = {1938-7228},\n month = {March},\n shorttitle = {Noise-Contrastive Estimation}\n}"}},"nocedalNumericalOptimization2006":{"title":"Numerical optimization","taxon":"Reference","tags":[],"route":"nocedalNumericalOptimization2006.xml","metas":{"bibtex":"@Book{nocedalNumericalOptimization2006,\n title = {Numerical Optimization},\n author = {Nocedal, Jorge and Wright, Stephen J.},\n year = {2006},\n isbn = {978-0-387-30303-1},\n edition = {2nd ed},\n series = {Springer Series in Operations Research},\n publisher = {Springer},\n address = {New York},\n file = {/home/kellen/Zotero/storage/ZNRAHE63/Nocedal and Wright - 2006 - Numerical optimization.pdf},\n annotation = {OCLC: ocm68629100},\n keywords = {Mathematical optimization},\n lccn = {QA402.5 .N62 2006},\n langid = {english}\n}"}},"oksendalStochasticDifferentialEquations2003":{"title":"Stochastic Differential Equations","taxon":"Reference","tags":[],"route":"oksendalStochasticDifferentialEquations2003.xml","metas":{"doi":"10.1007/978-3-642-14394-6","bibtex":"@Book{oksendalStochasticDifferentialEquations2003,\n title = {Stochastic {{Differential Equations}}},\n author = {{\\O}ksendal, Bernt},\n year = {2003},\n isbn = {978-3-540-04758-2 978-3-642-14394-6},\n doi = {10.1007/978-3-642-14394-6},\n urldate = {2025-01-25},\n series = {Universitext},\n publisher = {Springer Berlin Heidelberg},\n address = {Berlin, Heidelberg},\n file = {/home/kellen/Zotero/storage/WSH4RNVC/ksendal - 2003 - Stochastic Differential Equations.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm}\n}"}},"perkoDifferentialEquationsDynamical2001":{"title":"Differential Equations and Dynamical Systems","taxon":"Reference","tags":[],"route":"perkoDifferentialEquationsDynamical2001.xml","metas":{"doi":"10.1007/978-1-4613-0003-8","bibtex":"@Book{perkoDifferentialEquationsDynamical2001,\n title = {Differential {{Equations}} and {{Dynamical Systems}}},\n author = {Perko, Lawrence},\n year = {2001},\n isbn = {978-1-4612-6526-9 978-1-4613-0003-8},\n doi = {10.1007/978-1-4613-0003-8},\n urldate = {2025-01-25},\n series = {Texts in {{Applied Mathematics}}},\n editor = {Marsden, J. E. and Sirovich, L. and Golubitsky, M.},\n volume = {7},\n publisher = {Springer New York},\n address = {New York, NY},\n file = {/home/kellen/Zotero/storage/FRK58JEY/Perko - 2001 - Differential Equations and Dynamical Systems.pdf},\n langid = {english},\n copyright = {http://www.springer.com/tdm}\n}"}},"tsitsiklisAnalysisTemporaldifferenceLearning1997":{"title":"An analysis of temporal-difference learning with function approximation","taxon":"Reference","tags":[],"route":"tsitsiklisAnalysisTemporaldifferenceLearning1997.xml","metas":{"doi":"10.1109/9.580874","bibtex":"@Article{tsitsiklisAnalysisTemporaldifferenceLearning1997,\n title = {An Analysis of Temporal-Difference Learning with Function Approximation},\n author = {Tsitsiklis, J.N. and Van Roy, B.},\n year = {1997},\n doi = {10.1109/9.580874},\n urldate = {2025-01-16},\n journal = {IEEE Transactions on Automatic Control},\n volume = {42},\n number = {5},\n pages = {674--690},\n file = {/home/kellen/Zotero/storage/58WIJRVR/Tsitsiklis and Van Roy - 1997 - An analysis of temporal-difference learning with function approximation.pdf},\n langid = {english},\n copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},\n abstract = {We discuss the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of an infinite-horizon discounted Markov chain. The algorithm we analyze updates parameters of a linear function approximator online during a single endless trajectory of an irreducible aperiodic Markov chain with a finite or infinite state space. We present a proof of convergence (with probability one), a characterization of the limit of convergence, and a bound on the resulting approximation error. Furthermore, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning.},\n issn = {00189286},\n month = {May}\n}"}},"bertsekasNeurodynamicProgramming1996":{"title":"Neuro-dynamic programming","taxon":"Reference","tags":[],"route":"bertsekasNeurodynamicProgramming1996.xml","metas":{"bibtex":"@Book{bertsekasNeurodynamicProgramming1996,\n title = {Neuro-Dynamic Programming},\n author = {Bertsekas, Dimitri P. and Tsitsiklis, John N.},\n year = {1996},\n isbn = {978-1-886529-10-6},\n series = {Optimization and Neural Computation Series},\n publisher = {Athena Scientific},\n address = {Belmont, Mass},\n file = {/home/kellen/Zotero/storage/JEIDXXG5/Bertsekas and Tsitsiklis - 1996 - Neuro-dynamic programming.pdf},\n keywords = {Dynamic programming,Mathematical optimization,Neural networks (Computer Science)},\n lccn = {QA76.87 .B47 1996},\n langid = {english}\n}"}},"billingsleyProbabilityMeasure1995":{"title":"Probability and measure","taxon":"Reference","tags":[],"route":"billingsleyProbabilityMeasure1995.xml","metas":{"bibtex":"@Book{billingsleyProbabilityMeasure1995,\n title = {Probability and Measure},\n author = {Billingsley, Patrick},\n year = {1995},\n isbn = {978-0-471-00710-4},\n edition = {3. ed},\n series = {Wiley Series in Probability and Mathematical Statistics},\n publisher = {Wiley},\n address = {New York, NY},\n file = {/home/kellen/Zotero/storage/JSCDN2UH/Billingsley - 1995 - Probability and measure.pdf},\n langid = {english}\n}"}},"kak-005E":{"title":null,"taxon":null,"tags":[],"route":"kak-005E.xml","metas":{}},"latex-preamble":{"title":null,"taxon":null,"tags":[],"route":"latex-preamble.xml","metas":{}},"yaoConstraintConditionedPolicyOptimization":{"title":"Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning","taxon":"Reference","tags":[],"route":"yaoConstraintConditionedPolicyOptimization.xml","metas":{"bibtex":"@Article{yaoConstraintConditionedPolicyOptimization,\n title = {Constraint-{{Conditioned Policy Optimization}} for {{Versatile Safe Reinforcement Learning}}},\n author = {Yao, Yihang and Liu, Zuxin and Cen, Zhepeng and Zhu, Jiacheng and Yu, Wenhao and Zhang, Tingnan and Zhao, Ding},\n file = {/home/kellen/Zotero/storage/L9L6VL45/Yao et al. - Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning.pdf},\n langid = {english},\n abstract = {Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Constraint-Conditioned Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance, while preserving zero-shot adaptation capabilities to different constraint thresholds data-efficiently. This makes our approach suitable for real-world dynamic applications.}\n}"}},"eysenbach2023":{"title":"Constrastive Learning as Goal Conditioned Reinforcement Learning","taxon":"Reference","tags":["contrastive"],"route":"eysenbach2023.xml","metas":{"abstract":"In reinforcement learning (RL), it is easier to solve a task if\n              given a good representation. While deep RL should automatically\n              acquire such good representations, prior work often finds that\n              learning representations in an end-to-end fashion is unstable and\n              instead equip RL algorithms with additional representation learning\n              parts (e.g., auxiliary losses, data augmentation). How can we\n              design RL algorithms that directly acquire good representations? In\n              this paper, instead of adding representation learning parts to an\n              existing RL algorithm, we show (contrastive) representation\n              learning methods can be cast as RL algorithms in their own right.\n              To do this, we build upon prior work and apply contrastive\n              representation learning to action-labeled trajectories, in such a\n              way that the (inner product of) learned representations exactly\n              corresponds to a goal-conditioned value function. We use this idea\n              to reinterpret a prior RL method as performing contrastive learning\n              , and then use the idea to propose a much simpler method that\n              achieves similar performance. Across a range of goal-conditioned RL\n              tasks, we demonstrate that contrastive RL methods achieve higher\n              success rates than prior non-contrastive methods, including in the\n              offline RL setting. We also show that contrastive RL outperforms\n              prior methods on image-based tasks, without using data augmentation\n              or auxiliary objectives.","doi":"10.48550/ArXiv.2206.07568","bibtex":"@Misc{eysenbachContrastiveLearningGoalConditioned2023,\n  title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning\n           }}},\n  author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and\n            Levine, Sergey},\n  year = {2023},\n  month = feb,\n  number = {arXiv:2206.07568},\n  eprint = {2206.07568},\n  primaryclass = {cs},\n  publisher = {arXiv},\n  urldate = {2024-09-06},\n  archiveprefix = {arXiv},\n  keywords = {Computer Science - Artificial Intelligence,Computer Science -\n              Machine Learning},\n  file = {/home/kellen/Zotero/storage/WVQKIMN4/Eysenbach et al. - 2023 -\n          Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},\n}"}},"mcleod2022":{"title":"Continual Auxiliary Task Learning","taxon":"Reference","tags":["potw"],"route":"mcleod2022.xml","metas":{"abstract":"Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system.","doi":"10.48550/ArXiv.2202.11133","bibtex":"@Misc{mcleod2022continualauxiliarytasklearning,\n      title={Continual Auxiliary Task Learning}, \n      author={Matthew McLeod and Chunlok Lo and Matthew Schlegel and Andrew Jacobsen and Raksha Kumaraswamy and Martha White and Adam White},\n      year={2022},\n      eprint={2202.11133},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2202.11133}, \n}"}},"vandenOord2018":{"title":"Contrastive Predictive Decoding","taxon":"Reference","tags":["contrastive"],"route":"vandenOord2018.xml","metas":{"doi":"10.48550/ArXiv.1807.03748","bibtex":"@Misc{oord2019representationlearningcontrastivepredictive,\n      title={Representation Learning with Contrastive Predictive Coding}, \n      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},\n      year={2019},\n      eprint={1807.03748},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/1807.03748}, \n}"}},"zheng2024":{"title":"Contrastive Difference Predictive Coding","taxon":"Reference","tags":["contrastive","potw"],"route":"zheng2024.xml","metas":{"bibtex":"@Misc{zheng2024contrastivedifferencepredictivecoding,\n      title={Contrastive Difference Predictive Coding}, \n      author={Chongyi Zheng and Ruslan Salakhutdinov and Benjamin Eysenbach},\n      year={2024},\n      eprint={2310.20141},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2310.20141}, \n}"}},"elsayedDeepReinforcementLearning":{"title":"Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates","taxon":"Reference","tags":[],"route":"elsayedDeepReinforcementLearning.xml","metas":{"bibtex":"@Article{elsayedDeepReinforcementLearning,\n title = {Deep {{Reinforcement Learning Without Experience Replay}}, {{Target Networks}}, or {{Batch Updates}}},\n author = {Elsayed, Mohamed and Vasan, Gautham and Mahmood, A Rupam},\n file = {/home/kellen/Zotero/storage/XDJZFXVT/Elsayed et al. - Deep Reinforcement Learning Without Experience Replay, Target Networks, or Batch Updates.pdf},\n langid = {english},\n abstract = {Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resource-constrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.}\n}"}},"coverELEMENTSINFORMATIONTHEORY":{"title":"ELEMENTS OF INFORMATION THEORY","taxon":"Reference","tags":[],"route":"coverELEMENTSINFORMATIONTHEORY.xml","metas":{"bibtex":"@Article{coverELEMENTSINFORMATIONTHEORY,\n title = {{{ELEMENTS OF INFORMATION THEORY}}},\n author = {Cover, Thomas M and Thomas, Joy A},\n file = {/home/kellen/Zotero/storage/C6AJGW5I/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf},\n langid = {english}\n}"}},"zisselmanExploreGeneralizeZeroShot":{"title":"Explore to Generalize in Zero-Shot RL","taxon":"Reference","tags":[],"route":"zisselmanExploreGeneralizeZeroShot.xml","metas":{"bibtex":"@Article{zisselmanExploreGeneralizeZeroShot,\n title = {Explore to {{Generalize}} in {{Zero-Shot RL}}},\n author = {Zisselman, Ev and Lavie, Itai and Soudry, Daniel and Tamar, Aviv},\n file = {/home/kellen/Zotero/storage/9D7NJ4H9/Zisselman et al. - Explore to Generalize in Zero-Shot RL.pdf},\n langid = {english},\n abstract = {We study zero-shot generalization in reinforcement learning---optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariancebased approaches. Our Explore to Generalize algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions, which generalize well and drive us to a novel part of the state space, where the ensemble may potentially agree again. We show that our approach is the state-of-the-art on tasks of the ProcGen challenge that have thus far eluded effective generalization, yielding a success rate of 83\\% on the Maze task and 74\\% on Heist with 200 training levels. ExpGen can also be combined with an invariance based approach to gain the best of both worlds, setting new state-of-the-art results on ProcGen. Code available at https://github.com/EvZissel/expgen.}\n}"}},"kulkarniHierarchicalDeepReinforcement":{"title":"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation","taxon":"Reference","tags":[],"route":"kulkarniHierarchicalDeepReinforcement.xml","metas":{"bibtex":"@Article{kulkarniHierarchicalDeepReinforcement,\n title = {Hierarchical {{Deep Reinforcement Learning}}: {{Integrating Temporal Abstraction}} and {{Intrinsic Motivation}}},\n author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},\n file = {/home/kellen/Zotero/storage/A2XACTEA/Kulkarni et al. - Hierarchical Deep Reinforcement Learning Integrat.pdf},\n langid = {english},\n abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchicalDQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game --`Montezuma's Revenge'.}\n}"}},"vershyninHighDimensionalProbability":{"title":"High-Dimensional Probability","taxon":"Reference","tags":[],"route":"vershyninHighDimensionalProbability.xml","metas":{"bibtex":"@Article{vershyninHighDimensionalProbability,\n title = {High-{{Dimensional Probability}}},\n author = {Vershynin, Roman},\n file = {/home/kellen/Zotero/storage/DC3KWF38/Vershynin - High-Dimensional Probability.pdf},\n langid = {english}\n}"}},"kak-0001":{"title":"Kellen Kanarios","taxon":null,"tags":["home"],"route":"index.xml","metas":{"author":"False"}},"kellenkanarios":{"title":"Kellen Kanarios","taxon":"Person","tags":[],"route":"kellenkanarios.xml","metas":{"external":"Https://kellenkanarios.github.io/","institution":"[University of Michigan]","position":"PhD Student"}},"borgersLectureNotesGame":{"title":"Lecture Notes on Game Theory","taxon":"Reference","tags":[],"route":"borgersLectureNotesGame.xml","metas":{"bibtex":"@Article{borgersLectureNotesGame,\n title = {Lecture {{Notes}} on {{Game Theory}}},\n author = {Borgers, Tilman},\n file = {/home/kellen/Zotero/storage/CYJDCVHC/Borgers - Lecture Notes on Game Theory.pdf},\n langid = {english}\n}"}},"leiying":{"title":"Lei Ying","taxon":"Person","tags":[],"route":"leiying.xml","metas":{"external":"Https://leiying.engin.umich.edu/","institution":"University of Michigan","position":"Professor"}},"algebra":{"title":"Math 494 Notes","taxon":"Reference","tags":["texnote"],"route":"algebra.xml","metas":{"external":"Notes/494-note.pdf"}},"analysis":{"title":"Math 395 Notes","taxon":"Reference","tags":["texnote"],"route":"analysis.xml","metas":{"external":"Notes/395-note.pdf"}},"functional":{"title":"Math 602 Notes","taxon":"Reference","tags":["texnote"],"route":"functional.xml","metas":{"external":"Notes/602-note.pdf"}},"measure":{"title":"Math 597 Notes","taxon":"Reference","tags":["texnote"],"route":"measure.xml","metas":{"external":"Notes/597-note.pdf"}},"nanjiang":{"title":"Nan Jiang","taxon":"Person","tags":[],"route":"nanjiang.xml","metas":{"external":"Https://nanjiang.cs.illinois.edu/","institution":"University of Illinois Urbana-Champaign","position":"Professor"}},"nachum2019":{"title":"Near Optimal Representation Learning for Hierarchical Reinforcement Learning","taxon":"Reference","tags":["contrastive","potw"],"route":"nachum2019.xml","metas":{"bibtex":"@Misc{nachum2019nearoptimalrepresentationlearninghierarchical,\n      title={Near-Optimal Representation Learning for Hierarchical Reinforcement Learning}, \n      author={Ofir Nachum and Shixiang Gu and Honglak Lee and Sergey Levine},\n      year={2019},\n      eprint={1810.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/1810.01257}, \n}"}},"gutmann2012":{"title":"Noise-contrastive Estimation","taxon":"Reference","tags":["contrastive"],"route":"gutmann2012.xml","metas":{"abstract":"We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.","bibtex":"@InProceedings{pmlr-v9-gutmann10a,\n  title = \t {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},\n  author = \t {Gutmann, Michael and Hyvrinen, Aapo},\n  booktitle = \t {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},\n  pages = \t {297--304},\n  year = \t {2010},\n  editor = \t {Teh, Yee Whye and Titterington, Mike},\n  volume = \t {9},\n  series = \t {Proceedings of Machine Learning Research},\n  address = \t {Chia Laguna Resort, Sardinia, Italy},\n  month = \t {13--15 May},\n  publisher =    {PMLR},\n  pdf = \t {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},\n  url = \t {https://proceedings.mlr.press/v9/gutmann10a.html},\n}"}},"huangOccupancybasedPolicyGradient":{"title":"Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality","taxon":"Reference","tags":[],"route":"huangOccupancybasedPolicyGradient.xml","metas":{"bibtex":"@Article{huangOccupancybasedPolicyGradient,\n title = {Occupancy-Based {{Policy Gradient}}: {{Estimation}}, {{Convergence}}, and {{Optimality}}},\n author = {Huang, Audrey and Jiang, Nan},\n file = {/home/kellen/Zotero/storage/P7NI2N95/Huang and Jiang - Occupancy-based Policy Gradient Estimation, Convergence, and Optimality.pdf},\n langid = {english},\n abstract = {Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.}\n}"}},"jaakkolaConvergenceStochasticIterative":{"title":"On the Convergence of Stochastic Iterative Dynamic Programming Algorithms","taxon":"Reference","tags":[],"route":"jaakkolaConvergenceStochasticIterative.xml","metas":{"bibtex":"@Article{jaakkolaConvergenceStochasticIterative,\n title = {On the {{Convergence}} of {{Stochastic Iterative Dynamic Programming Algorithms}}},\n author = {Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},\n file = {/home/kellen/Zotero/storage/95ZWX95X/Jaakkola et al. - On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf},\n langid = {english},\n abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.}\n}"}},"silberschatzOperatingSystemConcepts":{"title":"Operating System Concepts","taxon":"Reference","tags":[],"route":"silberschatzOperatingSystemConcepts.xml","metas":{"bibtex":"@Article{silberschatzOperatingSystemConcepts,\n title = {Operating {{System Concepts}}},\n author = {Silberschatz, Abraham and Galvin, Peter Baer and Gagne, Greg},\n file = {/home/kellen/Zotero/storage/GI3ESXU4/Silberschatz et al. - Operating System Concepts.pdf}\n}"}},"eysenbachProbabilisticReinforcementLearning":{"title":"Probabilistic Reinforcement Learning: Using Data to Define Desired Outcomes and Inferring How to Get There","taxon":"Reference","tags":[],"route":"eysenbachProbabilisticReinforcementLearning.xml","metas":{"bibtex":"@Article{eysenbachProbabilisticReinforcementLearning,\n title = {Probabilistic {{Reinforcement Learning}}: {{Using Data}} to {{Define Desired Outcomes}} and {{Inferring How}} to {{Get There}}},\n author = {Eysenbach, Benjamin},\n file = {/home/kellen/Zotero/storage/PYCTKQX2/Eysenbach - Probabilistic Reinforcement Learning Using Data to Define Desired Outcomes and Inferring How to Get.pdf},\n langid = {english}\n}"}},"billingsley1986":{"title":"Probability and Measure","taxon":"Reference","tags":["selfstudy"],"route":"billingsley1986.xml","metas":{"bibtex":"@Book{Bill86,\n  Title                    = {Probability and Measure},\n  Author                   = {Patrick Billingsley},\n  Publisher                = {John Wiley and Sons},\n  Year                     = {1986},\n  Edition                  = {Second}\n}"}},"qiningzhang":{"title":"Qining Zhang","taxon":"Person","tags":[],"route":"qiningzhang.xml","metas":{"external":"Https://thumichzqn.github.io/","institution":"[University of Michigan]","position":"PhD Student"}},"rahimiRandomFeaturesLargeScale":{"title":"Random Features for Large-Scale Kernel Machines","taxon":"Reference","tags":[],"route":"rahimiRandomFeaturesLargeScale.xml","metas":{"bibtex":"@Article{rahimiRandomFeaturesLargeScale,\n title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},\n author = {Rahimi, Ali and Recht, Benjamin},\n file = {/home/kellen/Zotero/storage/9NSANMTT/013a006f03dbc5392effeb8f18fda755-Paper.pdf},\n langid = {english},\n abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.}\n}"}},"agarwalReinforcementLearningTheory":{"title":"Reinforcement Learning: Theory and Algorithms","taxon":"Reference","tags":[],"route":"agarwalReinforcementLearningTheory.xml","metas":{"bibtex":"@Article{agarwalReinforcementLearningTheory,\n title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},\n author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},\n file = {/home/kellen/Zotero/storage/Z3QU86C2/Agarwal et al. - Reinforcement Learning Theory and Algorithms.pdf},\n langid = {english}\n}"}},"jiang2024":{"title":"Reinforcement Learning: Theory and Algorithms","taxon":"Reference","tags":[],"route":"jiang2024.xml","metas":{"external":"Https://rltheorybook.github.io/rltheorybook_AJKS.pdf"}},"rlc":{"title":"Reinforcement Learning Conference","taxon":"Conference","tags":[],"route":"rlc.xml","metas":{"external":"Https://rl-conference.cc/"}},"sutton2022":{"title":"Reinforcement Learning: An Introduction","taxon":"Reference","tags":[],"route":"sutton2022.xml","metas":{"external":"Http://incompleteideas.net/book/the-book-2nd.html"}},"richardsutton":{"title":"Richard Sutton","taxon":"Person","tags":[],"route":"richardsutton.xml","metas":{"external":"Http://incompleteideas.net","institution":"University of Alberta","position":"Professor"}},"StochasticApproximationAlgorithms":{"title":"Stochastic Approximation Algorithms and Applications","taxon":"Reference","tags":[],"route":"StochasticApproximationAlgorithms.xml","metas":{"bibtex":"@Book{StochasticApproximationAlgorithms,\n title = {Stochastic {{Approximation Algorithms}} and {{Applications}}},\n file = {/home/kellen/Zotero/storage/6IGQEBVP/Stochastic Approximation Algorithms and Applications.pdf}\n}"}},"umich":{"title":"University of Michigan","taxon":"Institute","tags":[],"route":"umich.xml","metas":{"external":"Https://umich.edu/"}},"base-macros":{"title":"Basic macros","taxon":null,"tags":[],"route":"base-macros.xml","metas":{}},"loading":{"title":"Blog  Coming Soon!!!","taxon":null,"tags":[],"route":"loading.xml","metas":{}},"log":{"title":"Disk  Notebook","taxon":null,"tags":[],"route":"log.xml","metas":{}},"rn":{"title":"Kellen Kanarios  News","taxon":null,"tags":[],"route":"rn.xml","metas":{}},"meet":{"title":"Kellen Kanarios  Wanna Talk?","taxon":null,"tags":[],"route":"meet.xml","metas":{}},"pub":{"title":"Kellen Kanarios  research","taxon":null,"tags":[],"route":"pub.xml","metas":{}}}
