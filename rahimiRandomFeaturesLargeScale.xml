<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3566</fr:anchor><fr:addr
type="user">rahimiRandomFeaturesLargeScale</fr:addr><fr:route>rahimiRandomFeaturesLargeScale.xml</fr:route><fr:title
text="Random Features for Large-Scale Kernel Machines">Random Features for Large-Scale Kernel Machines</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author>Ali Rahimi</fr:author><fr:author>Benjamin Recht</fr:author></fr:authors><fr:meta
name="bibtex"><![CDATA[@article{rahimiRandomFeaturesLargeScale,
 title = {Random {{Features}} for {{Large-Scale Kernel Machines}}},
 author = {Rahimi, Ali and Recht, Benjamin},
 file = {/home/kellen/Zotero/storage/9NSANMTT/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 langid = {english},
 abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shiftinvariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>