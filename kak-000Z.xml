<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2449</fr:anchor><fr:addr
type="user">kak-000Z</fr:addr><fr:route>kak-000Z.xml</fr:route><fr:title
text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>A Markov Decision Process <fr:tex
display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<fr:ul><fr:li>A state space <fr:tex
display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</fr:li>
  <fr:li>A action space <fr:tex
display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[A]]></fr:tex> is finite.</fr:li>
  <fr:li>A transition function <fr:tex
display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex
display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex
display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex
display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex
display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex
display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>.</fr:li>
  <fr:li>A reward function <fr:tex
display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex
display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex
display="inline"><![CDATA[a]]></fr:tex>, <fr:tex
display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</fr:li>
  <fr:li>A discount factor <fr:tex
display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Context">Context</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2450</fr:anchor><fr:addr
type="user">kak-000Y</fr:addr><fr:route>kak-000Y.xml</fr:route><fr:title
text="Markov Decision Processes">Markov Decision Processes</fr:title><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>In reinforcement learning, the interactions between the agent and the environment are often described by an infinite-horizon, discounted Markov Decision Process (MDP).</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2442</fr:anchor><fr:addr
type="user">kak-000Z</fr:addr><fr:route>kak-000Z.xml</fr:route><fr:title
text="Markov Decision Process">Markov Decision Process</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>A Markov Decision Process <fr:tex
display="inline"><![CDATA[M=(S,A,P,\tau ,\gamma ,\mu )]]></fr:tex> is a tuple, where

<fr:ul><fr:li>A state space <fr:tex
display="inline"><![CDATA[S]]></fr:tex>, which may be finite or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[S]]></fr:tex> is finite or countably infinite.</fr:li>
  <fr:li>A action space <fr:tex
display="inline"><![CDATA[A]]></fr:tex>, which also may be discrete or infinite. For mathematical convenience, we will assume that <fr:tex
display="inline"><![CDATA[A]]></fr:tex> is finite.</fr:li>
  <fr:li>A transition function <fr:tex
display="inline"><![CDATA[P:S\times  A\to \Delta (S)]]></fr:tex>, where <fr:tex
display="inline"><![CDATA[\Delta (S)]]></fr:tex> is the space of probability distributions over <fr:tex
display="inline"><![CDATA[S]]></fr:tex> (i.e., the below) <fr:tex
display="inline"><![CDATA[P_{S}]]></fr:tex> is <fr:tex
display="inline"><![CDATA[\mathcal {P}(S|\tau _{0})]]></fr:tex> is the probability of transitioning to state <fr:tex
display="inline"><![CDATA[s^{\prime }]]></fr:tex> upon taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>.</fr:li>
  <fr:li>A reward function <fr:tex
display="inline"><![CDATA[r: S\times  A \to  [0,1]]]></fr:tex>. <fr:tex
display="inline"><![CDATA[r(s, a)]]></fr:tex> is the immediate reward associated with taking action <fr:tex
display="inline"><![CDATA[a]]></fr:tex> in state <fr:tex
display="inline"><![CDATA[s]]></fr:tex>. More generally, the <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> could be a random variable (where the distribution depends on <fr:tex
display="inline"><![CDATA[a]]></fr:tex>, <fr:tex
display="inline"><![CDATA[0]]></fr:tex>). While we largely focus on the case where <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is deterministic, the extension to methods with stochastic rewards are often straightforward.</fr:li>
  <fr:li>A discount factor <fr:tex
display="inline"><![CDATA[\gamma  \in  (0,1)]]></fr:tex>, which defines a horizon for the problem.</fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>From an MDP and a policy <fr:tex
display="inline"><![CDATA[\pi  : S \to  A]]></fr:tex>, we can define a Value function for <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex>.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2443</fr:anchor><fr:addr
type="user">kak-0010</fr:addr><fr:route>kak-0010.xml</fr:route><fr:title
text="Value Function">Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the value function <fr:tex
display="inline"><![CDATA[V_{M}^{s}:S\to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards

<fr:tex
display="block"><![CDATA[V_{M}^{s}(s)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>The expectation is over the randomness of the the transitions <fr:tex
display="inline"><![CDATA[P]]></fr:tex> and if the policy <fr:tex
display="inline"><![CDATA[\pi ]]></fr:tex> is stochastic. When determining a policy we are more interested with the value of an action in a specific state rather than just the state itself. To get this, we can define a <fr:tex
display="inline"><![CDATA[Q]]></fr:tex>-function in a similar way.</fr:p><fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2444</fr:anchor><fr:addr
type="user">kak-0011</fr:addr><fr:route>kak-0011.xml</fr:route><fr:title
text="Action Value Function">Action Value Function</fr:title><fr:taxon>Definition</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>For a fixed policy and a starting state <fr:tex
display="inline"><![CDATA[s_{0}=s]]></fr:tex>, we define the action value function <fr:tex
display="inline"><![CDATA[Q_{M}^{s}:S \times  A \to \mathbb {R}]]></fr:tex> as the discounted sum of future rewards after taking action <fr:tex
display="inline"><![CDATA[A]]></fr:tex>.

<fr:tex
display="block"><![CDATA[Q_{M}^{s}(s, a)=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi , s_{0}=s, a_{0} = a\Big ]]]></fr:tex></fr:p></fr:mainmatter><fr:backmatter /></fr:tree><fr:p>If the reward <fr:tex
display="inline"><![CDATA[r(s,a)]]></fr:tex> is bounded by some <fr:tex
display="inline"><![CDATA[R_{\text {max}}]]></fr:tex>. Then we can trivially bound both the value and action value function by <fr:tex
display="inline"><![CDATA[(R_{max}/1 - \gamma )]]></fr:tex></fr:p>
   
   <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2445</fr:anchor><fr:addr
type="machine">#356</fr:addr><fr:route>unstable-356.xml</fr:route><fr:taxon>Proof</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>11</fr:month><fr:day>22</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
    This is just a geometric series i.e. 
<fr:tex
display="block"><![CDATA[\begin {align*} V_{M}^{s}(s)&=\mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}r(s_{t},a_{t})\mid \pi ,s_{0}=s\Big ] \\ &\leq  \mathbb {E}\Big [\sum _{t=0}^{\infty }\gamma ^{t}R_{\text {max}}\mid \pi ,s_{0}=s\Big ] \\ &= R_{\text {max}} \sum _{t=0}^{\infty }\gamma ^{t} \\ &= R_{\text {max}}/ 1 - \gamma      \end {align*}   ]]></fr:tex>
  </fr:mainmatter><fr:backmatter /></fr:tree>
 

</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>