<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3417</fr:anchor><fr:addr
type="user">mcleod2022</fr:addr><fr:route>mcleod2022.xml</fr:route><fr:title
text="Continual Auxiliary Task Learning">Continual Auxiliary Task Learning</fr:title><fr:taxon>Reference</fr:taxon><fr:authors /><fr:meta
name="abstract">
  Learning auxiliary tasks, such as multiple predictions about the world, can provide many benefits to reinforcement learning systems. A variety of off-policy learning algorithms have been developed to learn such predictions, but as yet there is little work on how to adapt the behavior to gather useful data for those off-policy predictions. In this work, we investigate a reinforcement learning system designed to learn a collection of auxiliary tasks, with a behavior policy learning to take actions to improve those auxiliary predictions. We highlight the inherent non-stationarity in this continual auxiliary task learning problem, for both prediction learners and the behavior learner. We develop an algorithm based on successor features that facilitates tracking under non-stationary rewards, and prove the separation into learning successor features and rewards provides convergence rate improvements. We conduct an in-depth study into the resulting multi-prediction learning system. 
  </fr:meta><fr:meta
name="doi">10.48550/arXiv.2202.11133</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{mcleod2022continualauxiliarytasklearning,
      title={Continual Auxiliary Task Learning}, 
      author={Matthew McLeod and Chunlok Lo and Matthew Schlegel and Andrew Jacobsen and Raksha Kumaraswamy and Martha White and Adam White},
      year={2022},
      eprint={2202.11133},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.11133}, 
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>