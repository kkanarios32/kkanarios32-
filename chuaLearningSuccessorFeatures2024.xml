<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2750</fr:anchor><fr:addr
type="user">chuaLearningSuccessorFeatures2024</fr:addr><fr:route>chuaLearningSuccessorFeatures2024.xml</fr:route><fr:title
text="Learning Successor Features the Simple Way">Learning Successor Features the Simple Way</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month></fr:date><fr:authors><fr:author>Raymond Chua</fr:author><fr:author>Arna Ghosh</fr:author><fr:author>Christos Kaplanis</fr:author><fr:author>Blake A. Richards</fr:author><fr:author>Doina Precup</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2410.22133</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2410.22133</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{chuaLearningSuccessorFeatures2024,
 title = {Learning {{Successor Features}} the {{Simple Way}}},
 author = {Chua, Raymond and Ghosh, Arna and Kaplanis, Christos and Richards, Blake A. and Precup, Doina},
 year = {2024},
 doi = {10.48550/arXiv.2410.22133},
 urldate = {2024-12-04},
 number = {arXiv:2410.22133},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/D4MGLIU6/Chua et al. - 2024 - Learning Successor Features the Simple Way.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments. Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data. More recent methods for learning SFs can avoid representation collapse, but they often involve complex losses and multiple learning phases, reducing their efficiency. We introduce a novel, simple method for learning SFs directly from pixels. Our approach uses a combination of a Temporaldifference (TD) loss and a reward prediction loss, which together capture the basic mathematical definition of SFs. We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios. As well, our technique is efficient, and can reach higher levels of performance in less time than other approaches. Our work provides a new, streamlined technique for learning SFs directly from pixel observations, with no pretraining required1.},
 primaryclass = {cs},
 eprint = {2410.22133},
 month = {October}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>