<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3743</fr:anchor><fr:addr
type="machine">#247</fr:addr><fr:route>unstable-247.xml</fr:route><fr:title
text="11/20/2024 › Tomorrow Todo's"><fr:link
type="local"
href="log-0006.xml"
addr="log-0006"
title="11/20/2024">11/20/2024</fr:link> › Tomorrow Todo's</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>Section 3 of <fr:link
type="local"
href="billingsley1986.xml"
addr="billingsley1986"
title="Probability and Measure">Probability and Measure</fr:link> seems a bit dense. Finish construction of extension and see how we feel.</fr:li>
    <fr:li>Time to deeply understand contrastive RL. In particular, how Q-function comes from contrastive loss</fr:li>
    <fr:ul><fr:li>If in the mood, try to understand <fr:link
type="local"
href="nachum2019.xml"
addr="nachum2019"
title="Near Optimal Representation Learning for Hierarchical Reinforcement Learning">Near Optimal Representation Learning for Hierarchical Reinforcement Learning</fr:link> connection to contrastive RL</fr:li></fr:ul>
    <fr:li>Otherwise, pick one of tracer work / linear probing work to make some progress on.</fr:li></fr:ul>
</fr:mainmatter><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Context">Context</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3745</fr:anchor><fr:addr
type="user">log-0006</fr:addr><fr:route>log-0006.xml</fr:route><fr:title
text="11/20/2024">11/20/2024</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors><fr:meta
name="author">false</fr:meta></fr:frontmatter><fr:mainmatter><fr:p>I am still hitting an afternoon wall. I have yet to put a full day together in awhile, but the waking up early and working has helped a little bit.</fr:p>
  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2591</fr:anchor><fr:addr
type="machine">#246</fr:addr><fr:route>unstable-246.xml</fr:route><fr:title
text="Daily Summary">Daily Summary</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:p><fr:strong>Probability Theory:</fr:strong> Upon waking up, I seem to struggle to immediately go into coding mode. To still be productive, I have instituted a self-study or paper reading upon waking up. For some reason this is more manageable to me? Anyways, I have made it through the second section of <fr:link
type="local"
href="billingsley1986.xml"
addr="billingsley1986"
title="Probability and Measure">Probability and Measure</fr:link>, where we defined probabilty measures. I do think carefully going through this without worrying about problem sets, etc. has allowed me to gain a bit more intuition into what is going on.</fr:p>

  <fr:p><fr:strong>Weak-to-strong Generalization:</fr:strong> Upon reading the linear probing paper <fr:link
type="external"
href="https://arxiv.org/pdf/2202.10054">Kumar et al. 2022</fr:link> and Mihir's wonderful presentation, I understand better what their claims are and have kind of shifted focus.

    <fr:ul><fr:li>In the linear probing case, they assume the in distribution and out of distribution still come from the same "sample space" in some sense.</fr:li>
        <fr:li>They also are all correctly labeled unlike in weak-to-strong.</fr:li>
        <fr:li>To me, this applies better to easy-to-hard generalization, so we are now exploring this direction instead.</fr:li>
        <fr:li>The main question now is how to theoretically represent an "easy" and hard question? Along with how to represent the "latent knowledge" that allows the model to generalize from easy-to-hard?</fr:li>
        <fr:li>Need to setup experiments to test linear probing vs. fine-tuning in easy-to-hard.</fr:li>
        <fr:li>Existing papers only do linear probing vs. LoRA.</fr:li></fr:ul></fr:p>
<fr:p><fr:strong>Ghost in the shell:</fr:strong> This project has also started to take off a little bit. I have been tasked with implementing the python tracer to provide profile information to our LaMIR.
<fr:ul><fr:li>I am still feeling a bit ambitious and want to do this in C with CPython. However, my group is more intent on just getting something working, which I can understand.</fr:li>
    <fr:li>Need to better gauge how difficult a C tracer would be before I decide.</fr:li></fr:ul></fr:p>
</fr:mainmatter><fr:backmatter /></fr:tree>
  

  
    <fr:tree
toc="false"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2593</fr:anchor><fr:addr
type="machine">#247</fr:addr><fr:route>unstable-247.xml</fr:route><fr:title
text="Tomorrow Todo's">Tomorrow Todo's</fr:title><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter>
  <fr:ul><fr:li>Section 3 of <fr:link
type="local"
href="billingsley1986.xml"
addr="billingsley1986"
title="Probability and Measure">Probability and Measure</fr:link> seems a bit dense. Finish construction of extension and see how we feel.</fr:li>
    <fr:li>Time to deeply understand contrastive RL. In particular, how Q-function comes from contrastive loss</fr:li>
    <fr:ul><fr:li>If in the mood, try to understand <fr:link
type="local"
href="nachum2019.xml"
addr="nachum2019"
title="Near Optimal Representation Learning for Hierarchical Reinforcement Learning">Near Optimal Representation Learning for Hierarchical Reinforcement Learning</fr:link> connection to contrastive RL</fr:li></fr:ul>
    <fr:li>Otherwise, pick one of tracer work / linear probing work to make some progress on.</fr:li></fr:ul>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>