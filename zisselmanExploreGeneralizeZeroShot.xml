<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3550</fr:anchor><fr:addr
type="user">zisselmanExploreGeneralizeZeroShot</fr:addr><fr:route>zisselmanExploreGeneralizeZeroShot.xml</fr:route><fr:title
text="Explore to Generalize in Zero-Shot RL">Explore to Generalize in Zero-Shot RL</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author>Ev Zisselman</fr:author><fr:author>Itai Lavie</fr:author><fr:author>Daniel Soudry</fr:author><fr:author>Aviv Tamar</fr:author></fr:authors><fr:meta
name="bibtex"><![CDATA[@article{zisselmanExploreGeneralizeZeroShot,
 title = {Explore to {{Generalize}} in {{Zero-Shot RL}}},
 author = {Zisselman, Ev and Lavie, Itai and Soudry, Daniel and Tamar, Aviv},
 file = {/home/kellen/Zotero/storage/9D7NJ4H9/Zisselman et al. - Explore to Generalize in Zero-Shot RL.pdf},
 langid = {english},
 abstract = {We study zero-shot generalization in reinforcement learning---optimizing a policy on a set of training tasks to perform well on a similar but unseen test task. To mitigate overfitting, previous work explored different notions of invariance to the task. However, on problems such as the ProcGen Maze, an adequate solution that is invariant to the task visualization does not exist, and therefore invariance-based approaches fail. Our insight is that learning a policy that effectively explores the domain is harder to memorize than a policy that maximizes reward for a specific task, and therefore we expect such learned behavior to generalize well; we indeed demonstrate this empirically on several domains that are difficult for invariancebased approaches. Our Explore to Generalize algorithm (ExpGen) builds on this insight: we train an additional ensemble of agents that optimize reward. At test time, either the ensemble agrees on an action, and we generalize well, or we take exploratory actions, which generalize well and drive us to a novel part of the state space, where the ensemble may potentially agree again. We show that our approach is the state-of-the-art on tasks of the ProcGen challenge that have thus far eluded effective generalization, yielding a success rate of 83\% on the Maze task and 74\% on Heist with 200 training levels. ExpGen can also be combined with an invariance based approach to gain the best of both worlds, setting new state-of-the-art results on ProcGen. Code available at https://github.com/EvZissel/expgen.}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>