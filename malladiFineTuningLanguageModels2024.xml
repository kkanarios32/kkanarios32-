<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3179</fr:anchor><fr:addr
type="user">malladiFineTuningLanguageModels2024</fr:addr><fr:route>malladiFineTuningLanguageModels2024.xml</fr:route><fr:title
text="Fine-Tuning Language Models with Just Forward Passes">Fine-Tuning Language Models with Just Forward Passes</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>1</fr:month></fr:date><fr:authors><fr:author>Sadhika Malladi</fr:author><fr:author>Tianyu Gao</fr:author><fr:author>Eshaan Nichani</fr:author><fr:author>Alex Damian</fr:author><fr:author>Jason D. Lee</fr:author><fr:author>Danqi Chen</fr:author><fr:author>Sanjeev Arora</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2305.17333</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2305.17333</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{malladiFineTuningLanguageModels2024,
 title = {Fine-{{Tuning Language Models}} with {{Just Forward Passes}}},
 author = {Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D. and Chen, Danqi and Arora, Sanjeev},
 year = {2024},
 doi = {10.48550/arXiv.2305.17333},
 urldate = {2024-09-08},
 number = {arXiv:2305.17333},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/2J3FEQX7/Malladi et al. - 2024 - Fine-Tuning Language Models with Just Forward Passes.pdf},
 keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zerothorder optimizer (MeZO), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with the same memory footprint as inference. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12x memory reduction and up to 2x GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.},
 primaryclass = {cs},
 eprint = {2305.17333},
 month = {January}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>