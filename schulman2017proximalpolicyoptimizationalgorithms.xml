<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3372</fr:anchor><fr:addr
type="user">schulman2017proximalpolicyoptimizationalgorithms</fr:addr><fr:route>schulman2017proximalpolicyoptimizationalgorithms.xml</fr:route><fr:title
text="Proximal policy optimization algorithms">Proximal policy optimization algorithms</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2017</fr:year></fr:date><fr:authors><fr:author>John Schulman</fr:author><fr:author>Filip Wolski</fr:author><fr:author>Prafulla Dhariwal</fr:author><fr:author>Alec Radford</fr:author><fr:author>Oleg Klimov</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/1707.06347</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{schulman2017proximalpolicyoptimizationalgorithms,
 title = {Proximal Policy Optimization Algorithms},
 author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec
Radford and Oleg Klimov},
 year = {2017},
 url = {https://arxiv.org/abs/1707.06347},
 primaryclass = {cs.LG},
 archiveprefix = {arXiv},
 eprint = {1707.06347}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter><fr:tree
toc="false"
numbered="false"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:title
text="Backlinks">Backlinks</fr:title><fr:authors /></fr:frontmatter><fr:mainmatter><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3378</fr:anchor><fr:addr
type="user">kak-003X</fr:addr><fr:route>kak-003X.xml</fr:route><fr:title
text="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter><fr:p>Traditional actor critic RL algorithms, require training both an actor and a critic (as the name implies). Typically, these components are both of equal size. In the field of RL, this is non-problematic because models are typically rather small (at least in comparison to LLMs).
In <fr:link
type="local"
href="shao2024deepseekmathpushinglimitsmathematical.xml"
addr="shao2024deepseekmathpushinglimitsmathematical"
title="DeepSeekMath: Pushing the limits of mathematical reasoning in open language models">DeepSeekMath: Pushing the limits of mathematical reasoning in open language models</fr:link></fr:p>
  
    <fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="false"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>470</fr:anchor><fr:addr
type="machine">#288</fr:addr><fr:route>unstable-288.xml</fr:route><fr:title
text="Math">Math</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors /></fr:frontmatter><fr:mainmatter>
  <fr:tex
display="block"><![CDATA[     \begin {align*}     {\mathcal {J}}_{\mathrm {GRPO}}(\theta )&= \mathbb {E}[q\sim  P(Q),\{o_{i}\}_{i=1}^{G}\sim \pi _{\theta _{o l d}}(O|q)] \\     &= \frac {1}{G}\sum _{i=1}^{G}\left (\operatorname *{min}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d}}(o_{i}|q)}A_{i},\operatorname *{clip}\left (\frac {\pi _{\theta }(o_{i}|q)}{\pi _{\theta _{o d d}}(o_{i}|q)},1-\varepsilon ,1+\varepsilon \right )A_{i}\right )-\beta \mathbb {D}_{K L}\left (\pi _{\theta }||\pi _{r e f}\right )\right )     \end {align*}   ]]></fr:tex>
  where
  <fr:tex
display="block"><![CDATA[         \mathbb {D}_{\mathrm {K L}}\left (\pi _{\theta }||\pi _{\mathrm {ref}}\right )=\frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-\log \frac {\pi _{\mathrm {ref}}(o_{i}|q)}{\pi _{\theta }(o_{i}|q)}-1     ]]></fr:tex>
    The astute RL reader will notice this is essentially <fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">PPO</fr:link>.
    The key distinction here is that the advantage <fr:tex
display="inline"><![CDATA[A_i]]></fr:tex> is not computed using a critic model. Instead, 
<fr:tex
display="block"><![CDATA[A_{i}=\frac {r_{i}-\mathrm {mean}(\{r_{1},r_{2},\cdots ,r_{G}\})}{\mathrm {std}(\{r_{1},r_{2},\cdots ,r_{G}\})}.]]></fr:tex>
</fr:mainmatter><fr:backmatter /></fr:tree>
  
</fr:mainmatter><fr:backmatter /></fr:tree><fr:tree
toc="true"
numbered="false"
show-heading="true"
show-metadata="true"
expanded="false"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3379</fr:anchor><fr:addr
type="user">kak-003Y</fr:addr><fr:route>kak-003Y.xml</fr:route><fr:title
text="The History and Evolution of Policy Gradient Algorithms">The History and Evolution of Policy Gradient Algorithms</fr:title><fr:date><fr:year>2025</fr:year><fr:month>1</fr:month><fr:day>30</fr:day></fr:date><fr:authors><fr:author><fr:link
type="local"
href="kellenkanarios.xml"
addr="kellenkanarios"
title="Kellen Kanarios">Kellen Kanarios</fr:link></fr:author></fr:authors></fr:frontmatter><fr:mainmatter><fr:p>Rough itinerary,
  <fr:ul><fr:li>Vanilla policy gradient
      <fr:ul><fr:li>Policy gradient theorem + proof</fr:li>
          <fr:li>Deterministic policy gradient theorem + (maybe)proof</fr:li></fr:ul></fr:li>
      <fr:li>Actor critic method
      <fr:ul><fr:li>A2C: Variance reduction method</fr:li>
        <fr:li>(Maybe) A3C: Asynchronous update</fr:li></fr:ul></fr:li>
      <fr:li>Trust region policy optimization</fr:li>
      <fr:li>Soft Actor Critic</fr:li>
      <fr:li><fr:link
type="local"
href="schulman2017proximalpolicyoptimizationalgorithms.xml"
addr="schulman2017proximalpolicyoptimizationalgorithms"
title="Proximal policy optimization algorithms">Proximal Policy Optimization</fr:link></fr:li>
      <fr:li><fr:link
type="local"
href="kak-003X.xml"
addr="kak-003X"
title="Group Relative Policy Optimization">Group Relative Policy Optimization</fr:link></fr:li></fr:ul></fr:p></fr:mainmatter><fr:backmatter /></fr:tree></fr:mainmatter><fr:backmatter /></fr:tree></fr:backmatter></fr:tree>