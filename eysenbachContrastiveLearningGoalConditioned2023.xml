<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3199</fr:anchor><fr:addr
type="user">eysenbachContrastiveLearningGoalConditioned2023</fr:addr><fr:route>eysenbachContrastiveLearningGoalConditioned2023.xml</fr:route><fr:title
text="Contrastive Learning as Goal-Conditioned Reinforcement Learning">Contrastive Learning as Goal-Conditioned Reinforcement Learning</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2023</fr:year><fr:month>2</fr:month></fr:date><fr:authors><fr:author>Benjamin Eysenbach</fr:author><fr:author>Tianjun Zhang</fr:author><fr:author>Ruslan Salakhutdinov</fr:author><fr:author>Sergey Levine</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2206.07568</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2206.07568</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{eysenbachContrastiveLearningGoalConditioned2023,
 title = {Contrastive {{Learning}} as {{Goal-Conditioned Reinforcement Learning}}},
 author = {Eysenbach, Benjamin and Zhang, Tianjun and Salakhutdinov, Ruslan and Levine, Sergey},
 year = {2023},
 doi = {10.48550/arXiv.2206.07568},
 urldate = {2024-09-06},
 number = {arXiv:2206.07568},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/WVQKIMN4/Eysenbach et al. - 2023 - Contrastive Learning as Goal-Conditioned Reinforcement Learning.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often finds that learning representations in an end-to-end fashion is unstable and instead equip RL algorithms with additional representation learning parts (e.g., auxiliary losses, data augmentation). How can we design RL algorithms that directly acquire good representations? In this paper, instead of adding representation learning parts to an existing RL algorithm, we show (contrastive) representation learning methods can be cast as RL algorithms in their own right. To do this, we build upon prior work and apply contrastive representation learning to action-labeled trajectories, in such a way that the (inner product of) learned representations exactly corresponds to a goal-conditioned value function. We use this idea to reinterpret a prior RL method as performing contrastive learning, and then use the idea to propose a much simpler method that achieves similar performance. Across a range of goal-conditioned RL tasks, we demonstrate that contrastive RL methods achieve higher success rates than prior non-contrastive methods, including in the offline RL setting. We also show that contrastive RL outperforms prior methods on image-based tasks, without using data augmentation or auxiliary objectives.},
 primaryclass = {cs},
 eprint = {2206.07568},
 month = {February}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>