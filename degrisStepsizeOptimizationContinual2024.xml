<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3127</fr:anchor><fr:addr
type="user">degrisStepsizeOptimizationContinual2024</fr:addr><fr:route>degrisStepsizeOptimizationContinual2024.xml</fr:route><fr:title
text="Step-size Optimization for Continual Learning">Step-size Optimization for Continual Learning</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>1</fr:month></fr:date><fr:authors><fr:author>Thomas Degris</fr:author><fr:author>Khurram Javed</fr:author><fr:author>Arsalan Sharifnassab</fr:author><fr:author>Yuxin Liu</fr:author><fr:author>Richard Sutton</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/2401.17401v1</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{degrisStepsizeOptimizationContinual2024,
 title = {Step-Size {{Optimization}} for {{Continual Learning}}},
 author = {Degris, Thomas and Javed, Khurram and Sharifnassab, Arsalan and Liu, Yuxin and Sutton, Richard},
 year = {2024},
 urldate = {2024-09-06},
 howpublished = {https://arxiv.org/abs/2401.17401v1},
 journal = {arXiv.org},
 file = {/home/kellen/Zotero/storage/YSGENB8H/Degris et al. - 2024 - Step-size Optimization for Continual Learning.pdf},
 langid = {english},
 abstract = {In continual learning, a learner has to keep learning from the data over its whole life time. A key issue is to decide what knowledge to keep and what knowledge to let go. In a neural network, this can be implemented by using a step-size vector to scale how much gradient samples change network weights. Common algorithms, like RMSProp and Adam, use heuristics, specifically normalization, to adapt this step-size vector. In this paper, we show that those heuristics ignore the effect of their adaptation on the overall objective function, for example by moving the step-size vector away from better step-size vectors. On the other hand, stochastic meta-gradient descent algorithms, like IDBD (Sutton, 1992), explicitly optimize the step-size vector with respect to the overall objective function. On simple problems, we show that IDBD is able to consistently improve step-size vectors, where RMSProp and Adam do not. We explain the differences between the two approaches and their respective limitations. We conclude by suggesting that combining both approaches could be a promising future direction to improve the performance of neural networks in continual learning.},
 month = {January}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>