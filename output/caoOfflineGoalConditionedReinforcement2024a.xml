<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2935</fr:anchor><fr:addr
type="user">caoOfflineGoalConditionedReinforcement2024a</fr:addr><fr:route>caoOfflineGoalConditionedReinforcement2024a.xml</fr:route><fr:title
text="Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy">Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>3</fr:month></fr:date><fr:authors><fr:author>Chenyang Cao</fr:author><fr:author>Zichen Yan</fr:author><fr:author>Renhao Lu</fr:author><fr:author>Junbo Tan</fr:author><fr:author>Xueqian Wang</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2403.01734</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2403.01734</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{caoOfflineGoalConditionedReinforcement2024a,
 title = {Offline {{Goal-Conditioned Reinforcement Learning}} for {{Safety-Critical Tasks}} with {{Recovery Policy}}},
 author = {Cao, Chenyang and Yan, Zichen and Lu, Renhao and Tan, Junbo and Wang, Xueqian},
 year = {2024},
 doi = {10.48550/arXiv.2403.01734},
 urldate = {2025-01-03},
 number = {arXiv:2403.01734},
 publisher = {arXiv},
 file = {/home/kellenkanarios/Downloads/Papers/Safe-RL/Cao et al_2024_Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with.pdf;/home/kellen/Zotero/storage/MIF9JL6P/2403.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
 archiveprefix = {arXiv},
 abstract = {Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GCRL algorithms and one offline safe RL algorithm. As a result, our method outperforms the existing state-of-the-art methods to a large extent. Furthermore, we validate the practicality and effectiveness of RbSL by deploying it on a real Panda manipulator. Code is available at https://github.com/Sunlighted/RbSL.git.},
 primaryclass = {cs},
 eprint = {2403.01734},
 month = {March}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>