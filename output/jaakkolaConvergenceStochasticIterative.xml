<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>3686</fr:anchor><fr:addr
type="user">jaakkolaConvergenceStochasticIterative</fr:addr><fr:route>jaakkolaConvergenceStochasticIterative.xml</fr:route><fr:title
text="On the Convergence of Stochastic Iterative Dynamic Programming Algorithms">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</fr:title><fr:taxon>Reference</fr:taxon><fr:authors><fr:author>Tommi Jaakkola</fr:author><fr:author>Michael I Jordan</fr:author><fr:author>Satinder P Singh</fr:author></fr:authors><fr:meta
name="bibtex"><![CDATA[@article{jaakkolaConvergenceStochasticIterative,
 title = {On the {{Convergence}} of {{Stochastic Iterative Dynamic Programming Algorithms}}},
 author = {Jaakkola, Tommi and Jordan, Michael I and Singh, Satinder P},
 file = {/home/kellen/Zotero/storage/95ZWX95X/Jaakkola et al. - On the Convergence of Stochastic Iterative Dynamic Programming Algorithms.pdf},
 langid = {english},
 abstract = {Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD( ) algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD( ) and Q-learning belong.}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>