<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2806</fr:anchor><fr:addr
type="user">jainAdaptiveExplorationDataEfficient2024</fr:addr><fr:route>jainAdaptiveExplorationDataEfficient2024.xml</fr:route><fr:title
text="Adaptive Exploration for Data-Efficient General Value Function Evaluations">Adaptive Exploration for Data-Efficient General Value Function Evaluations</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>10</fr:month></fr:date><fr:authors><fr:author>Arushi Jain</fr:author><fr:author>Josiah P. Hanna</fr:author><fr:author>Doina Precup</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2405.07838</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2405.07838</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{jainAdaptiveExplorationDataEfficient2024,
 title = {Adaptive {{Exploration}} for {{Data-Efficient General Value Function Evaluations}}},
 author = {Jain, Arushi and Hanna, Josiah P. and Precup, Doina},
 year = {2024},
 doi = {10.48550/arXiv.2405.07838},
 urldate = {2024-12-18},
 number = {arXiv:2405.07838},
 publisher = {arXiv},
 file = {/home/kellenkanarios/Downloads/Papers/Continual-RL/Jain et al_2024_Adaptive Exploration for Data-Efficient General Value Function Evaluations.pdf;/home/kellen/Zotero/storage/SAPYRAL8/2405.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 archiveprefix = {arXiv},
 abstract = {General Value Functions (GVFs) (Sutton et al., 2011) represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique reward. Existing methods relying on fixed behavior policies or pre-collected data often face data efficiency issues when learning multiple GVFs in parallel using off-policy methods. To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel. Our method optimizes the behavior policy by minimizing the total variance in return across GVFs, thereby reducing the required environmental interactions. We use an existing temporal-difference-style variance estimator to approximate the return variance. We prove that each behavior policy update decreases the overall mean squared error in GVF predictions. We empirically show our method's performance in tabular and nonlinear function approximation settings, including Mujoco environments, with stationary and non-stationary reward signals, optimizing data usage and reducing prediction errors across multiple GVFs.},
 primaryclass = {cs},
 eprint = {2405.07838},
 month = {October}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>