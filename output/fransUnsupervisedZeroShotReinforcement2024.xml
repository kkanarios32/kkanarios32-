<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2958</fr:anchor><fr:addr
type="user">fransUnsupervisedZeroShotReinforcement2024</fr:addr><fr:route>fransUnsupervisedZeroShotReinforcement2024.xml</fr:route><fr:title
text="Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings">Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>2</fr:month></fr:date><fr:authors><fr:author>Kevin Frans</fr:author><fr:author>Seohong Park</fr:author><fr:author>Pieter Abbeel</fr:author><fr:author>Sergey Levine</fr:author></fr:authors><fr:meta
name="doi">10.48550/arXiv.2402.17135</fr:meta><fr:meta
name="external">https://arxiv.org/abs/2402.17135</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{fransUnsupervisedZeroShotReinforcement2024,
 title = {Unsupervised {{Zero-Shot Reinforcement Learning}} via {{Functional Reward Encodings}}},
 author = {Frans, Kevin and Park, Seohong and Abbeel, Pieter and Levine, Sergey},
 year = {2024},
 doi = {10.48550/arXiv.2402.17135},
 urldate = {2024-12-04},
 number = {arXiv:2402.17135},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/FUDVV3U4/Frans et al. - 2024 - Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings.pdf},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformerbased variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zeroshot manner, given a small number of rewardannotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL and offline RL methods. Code for this project is provided at: github.com/kvfrans/fre.},
 primaryclass = {cs},
 eprint = {2402.17135},
 month = {February}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>