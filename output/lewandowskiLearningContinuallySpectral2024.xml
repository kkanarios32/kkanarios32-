<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="default.xsl"?>
<fr:tree
toc="true"
numbered="true"
show-heading="true"
show-metadata="true"
expanded="true"
root="false"
xmlns:fr="http://www.jonmsterling.com/jms-005P.xml"><fr:frontmatter><fr:anchor>2863</fr:anchor><fr:addr
type="user">lewandowskiLearningContinuallySpectral2024</fr:addr><fr:route>lewandowskiLearningContinuallySpectral2024.xml</fr:route><fr:title
text="Learning Continually by Spectral Regularization">Learning Continually by Spectral Regularization</fr:title><fr:taxon>Reference</fr:taxon><fr:date><fr:year>2024</fr:year><fr:month>6</fr:month></fr:date><fr:authors><fr:author>Alex Lewandowski</fr:author><fr:author>Saurabh Kumar</fr:author><fr:author>Dale Schuurmans</fr:author><fr:author>András György</fr:author><fr:author>Marlos C. Machado</fr:author></fr:authors><fr:meta
name="external">https://arxiv.org/abs/2406.06811</fr:meta><fr:meta
name="bibtex"><![CDATA[@misc{lewandowskiLearningContinuallySpectral2024,
 title = {Learning {{Continually}} by {{Spectral Regularization}}},
 author = {Lewandowski, Alex and Kumar, Saurabh and Schuurmans, Dale and Gy{\"o}rgy, Andr{\'a}s and Machado, Marlos C.},
 year = {2024},
 urldate = {2024-10-18},
 number = {arXiv:2406.06811},
 publisher = {arXiv},
 file = {/home/kellen/Zotero/storage/6HX2DA95/Lewandowski et al. - 2024 - Learning Continually by Spectral Regularization.pdf},
 keywords = {Computer Science - Machine Learning},
 langid = {english},
 archiveprefix = {arXiv},
 abstract = {Loss of plasticity is a phenomenon where neural networks become more difficult to train during the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good predictive performance while maintaining network trainability. We develop new techniques for improving continual learning by first reconsidering how initialization can ensure trainability during early phases of learning. From this perspective, we derive new regularization strategies for continual learning that ensure beneficial initialization properties are better maintained throughout training. In particular, we investigate two new regularization techniques for continual learning: (i) Wasserstein regularization toward the initial weight distribution, which is less restrictive than regularizing toward initial weights; and (ii) regularizing weight matrix singular values, which directly ensures gradient diversity is maintained throughout training. We present an experimental analysis that shows these alternative regularizers can improve continual learning performance across a range of supervised learning tasks and model architectures. The alternative regularizers prove to be less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance.},
 primaryclass = {cs},
 eprint = {2406.06811},
 month = {June}
}]]></fr:meta></fr:frontmatter><fr:mainmatter /><fr:backmatter /></fr:tree>